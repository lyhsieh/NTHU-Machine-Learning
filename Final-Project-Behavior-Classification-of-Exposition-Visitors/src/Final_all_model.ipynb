{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook includes most of our main experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 21:00:10.274696: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-15 21:00:10.274720: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.layers import SimpleRNN\n",
    "from transformers import TFBertModel, BertConfig\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#FF0000> Data preprocess (w/o group)</font> \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mac_hash</th>\n",
       "      <th>sniffer_loc</th>\n",
       "      <th>created_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0003ae8541d0e925fcee242287e2ad27</td>\n",
       "      <td>11</td>\n",
       "      <td>2018-12-07 16:48:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00078611037990f7f36b722f22595fe7</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-12-07 16:29:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00078611037990f7f36b722f22595fe7</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-12-07 16:30:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00078611037990f7f36b722f22595fe7</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-12-07 16:37:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00078611037990f7f36b722f22595fe7</td>\n",
       "      <td>8</td>\n",
       "      <td>2018-12-07 16:37:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41635</th>\n",
       "      <td>fff2ca0333532d6348d5b4fa39028dd4</td>\n",
       "      <td>3</td>\n",
       "      <td>2018-12-07 18:37:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41636</th>\n",
       "      <td>fff2ca0333532d6348d5b4fa39028dd4</td>\n",
       "      <td>13</td>\n",
       "      <td>2018-12-07 18:47:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41637</th>\n",
       "      <td>fff2ca0333532d6348d5b4fa39028dd4</td>\n",
       "      <td>12</td>\n",
       "      <td>2018-12-07 18:47:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41638</th>\n",
       "      <td>fff2ca0333532d6348d5b4fa39028dd4</td>\n",
       "      <td>11</td>\n",
       "      <td>2018-12-07 18:47:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41639</th>\n",
       "      <td>fff2ca0333532d6348d5b4fa39028dd4</td>\n",
       "      <td>9</td>\n",
       "      <td>2018-12-07 18:47:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41640 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               mac_hash  sniffer_loc         created_time\n",
       "0      0003ae8541d0e925fcee242287e2ad27           11  2018-12-07 16:48:08\n",
       "1      00078611037990f7f36b722f22595fe7            3  2018-12-07 16:29:35\n",
       "2      00078611037990f7f36b722f22595fe7            2  2018-12-07 16:30:41\n",
       "3      00078611037990f7f36b722f22595fe7            4  2018-12-07 16:37:06\n",
       "4      00078611037990f7f36b722f22595fe7            8  2018-12-07 16:37:07\n",
       "...                                 ...          ...                  ...\n",
       "41635  fff2ca0333532d6348d5b4fa39028dd4            3  2018-12-07 18:37:37\n",
       "41636  fff2ca0333532d6348d5b4fa39028dd4           13  2018-12-07 18:47:53\n",
       "41637  fff2ca0333532d6348d5b4fa39028dd4           12  2018-12-07 18:47:53\n",
       "41638  fff2ca0333532d6348d5b4fa39028dd4           11  2018-12-07 18:47:54\n",
       "41639  fff2ca0333532d6348d5b4fa39028dd4            9  2018-12-07 18:47:54\n",
       "\n",
       "[41640 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "train_label = pd.read_csv('training-label.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = train_label['mac_hash']\n",
    "label_list = train_label['label']\n",
    "\n",
    "id_dict = {m: [] for m in id_list}\n",
    "\n",
    "# label_dict\n",
    "label_dict = dict()\n",
    "\n",
    "for i, val in enumerate(id_list):\n",
    "    label_dict[val] = int(label_list[i])\n",
    "\n",
    "for data in list(train_data.values):\n",
    "    loc = data[1]\n",
    "    id_dict[data[0]].append(loc)\n",
    "\n",
    "for k, v in id_dict.items():\n",
    "    if len(v) != 14:\n",
    "        id_dict[k] = [0] * (14 - len(v)) + v\n",
    "    id_dict[k].append(label_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(list(id_dict.values()))\n",
    "X_train = data[:,:-1]\n",
    "y_train = data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('submit_samples.csv')\n",
    "\n",
    "id_list_test = sample['mac_hash']\n",
    "\n",
    "id_dict_test = {m: [] for m in id_list_test}\n",
    "\n",
    "# label_dict\n",
    "label_dict_test = dict()\n",
    "\n",
    "for data in list(test_data.values):\n",
    "    loc = data[1]\n",
    "    id_dict_test[data[0]].append(loc)\n",
    "\n",
    "for k, v in id_dict_test.items():\n",
    "    if len(v) != 14:\n",
    "        id_dict_test[k] = [0] * (14 - len(v)) + v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(list(id_dict_test.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#FF0000> Random Forest (w/o group)</font> \t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (with cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation mean accuracy: 0.9535854466727816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done   2 out of   5 | elapsed:    2.8s remaining:    4.2s\n",
      "[Parallel(n_jobs=6)]: Done   5 out of   5 | elapsed:    2.9s finished\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=500, criterion='gini')\n",
    "result = cross_validate(model, X_train, y_train, cv=5, verbose=3, n_jobs=6)\n",
    "print('Cross validation mean accuracy:', result['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (train with  whole training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=500, criterion='gini')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_prob = model.predict_proba(X_train)\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "print('Training set accuracy:', accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.174, 0.232, 0.594, 0.   , 0.   ],\n",
       "       [0.   , 0.   , 0.   , 0.016, 0.984],\n",
       "       [1.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "       ...,\n",
       "       [1.   , 0.   , 0.   , 0.   , 0.   ],\n",
       "       [0.072, 0.004, 0.924, 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   , 0.   , 0.   ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob = model.predict_proba(X_test)\n",
    "y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mac_hash</th>\n",
       "      <th>C0</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b882f1d44602a25349a08f4a0af32977</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86e644b498922f3a2fd0e6e1a1525de3</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>691662b04ee08015062d901a4c5628b1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52b5c510a28774237f4f118764c2ed6f</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9f3c995e53d109f532056b6eae29a0b5</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.214</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>5001c0dfe522886ed884c0dc4e5848df</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425</th>\n",
       "      <td>f1c8781e701e89068fa0b8a62ecbc564</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426</th>\n",
       "      <td>646136b402e136422466a2acd8636630</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427</th>\n",
       "      <td>c7bea6491029f86ab3d1f0f9b599ca2c</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428</th>\n",
       "      <td>822fd3104f38cc6374f2996d3d6920a2</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3429 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              mac_hash     C0     C1     C2     C3     C4\n",
       "0     b882f1d44602a25349a08f4a0af32977  0.174  0.232  0.594  0.000  0.000\n",
       "1     86e644b498922f3a2fd0e6e1a1525de3  0.000  0.000  0.000  0.016  0.984\n",
       "2     691662b04ee08015062d901a4c5628b1  1.000  0.000  0.000  0.000  0.000\n",
       "3     52b5c510a28774237f4f118764c2ed6f  0.000  1.000  0.000  0.000  0.000\n",
       "4     9f3c995e53d109f532056b6eae29a0b5  0.002  0.214  0.776  0.004  0.004\n",
       "...                                ...    ...    ...    ...    ...    ...\n",
       "3424  5001c0dfe522886ed884c0dc4e5848df  1.000  0.000  0.000  0.000  0.000\n",
       "3425  f1c8781e701e89068fa0b8a62ecbc564  0.000  0.000  0.010  0.054  0.936\n",
       "3426  646136b402e136422466a2acd8636630  1.000  0.000  0.000  0.000  0.000\n",
       "3427  c7bea6491029f86ab3d1f0f9b599ca2c  0.072  0.004  0.924  0.000  0.000\n",
       "3428  822fd3104f38cc6374f2996d3d6920a2  1.000  0.000  0.000  0.000  0.000\n",
       "\n",
       "[3429 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame({'mac_hash':sample['mac_hash'], 'C0':y_prob[:,0], 'C1':y_prob[:,1], 'C2':y_prob[:,2], 'C3':y_prob[:,3], 'C4':y_prob[:,4]})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"output_RandomForest.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#FF0000> Catboost (w/o group)</font> \t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (with cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "AUC is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 5.73ms\tremaining: 5.72s\n",
      "100:\ttotal: 487ms\tremaining: 4.33s\n",
      "200:\ttotal: 927ms\tremaining: 3.68s\n",
      "300:\ttotal: 1.36s\tremaining: 3.17s\n",
      "400:\ttotal: 1.8s\tremaining: 2.69s\n",
      "500:\ttotal: 2.23s\tremaining: 2.23s\n",
      "600:\ttotal: 2.67s\tremaining: 1.77s\n",
      "700:\ttotal: 3.11s\tremaining: 1.32s\n",
      "800:\ttotal: 3.54s\tremaining: 881ms\n",
      "900:\ttotal: 3.98s\tremaining: 438ms\n",
      "999:\ttotal: 4.42s\tremaining: 0us\n",
      "[CV] END ......................................., score=0.958 total time=   4.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.8s remaining:    0.0s\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "AUC is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 6.05ms\tremaining: 6.05s\n",
      "100:\ttotal: 441ms\tremaining: 3.93s\n",
      "200:\ttotal: 829ms\tremaining: 3.29s\n",
      "300:\ttotal: 1.27s\tremaining: 2.94s\n",
      "400:\ttotal: 1.66s\tremaining: 2.49s\n",
      "500:\ttotal: 2.1s\tremaining: 2.09s\n",
      "600:\ttotal: 2.56s\tremaining: 1.7s\n",
      "700:\ttotal: 2.95s\tremaining: 1.26s\n",
      "800:\ttotal: 3.41s\tremaining: 847ms\n",
      "900:\ttotal: 3.81s\tremaining: 418ms\n",
      "999:\ttotal: 4.21s\tremaining: 0us\n",
      "[CV] END ......................................., score=0.965 total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    9.4s remaining:    0.0s\n",
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "AUC is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 5.94ms\tremaining: 5.94s\n",
      "100:\ttotal: 426ms\tremaining: 3.79s\n",
      "200:\ttotal: 887ms\tremaining: 3.53s\n",
      "300:\ttotal: 1.28s\tremaining: 2.97s\n",
      "400:\ttotal: 1.68s\tremaining: 2.51s\n",
      "500:\ttotal: 2.08s\tremaining: 2.07s\n",
      "600:\ttotal: 2.48s\tremaining: 1.64s\n",
      "700:\ttotal: 2.9s\tremaining: 1.24s\n",
      "800:\ttotal: 3.35s\tremaining: 831ms\n",
      "900:\ttotal: 3.77s\tremaining: 415ms\n",
      "999:\ttotal: 4.19s\tremaining: 0us\n",
      "[CV] END ......................................., score=0.965 total time=   4.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "AUC is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 5.98ms\tremaining: 5.97s\n",
      "100:\ttotal: 458ms\tremaining: 4.08s\n",
      "200:\ttotal: 863ms\tremaining: 3.43s\n",
      "300:\ttotal: 1.31s\tremaining: 3.05s\n",
      "400:\ttotal: 1.71s\tremaining: 2.56s\n",
      "500:\ttotal: 2.13s\tremaining: 2.13s\n",
      "600:\ttotal: 2.59s\tremaining: 1.72s\n",
      "700:\ttotal: 3.05s\tremaining: 1.3s\n",
      "800:\ttotal: 3.49s\tremaining: 868ms\n",
      "900:\ttotal: 3.9s\tremaining: 429ms\n",
      "999:\ttotal: 4.34s\tremaining: 0us\n",
      "[CV] END ......................................., score=0.962 total time=   4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "AUC is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 5.66ms\tremaining: 5.65s\n",
      "100:\ttotal: 429ms\tremaining: 3.82s\n",
      "200:\ttotal: 851ms\tremaining: 3.38s\n",
      "300:\ttotal: 1.27s\tremaining: 2.95s\n",
      "400:\ttotal: 1.68s\tremaining: 2.51s\n",
      "500:\ttotal: 2.13s\tremaining: 2.12s\n",
      "600:\ttotal: 2.58s\tremaining: 1.71s\n",
      "700:\ttotal: 3s\tremaining: 1.28s\n",
      "800:\ttotal: 3.44s\tremaining: 853ms\n",
      "900:\ttotal: 3.84s\tremaining: 422ms\n",
      "999:\ttotal: 4.26s\tremaining: 0us\n",
      "[CV] END ......................................., score=0.963 total time=   4.7s\n",
      "Cross validation mean accuracy: 0.9624946288537973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   23.5s finished\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(iterations=1000,learning_rate=0.1,max_depth=7,verbose=100,\n",
    "                           early_stopping_rounds=500,task_type='GPU',eval_metric='AUC')\n",
    "\n",
    "result = cross_validate(model, X_train, y_train, cv=5, verbose=3)\n",
    "print('Cross validation mean accuracy:', result['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (train with  whole training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default metric period is 5 because AUC is/are not implemented for GPU\n",
      "AUC is not implemented on GPU. Will use CPU for metric computation, this could significantly affect learning time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttotal: 4.57ms\tremaining: 4.57s\n",
      "100:\ttotal: 406ms\tremaining: 3.61s\n",
      "200:\ttotal: 837ms\tremaining: 3.33s\n",
      "300:\ttotal: 1.29s\tremaining: 3s\n",
      "400:\ttotal: 1.72s\tremaining: 2.57s\n",
      "500:\ttotal: 2.12s\tremaining: 2.11s\n",
      "600:\ttotal: 2.57s\tremaining: 1.7s\n",
      "700:\ttotal: 2.99s\tremaining: 1.27s\n",
      "800:\ttotal: 3.44s\tremaining: 854ms\n",
      "900:\ttotal: 3.87s\tremaining: 425ms\n",
      "999:\ttotal: 4.28s\tremaining: 0us\n",
      "Training set accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(iterations=1000,learning_rate=0.1,max_depth=7,verbose=100,\n",
    "                           early_stopping_rounds=500,task_type='GPU',eval_metric='AUC')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_prob = model.predict_proba(X_train)\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "print('Training set accuracy:', accuracy_score(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.17920453e-01, 1.65108228e-02, 8.64814066e-01, 6.35726734e-04,\n",
       "        1.18931508e-04],\n",
       "       [2.20586851e-06, 1.60560543e-06, 7.58320423e-05, 4.58110866e-03,\n",
       "        9.95339248e-01],\n",
       "       [9.99988129e-01, 1.09094005e-05, 4.48668365e-07, 4.27234932e-07,\n",
       "        8.55970272e-08],\n",
       "       ...,\n",
       "       [9.99814317e-01, 1.82633354e-04, 2.09564183e-06, 8.62817252e-07,\n",
       "        9.10079788e-08],\n",
       "       [1.12559751e-02, 6.79574819e-05, 9.88651026e-01, 2.04516712e-05,\n",
       "        4.59012792e-06],\n",
       "       [9.99821657e-01, 1.76760597e-04, 1.47514272e-07, 1.31998981e-06,\n",
       "        1.14822247e-07]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob = model.predict_proba(X_test)\n",
    "y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mac_hash</th>\n",
       "      <th>C0</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b882f1d44602a25349a08f4a0af32977</td>\n",
       "      <td>0.117920</td>\n",
       "      <td>0.016511</td>\n",
       "      <td>8.648141e-01</td>\n",
       "      <td>6.357267e-04</td>\n",
       "      <td>1.189315e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86e644b498922f3a2fd0e6e1a1525de3</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>7.583204e-05</td>\n",
       "      <td>4.581109e-03</td>\n",
       "      <td>9.953392e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>691662b04ee08015062d901a4c5628b1</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>4.486684e-07</td>\n",
       "      <td>4.272349e-07</td>\n",
       "      <td>8.559703e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52b5c510a28774237f4f118764c2ed6f</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.997549</td>\n",
       "      <td>5.361618e-07</td>\n",
       "      <td>1.487764e-06</td>\n",
       "      <td>1.237406e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9f3c995e53d109f532056b6eae29a0b5</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.066536</td>\n",
       "      <td>9.302961e-01</td>\n",
       "      <td>1.792449e-03</td>\n",
       "      <td>9.980440e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>5001c0dfe522886ed884c0dc4e5848df</td>\n",
       "      <td>0.999814</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>1.904194e-07</td>\n",
       "      <td>1.165337e-06</td>\n",
       "      <td>1.026893e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425</th>\n",
       "      <td>f1c8781e701e89068fa0b8a62ecbc564</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>6.425098e-04</td>\n",
       "      <td>1.080604e-02</td>\n",
       "      <td>9.885204e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426</th>\n",
       "      <td>646136b402e136422466a2acd8636630</td>\n",
       "      <td>0.999814</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>2.095642e-06</td>\n",
       "      <td>8.628173e-07</td>\n",
       "      <td>9.100798e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427</th>\n",
       "      <td>c7bea6491029f86ab3d1f0f9b599ca2c</td>\n",
       "      <td>0.011256</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>9.886510e-01</td>\n",
       "      <td>2.045167e-05</td>\n",
       "      <td>4.590128e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428</th>\n",
       "      <td>822fd3104f38cc6374f2996d3d6920a2</td>\n",
       "      <td>0.999822</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>1.475143e-07</td>\n",
       "      <td>1.319990e-06</td>\n",
       "      <td>1.148222e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3429 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              mac_hash        C0        C1            C2  \\\n",
       "0     b882f1d44602a25349a08f4a0af32977  0.117920  0.016511  8.648141e-01   \n",
       "1     86e644b498922f3a2fd0e6e1a1525de3  0.000002  0.000002  7.583204e-05   \n",
       "2     691662b04ee08015062d901a4c5628b1  0.999988  0.000011  4.486684e-07   \n",
       "3     52b5c510a28774237f4f118764c2ed6f  0.002449  0.997549  5.361618e-07   \n",
       "4     9f3c995e53d109f532056b6eae29a0b5  0.000377  0.066536  9.302961e-01   \n",
       "...                                ...       ...       ...           ...   \n",
       "3424  5001c0dfe522886ed884c0dc4e5848df  0.999814  0.000185  1.904194e-07   \n",
       "3425  f1c8781e701e89068fa0b8a62ecbc564  0.000011  0.000020  6.425098e-04   \n",
       "3426  646136b402e136422466a2acd8636630  0.999814  0.000183  2.095642e-06   \n",
       "3427  c7bea6491029f86ab3d1f0f9b599ca2c  0.011256  0.000068  9.886510e-01   \n",
       "3428  822fd3104f38cc6374f2996d3d6920a2  0.999822  0.000177  1.475143e-07   \n",
       "\n",
       "                C3            C4  \n",
       "0     6.357267e-04  1.189315e-04  \n",
       "1     4.581109e-03  9.953392e-01  \n",
       "2     4.272349e-07  8.559703e-08  \n",
       "3     1.487764e-06  1.237406e-07  \n",
       "4     1.792449e-03  9.980440e-04  \n",
       "...            ...           ...  \n",
       "3424  1.165337e-06  1.026893e-07  \n",
       "3425  1.080604e-02  9.885204e-01  \n",
       "3426  8.628173e-07  9.100798e-08  \n",
       "3427  2.045167e-05  4.590128e-06  \n",
       "3428  1.319990e-06  1.148222e-07  \n",
       "\n",
       "[3429 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame({'mac_hash':sample['mac_hash'], 'C0':y_prob[:,0], 'C1':y_prob[:,1], 'C2':y_prob[:,2], 'C3':y_prob[:,3], 'C4':y_prob[:,4]})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"output_Catboost.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#FF0000> LSTM (w/o group)</font> \t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (with validation set spiltted from training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 14, 20)            1760      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 20)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 20)                3280      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                672       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,877\n",
      "Trainable params: 5,877\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-15 21:00:46.001854: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-15 21:00:46.002053: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-15 21:00:46.002102: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-15 21:00:46.002148: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-15 21:00:46.002192: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-15 21:00:46.002237: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-15 21:00:46.002282: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-15 21:00:46.002327: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-15 21:00:46.002372: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-06-15 21:00:46.002379: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-15 21:00:46.002575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units = 20, activation='relu', return_sequences=True, input_shape = (X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units = 20, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "49/49 [==============================] - 2s 13ms/step - loss: 1.3574 - acc: 0.5191 - val_loss: 1.0115 - val_acc: 0.7126\n",
      "Epoch 2/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.7876 - acc: 0.6874 - val_loss: 0.4860 - val_acc: 0.7816\n",
      "Epoch 3/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.5685 - acc: 0.7563 - val_loss: 0.4119 - val_acc: 0.8003\n",
      "Epoch 4/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.4941 - acc: 0.7884 - val_loss: 0.3816 - val_acc: 0.8376\n",
      "Epoch 5/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.4597 - acc: 0.8106 - val_loss: 0.3425 - val_acc: 0.8664\n",
      "Epoch 6/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.4368 - acc: 0.8159 - val_loss: 0.3082 - val_acc: 0.8937\n",
      "Epoch 7/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.3997 - acc: 0.8328 - val_loss: 0.2791 - val_acc: 0.8951\n",
      "Epoch 8/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.3748 - acc: 0.8474 - val_loss: 0.2594 - val_acc: 0.9009\n",
      "Epoch 9/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.3697 - acc: 0.8531 - val_loss: 0.2718 - val_acc: 0.8922\n",
      "Epoch 10/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.3569 - acc: 0.8547 - val_loss: 0.2380 - val_acc: 0.9124\n",
      "Epoch 11/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.3332 - acc: 0.8641 - val_loss: 0.2424 - val_acc: 0.9080\n",
      "Epoch 12/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.3242 - acc: 0.8705 - val_loss: 0.2343 - val_acc: 0.9109\n",
      "Epoch 13/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2994 - acc: 0.8814 - val_loss: 0.2640 - val_acc: 0.8980\n",
      "Epoch 14/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2913 - acc: 0.8865 - val_loss: 0.2465 - val_acc: 0.9052\n",
      "Epoch 15/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2891 - acc: 0.8905 - val_loss: 0.2168 - val_acc: 0.9080\n",
      "Epoch 16/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2825 - acc: 0.8879 - val_loss: 0.2121 - val_acc: 0.9181\n",
      "Epoch 17/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2738 - acc: 0.8941 - val_loss: 0.3222 - val_acc: 0.8606\n",
      "Epoch 18/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2672 - acc: 0.8988 - val_loss: 0.1995 - val_acc: 0.9224\n",
      "Epoch 19/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2714 - acc: 0.8951 - val_loss: 0.1947 - val_acc: 0.9253\n",
      "Epoch 20/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2460 - acc: 0.9053 - val_loss: 0.1940 - val_acc: 0.9195\n",
      "Epoch 21/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2637 - acc: 0.9005 - val_loss: 0.2109 - val_acc: 0.9195\n",
      "Epoch 22/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2555 - acc: 0.9018 - val_loss: 0.1799 - val_acc: 0.9325\n",
      "Epoch 23/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2593 - acc: 0.9015 - val_loss: 0.1889 - val_acc: 0.9239\n",
      "Epoch 24/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2452 - acc: 0.9093 - val_loss: 0.1788 - val_acc: 0.9253\n",
      "Epoch 25/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2382 - acc: 0.9096 - val_loss: 0.1745 - val_acc: 0.9353\n",
      "Epoch 26/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2375 - acc: 0.9131 - val_loss: 0.2025 - val_acc: 0.9253\n",
      "Epoch 27/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2369 - acc: 0.9114 - val_loss: 0.2196 - val_acc: 0.9152\n",
      "Epoch 28/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2356 - acc: 0.9093 - val_loss: 0.1876 - val_acc: 0.9239\n",
      "Epoch 29/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2314 - acc: 0.9120 - val_loss: 0.1886 - val_acc: 0.9296\n",
      "Epoch 30/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2271 - acc: 0.9131 - val_loss: 0.2111 - val_acc: 0.9195\n",
      "Epoch 31/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2342 - acc: 0.9139 - val_loss: 0.1688 - val_acc: 0.9282\n",
      "Epoch 32/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2252 - acc: 0.9159 - val_loss: 0.1670 - val_acc: 0.9382\n",
      "Epoch 33/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2130 - acc: 0.9206 - val_loss: 0.1898 - val_acc: 0.9239\n",
      "Epoch 34/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2207 - acc: 0.9167 - val_loss: 0.1774 - val_acc: 0.9382\n",
      "Epoch 35/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2149 - acc: 0.9163 - val_loss: 0.2259 - val_acc: 0.9181\n",
      "Epoch 36/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2147 - acc: 0.9195 - val_loss: 0.1649 - val_acc: 0.9325\n",
      "Epoch 37/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2129 - acc: 0.9178 - val_loss: 0.1737 - val_acc: 0.9310\n",
      "Epoch 38/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2079 - acc: 0.9200 - val_loss: 0.1658 - val_acc: 0.9339\n",
      "Epoch 39/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2085 - acc: 0.9216 - val_loss: 0.1776 - val_acc: 0.9267\n",
      "Epoch 40/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2061 - acc: 0.9226 - val_loss: 0.1782 - val_acc: 0.9210\n",
      "Epoch 41/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2093 - acc: 0.9206 - val_loss: 0.1675 - val_acc: 0.9296\n",
      "Epoch 42/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2106 - acc: 0.9216 - val_loss: 0.1976 - val_acc: 0.9325\n",
      "Epoch 43/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2031 - acc: 0.9237 - val_loss: 0.1719 - val_acc: 0.9339\n",
      "Epoch 44/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2023 - acc: 0.9246 - val_loss: 0.1576 - val_acc: 0.9411\n",
      "Epoch 45/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2004 - acc: 0.9237 - val_loss: 0.2064 - val_acc: 0.9167\n",
      "Epoch 46/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1967 - acc: 0.9232 - val_loss: 0.1694 - val_acc: 0.9325\n",
      "Epoch 47/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1930 - acc: 0.9293 - val_loss: 0.1525 - val_acc: 0.9425\n",
      "Epoch 48/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1948 - acc: 0.9277 - val_loss: 0.1512 - val_acc: 0.9397\n",
      "Epoch 49/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1960 - acc: 0.9267 - val_loss: 0.1791 - val_acc: 0.9253\n",
      "Epoch 50/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1911 - acc: 0.9293 - val_loss: 0.1600 - val_acc: 0.9411\n",
      "Epoch 51/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1924 - acc: 0.9258 - val_loss: 0.2114 - val_acc: 0.9167\n",
      "Epoch 52/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1920 - acc: 0.9274 - val_loss: 0.1621 - val_acc: 0.9310\n",
      "Epoch 53/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1899 - acc: 0.9281 - val_loss: 0.1498 - val_acc: 0.9397\n",
      "Epoch 54/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1912 - acc: 0.9270 - val_loss: 0.1435 - val_acc: 0.9440\n",
      "Epoch 55/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1883 - acc: 0.9310 - val_loss: 0.1482 - val_acc: 0.9454\n",
      "Epoch 56/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1916 - acc: 0.9302 - val_loss: 0.1640 - val_acc: 0.9353\n",
      "Epoch 57/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1921 - acc: 0.9264 - val_loss: 0.1803 - val_acc: 0.9310\n",
      "Epoch 58/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1831 - acc: 0.9304 - val_loss: 0.1431 - val_acc: 0.9397\n",
      "Epoch 59/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1802 - acc: 0.9293 - val_loss: 0.1596 - val_acc: 0.9368\n",
      "Epoch 60/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1828 - acc: 0.9299 - val_loss: 0.1654 - val_acc: 0.9353\n",
      "Epoch 61/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1828 - acc: 0.9331 - val_loss: 0.1418 - val_acc: 0.9468\n",
      "Epoch 62/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1809 - acc: 0.9286 - val_loss: 0.1670 - val_acc: 0.9339\n",
      "Epoch 63/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1849 - acc: 0.9293 - val_loss: 0.1433 - val_acc: 0.9468\n",
      "Epoch 64/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1854 - acc: 0.9294 - val_loss: 0.1456 - val_acc: 0.9440\n",
      "Epoch 65/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1761 - acc: 0.9328 - val_loss: 0.1471 - val_acc: 0.9397\n",
      "Epoch 66/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1838 - acc: 0.9280 - val_loss: 0.1490 - val_acc: 0.9454\n",
      "Epoch 67/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1751 - acc: 0.9337 - val_loss: 0.1605 - val_acc: 0.9310\n",
      "Epoch 68/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1785 - acc: 0.9342 - val_loss: 0.1496 - val_acc: 0.9397\n",
      "Epoch 69/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1783 - acc: 0.9323 - val_loss: 0.1374 - val_acc: 0.9454\n",
      "Epoch 70/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1829 - acc: 0.9337 - val_loss: 0.1472 - val_acc: 0.9368\n",
      "Epoch 71/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1848 - acc: 0.9353 - val_loss: 0.1507 - val_acc: 0.9511\n",
      "Epoch 72/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1783 - acc: 0.9296 - val_loss: 0.1512 - val_acc: 0.9440\n",
      "Epoch 73/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1649 - acc: 0.9361 - val_loss: 0.2339 - val_acc: 0.9267\n",
      "Epoch 74/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1665 - acc: 0.9355 - val_loss: 0.1469 - val_acc: 0.9368\n",
      "Epoch 75/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1851 - acc: 0.9337 - val_loss: 0.1401 - val_acc: 0.9368\n",
      "Epoch 76/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1640 - acc: 0.9355 - val_loss: 0.1402 - val_acc: 0.9483\n",
      "Epoch 77/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1821 - acc: 0.9318 - val_loss: 0.1399 - val_acc: 0.9511\n",
      "Epoch 78/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1789 - acc: 0.9366 - val_loss: 0.1486 - val_acc: 0.9468\n",
      "Epoch 79/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1637 - acc: 0.9390 - val_loss: 0.1756 - val_acc: 0.9353\n",
      "Epoch 80/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1775 - acc: 0.9350 - val_loss: 0.1532 - val_acc: 0.9411\n",
      "Epoch 81/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1664 - acc: 0.9403 - val_loss: 0.1386 - val_acc: 0.9497\n",
      "Epoch 82/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1753 - acc: 0.9342 - val_loss: 0.1545 - val_acc: 0.9511\n",
      "Epoch 83/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1658 - acc: 0.9384 - val_loss: 0.1487 - val_acc: 0.9368\n",
      "Epoch 84/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1656 - acc: 0.9379 - val_loss: 0.1440 - val_acc: 0.9440\n",
      "Epoch 85/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1636 - acc: 0.9398 - val_loss: 0.1464 - val_acc: 0.9411\n",
      "Epoch 86/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1680 - acc: 0.9357 - val_loss: 0.1792 - val_acc: 0.9339\n",
      "Epoch 87/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1625 - acc: 0.9393 - val_loss: 0.1402 - val_acc: 0.9382\n",
      "Epoch 88/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1653 - acc: 0.9401 - val_loss: 0.1595 - val_acc: 0.9368\n",
      "Epoch 89/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1762 - acc: 0.9347 - val_loss: 0.1404 - val_acc: 0.9468\n",
      "Epoch 90/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1587 - acc: 0.9390 - val_loss: 0.1296 - val_acc: 0.9497\n",
      "Epoch 91/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1700 - acc: 0.9403 - val_loss: 0.1907 - val_acc: 0.9239\n",
      "Epoch 92/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1673 - acc: 0.9355 - val_loss: 0.1398 - val_acc: 0.9569\n",
      "Epoch 93/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1619 - acc: 0.9390 - val_loss: 0.1458 - val_acc: 0.9440\n",
      "Epoch 94/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1740 - acc: 0.9349 - val_loss: 0.1448 - val_acc: 0.9454\n",
      "Epoch 95/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1587 - acc: 0.9403 - val_loss: 0.1353 - val_acc: 0.9411\n",
      "Epoch 96/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1625 - acc: 0.9400 - val_loss: 0.1344 - val_acc: 0.9569\n",
      "Epoch 97/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1597 - acc: 0.9365 - val_loss: 0.1470 - val_acc: 0.9468\n",
      "Epoch 98/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1630 - acc: 0.9427 - val_loss: 0.1476 - val_acc: 0.9497\n",
      "Epoch 99/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1631 - acc: 0.9357 - val_loss: 0.1436 - val_acc: 0.9497\n",
      "Epoch 100/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1571 - acc: 0.9400 - val_loss: 0.1494 - val_acc: 0.9411\n",
      "Epoch 101/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1626 - acc: 0.9380 - val_loss: 0.1509 - val_acc: 0.9425\n",
      "Epoch 102/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1874 - acc: 0.9380 - val_loss: 0.1396 - val_acc: 0.9454\n",
      "Epoch 103/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1639 - acc: 0.9373 - val_loss: 0.1481 - val_acc: 0.9411\n",
      "Epoch 104/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1633 - acc: 0.9382 - val_loss: 0.1397 - val_acc: 0.9497\n",
      "Epoch 105/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1613 - acc: 0.9396 - val_loss: 0.1464 - val_acc: 0.9425\n",
      "Epoch 106/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1579 - acc: 0.9400 - val_loss: 0.1564 - val_acc: 0.9411\n",
      "Epoch 107/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1551 - acc: 0.9400 - val_loss: 0.1378 - val_acc: 0.9511\n",
      "Epoch 108/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1567 - acc: 0.9422 - val_loss: 0.1492 - val_acc: 0.9368\n",
      "Epoch 109/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1512 - acc: 0.9444 - val_loss: 0.1443 - val_acc: 0.9511\n",
      "Epoch 110/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1587 - acc: 0.9414 - val_loss: 0.1502 - val_acc: 0.9440\n",
      "Epoch 111/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1596 - acc: 0.9382 - val_loss: 0.4565 - val_acc: 0.8951\n",
      "Epoch 112/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1644 - acc: 0.9398 - val_loss: 0.1438 - val_acc: 0.9454\n",
      "Epoch 113/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1545 - acc: 0.9396 - val_loss: 0.1533 - val_acc: 0.9397\n",
      "Epoch 114/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1576 - acc: 0.9414 - val_loss: 0.1322 - val_acc: 0.9526\n",
      "Epoch 115/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1586 - acc: 0.9403 - val_loss: 0.1332 - val_acc: 0.9454\n",
      "Epoch 116/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1500 - acc: 0.9424 - val_loss: 0.1504 - val_acc: 0.9511\n",
      "Epoch 117/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1627 - acc: 0.9377 - val_loss: 0.1414 - val_acc: 0.9468\n",
      "Epoch 118/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1696 - acc: 0.9409 - val_loss: 0.1446 - val_acc: 0.9569\n",
      "Epoch 119/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1507 - acc: 0.9427 - val_loss: 0.1464 - val_acc: 0.9411\n",
      "Epoch 120/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1557 - acc: 0.9400 - val_loss: 0.1480 - val_acc: 0.9526\n",
      "Epoch 121/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1495 - acc: 0.9387 - val_loss: 0.1392 - val_acc: 0.9468\n",
      "Epoch 122/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1556 - acc: 0.9377 - val_loss: 0.1389 - val_acc: 0.9454\n",
      "Epoch 123/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1546 - acc: 0.9427 - val_loss: 0.1868 - val_acc: 0.9181\n",
      "Epoch 124/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1687 - acc: 0.9388 - val_loss: 0.1427 - val_acc: 0.9583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1497 - acc: 0.9411 - val_loss: 0.1271 - val_acc: 0.9569\n",
      "Epoch 126/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1497 - acc: 0.9422 - val_loss: 0.1349 - val_acc: 0.9497\n",
      "Epoch 127/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1512 - acc: 0.9452 - val_loss: 0.1408 - val_acc: 0.9483\n",
      "Epoch 128/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1532 - acc: 0.9417 - val_loss: 0.1365 - val_acc: 0.9411\n",
      "Epoch 129/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1612 - acc: 0.9436 - val_loss: 0.1466 - val_acc: 0.9425\n",
      "Epoch 130/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1477 - acc: 0.9452 - val_loss: 0.1415 - val_acc: 0.9511\n",
      "Epoch 131/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1518 - acc: 0.9408 - val_loss: 0.1326 - val_acc: 0.9440\n",
      "Epoch 132/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1511 - acc: 0.9419 - val_loss: 0.1429 - val_acc: 0.9511\n",
      "Epoch 133/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1460 - acc: 0.9400 - val_loss: 0.1299 - val_acc: 0.9440\n",
      "Epoch 134/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1495 - acc: 0.9427 - val_loss: 0.1267 - val_acc: 0.9526\n",
      "Epoch 135/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1580 - acc: 0.9412 - val_loss: 0.1244 - val_acc: 0.9583\n",
      "Epoch 136/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1439 - acc: 0.9438 - val_loss: 0.1275 - val_acc: 0.9468\n",
      "Epoch 137/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1514 - acc: 0.9387 - val_loss: 0.1410 - val_acc: 0.9555\n",
      "Epoch 138/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2930 - acc: 0.9392 - val_loss: 0.1344 - val_acc: 0.9497\n",
      "Epoch 139/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1453 - acc: 0.9440 - val_loss: 0.1629 - val_acc: 0.9353\n",
      "Epoch 140/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1522 - acc: 0.9414 - val_loss: 0.1450 - val_acc: 0.9468\n",
      "Epoch 141/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1513 - acc: 0.9393 - val_loss: 0.1371 - val_acc: 0.9497\n",
      "Epoch 142/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1406 - acc: 0.9441 - val_loss: 0.1449 - val_acc: 0.9454\n",
      "Epoch 143/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1510 - acc: 0.9457 - val_loss: 0.1359 - val_acc: 0.9454\n",
      "Epoch 144/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1447 - acc: 0.9424 - val_loss: 0.1453 - val_acc: 0.9497\n",
      "Epoch 145/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1513 - acc: 0.9433 - val_loss: 0.1476 - val_acc: 0.9382\n",
      "Epoch 146/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1428 - acc: 0.9457 - val_loss: 0.1418 - val_acc: 0.9497\n",
      "Epoch 147/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1457 - acc: 0.9456 - val_loss: 0.1340 - val_acc: 0.9440\n",
      "Epoch 148/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1432 - acc: 0.9441 - val_loss: 0.1393 - val_acc: 0.9468\n",
      "Epoch 149/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1525 - acc: 0.9416 - val_loss: 0.1462 - val_acc: 0.9454\n",
      "Epoch 150/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1538 - acc: 0.9468 - val_loss: 0.1491 - val_acc: 0.9411\n",
      "Epoch 151/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1403 - acc: 0.9468 - val_loss: 0.1406 - val_acc: 0.9483\n",
      "Epoch 152/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1409 - acc: 0.9444 - val_loss: 0.1405 - val_acc: 0.9483\n",
      "Epoch 153/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1391 - acc: 0.9460 - val_loss: 0.1538 - val_acc: 0.9497\n",
      "Epoch 154/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1469 - acc: 0.9432 - val_loss: 0.1308 - val_acc: 0.9468\n",
      "Epoch 155/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1373 - acc: 0.9495 - val_loss: 0.1348 - val_acc: 0.9468\n",
      "Epoch 156/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1467 - acc: 0.9440 - val_loss: 0.1369 - val_acc: 0.9468\n",
      "Epoch 157/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1464 - acc: 0.9425 - val_loss: 0.1322 - val_acc: 0.9411\n",
      "Epoch 158/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1394 - acc: 0.9438 - val_loss: 0.1366 - val_acc: 0.9483\n",
      "Epoch 159/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1453 - acc: 0.9471 - val_loss: 0.1318 - val_acc: 0.9511\n",
      "Epoch 160/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1356 - acc: 0.9471 - val_loss: 0.1486 - val_acc: 0.9468\n",
      "Epoch 161/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1476 - acc: 0.9452 - val_loss: 0.1440 - val_acc: 0.9440\n",
      "Epoch 162/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1394 - acc: 0.9468 - val_loss: 0.1431 - val_acc: 0.9382\n",
      "Epoch 163/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1453 - acc: 0.9416 - val_loss: 0.1351 - val_acc: 0.9555\n",
      "Epoch 164/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1433 - acc: 0.9456 - val_loss: 0.1352 - val_acc: 0.9511\n",
      "Epoch 165/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.2141 - acc: 0.9420 - val_loss: 0.1312 - val_acc: 0.9555\n",
      "Epoch 166/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1484 - acc: 0.9478 - val_loss: 0.1325 - val_acc: 0.9555\n",
      "Epoch 167/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1475 - acc: 0.9454 - val_loss: 0.1348 - val_acc: 0.9526\n",
      "Epoch 168/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1413 - acc: 0.9444 - val_loss: 0.1353 - val_acc: 0.9540\n",
      "Epoch 169/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1444 - acc: 0.9435 - val_loss: 0.1284 - val_acc: 0.9526\n",
      "Epoch 170/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1397 - acc: 0.9441 - val_loss: 0.1276 - val_acc: 0.9540\n",
      "Epoch 171/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1412 - acc: 0.9475 - val_loss: 0.1370 - val_acc: 0.9483\n",
      "Epoch 172/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1494 - acc: 0.9440 - val_loss: 0.1419 - val_acc: 0.9540\n",
      "Epoch 173/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1442 - acc: 0.9454 - val_loss: 0.1294 - val_acc: 0.9468\n",
      "Epoch 174/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1392 - acc: 0.9470 - val_loss: 0.1408 - val_acc: 0.9555\n",
      "Epoch 175/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1339 - acc: 0.9492 - val_loss: 0.1347 - val_acc: 0.9526\n",
      "Epoch 176/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1404 - acc: 0.9468 - val_loss: 0.1235 - val_acc: 0.9598\n",
      "Epoch 177/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1386 - acc: 0.9459 - val_loss: 0.1279 - val_acc: 0.9540\n",
      "Epoch 178/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1371 - acc: 0.9478 - val_loss: 0.1332 - val_acc: 0.9511\n",
      "Epoch 179/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1455 - acc: 0.9467 - val_loss: 0.1289 - val_acc: 0.9569\n",
      "Epoch 180/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1372 - acc: 0.9465 - val_loss: 0.1283 - val_acc: 0.9497\n",
      "Epoch 181/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1333 - acc: 0.9465 - val_loss: 0.1369 - val_acc: 0.9483\n",
      "Epoch 182/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1355 - acc: 0.9465 - val_loss: 0.1316 - val_acc: 0.9526\n",
      "Epoch 183/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1389 - acc: 0.9452 - val_loss: 0.1317 - val_acc: 0.9555\n",
      "Epoch 184/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1341 - acc: 0.9476 - val_loss: 0.1287 - val_acc: 0.9483\n",
      "Epoch 185/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1443 - acc: 0.9451 - val_loss: 0.1268 - val_acc: 0.9497\n",
      "Epoch 186/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1455 - acc: 0.9470 - val_loss: 0.1355 - val_acc: 0.9440\n",
      "Epoch 187/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1602 - acc: 0.9492 - val_loss: 0.1298 - val_acc: 0.9497\n",
      "Epoch 188/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1337 - acc: 0.9467 - val_loss: 0.1529 - val_acc: 0.9497\n",
      "Epoch 189/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1463 - acc: 0.9430 - val_loss: 0.1295 - val_acc: 0.9468\n",
      "Epoch 190/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1346 - acc: 0.9486 - val_loss: 0.1247 - val_acc: 0.9526\n",
      "Epoch 191/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1348 - acc: 0.9515 - val_loss: 0.1973 - val_acc: 0.9310\n",
      "Epoch 192/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1417 - acc: 0.9452 - val_loss: 0.1313 - val_acc: 0.9511\n",
      "Epoch 193/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1530 - acc: 0.9468 - val_loss: 0.1264 - val_acc: 0.9526\n",
      "Epoch 194/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1304 - acc: 0.9495 - val_loss: 0.1221 - val_acc: 0.9526\n",
      "Epoch 195/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1409 - acc: 0.9475 - val_loss: 0.1287 - val_acc: 0.9555\n",
      "Epoch 196/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1321 - acc: 0.9492 - val_loss: 0.1236 - val_acc: 0.9569\n",
      "Epoch 197/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1373 - acc: 0.9475 - val_loss: 0.1212 - val_acc: 0.9526\n",
      "Epoch 198/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1288 - acc: 0.9484 - val_loss: 0.1212 - val_acc: 0.9626\n",
      "Epoch 199/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1431 - acc: 0.9468 - val_loss: 0.1268 - val_acc: 0.9468\n",
      "Epoch 200/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1337 - acc: 0.9486 - val_loss: 0.1530 - val_acc: 0.9411\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'RMSprop', loss = 'sparse_categorical_crossentropy',  metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs = 200, batch_size = 128, verbose = 1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training - Loss Function')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.subplot()\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Training - Loss Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (train with  whole training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "55/55 [==============================] - 2s 6ms/step - loss: 1.3647 - acc: 0.5465\n",
      "Epoch 2/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.8353 - acc: 0.6538\n",
      "Epoch 3/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.7009 - acc: 0.6869\n",
      "Epoch 4/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.6358 - acc: 0.7181\n",
      "Epoch 5/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5917 - acc: 0.7448\n",
      "Epoch 6/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.5445 - acc: 0.7665\n",
      "Epoch 7/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4886 - acc: 0.7998\n",
      "Epoch 8/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4322 - acc: 0.8166\n",
      "Epoch 9/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.4061 - acc: 0.8368\n",
      "Epoch 10/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3920 - acc: 0.8362\n",
      "Epoch 11/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3747 - acc: 0.8491\n",
      "Epoch 12/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3578 - acc: 0.8583\n",
      "Epoch 13/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3472 - acc: 0.8602\n",
      "Epoch 14/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3385 - acc: 0.8685\n",
      "Epoch 15/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3267 - acc: 0.8689\n",
      "Epoch 16/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3119 - acc: 0.8781\n",
      "Epoch 17/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.3126 - acc: 0.8787\n",
      "Epoch 18/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2904 - acc: 0.8922\n",
      "Epoch 19/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2870 - acc: 0.8883\n",
      "Epoch 20/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2821 - acc: 0.8888\n",
      "Epoch 21/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2743 - acc: 0.8944\n",
      "Epoch 22/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2755 - acc: 0.8948\n",
      "Epoch 23/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2633 - acc: 0.8968\n",
      "Epoch 24/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2580 - acc: 0.9014\n",
      "Epoch 25/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2567 - acc: 0.9016\n",
      "Epoch 26/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2505 - acc: 0.9076\n",
      "Epoch 27/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2496 - acc: 0.9021\n",
      "Epoch 28/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2401 - acc: 0.9090\n",
      "Epoch 29/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2410 - acc: 0.9075\n",
      "Epoch 30/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2432 - acc: 0.9092\n",
      "Epoch 31/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2437 - acc: 0.9085\n",
      "Epoch 32/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2281 - acc: 0.9135\n",
      "Epoch 33/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2257 - acc: 0.9115\n",
      "Epoch 34/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2223 - acc: 0.9141\n",
      "Epoch 35/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2275 - acc: 0.9121\n",
      "Epoch 36/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2216 - acc: 0.9179\n",
      "Epoch 37/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2185 - acc: 0.9159\n",
      "Epoch 38/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2157 - acc: 0.9167\n",
      "Epoch 39/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2064 - acc: 0.9240\n",
      "Epoch 40/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2143 - acc: 0.9178\n",
      "Epoch 41/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2100 - acc: 0.9190\n",
      "Epoch 42/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2033 - acc: 0.9251\n",
      "Epoch 43/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2062 - acc: 0.9169\n",
      "Epoch 44/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2040 - acc: 0.9187\n",
      "Epoch 45/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2087 - acc: 0.9207\n",
      "Epoch 46/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2038 - acc: 0.9217\n",
      "Epoch 47/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.2080 - acc: 0.9233\n",
      "Epoch 48/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1973 - acc: 0.9251\n",
      "Epoch 49/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1926 - acc: 0.9297\n",
      "Epoch 50/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1976 - acc: 0.9234\n",
      "Epoch 51/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1933 - acc: 0.9257\n",
      "Epoch 52/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1950 - acc: 0.9241\n",
      "Epoch 53/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1931 - acc: 0.9208\n",
      "Epoch 54/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1874 - acc: 0.9290\n",
      "Epoch 55/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1899 - acc: 0.9276\n",
      "Epoch 56/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1890 - acc: 0.9241\n",
      "Epoch 57/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1917 - acc: 0.9293\n",
      "Epoch 58/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1851 - acc: 0.9296\n",
      "Epoch 59/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1824 - acc: 0.9304\n",
      "Epoch 60/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1807 - acc: 0.9283\n",
      "Epoch 61/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1932 - acc: 0.9284\n",
      "Epoch 62/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1806 - acc: 0.9304\n",
      "Epoch 63/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1821 - acc: 0.9316\n",
      "Epoch 64/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1818 - acc: 0.9306\n",
      "Epoch 65/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1754 - acc: 0.9322\n",
      "Epoch 66/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1766 - acc: 0.9322\n",
      "Epoch 67/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1750 - acc: 0.9359\n",
      "Epoch 68/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1770 - acc: 0.9322\n",
      "Epoch 69/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1782 - acc: 0.9315\n",
      "Epoch 70/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1753 - acc: 0.9306\n",
      "Epoch 71/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1746 - acc: 0.9339\n",
      "Epoch 72/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1773 - acc: 0.9312\n",
      "Epoch 73/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1770 - acc: 0.9327\n",
      "Epoch 74/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1727 - acc: 0.9329\n",
      "Epoch 75/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1812 - acc: 0.9306\n",
      "Epoch 76/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1784 - acc: 0.9332\n",
      "Epoch 77/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1743 - acc: 0.9362\n",
      "Epoch 78/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1712 - acc: 0.9350\n",
      "Epoch 79/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1734 - acc: 0.9322\n",
      "Epoch 80/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1716 - acc: 0.9350\n",
      "Epoch 81/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1738 - acc: 0.9329\n",
      "Epoch 82/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1682 - acc: 0.9376\n",
      "Epoch 83/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1675 - acc: 0.9359\n",
      "Epoch 84/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1648 - acc: 0.9379\n",
      "Epoch 85/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1670 - acc: 0.9373\n",
      "Epoch 86/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1673 - acc: 0.9349\n",
      "Epoch 87/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1743 - acc: 0.9352\n",
      "Epoch 88/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1660 - acc: 0.9369\n",
      "Epoch 89/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1611 - acc: 0.9353\n",
      "Epoch 90/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1681 - acc: 0.9362\n",
      "Epoch 91/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1645 - acc: 0.9372\n",
      "Epoch 92/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1629 - acc: 0.9352\n",
      "Epoch 93/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1642 - acc: 0.9346\n",
      "Epoch 94/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1633 - acc: 0.9373\n",
      "Epoch 95/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1635 - acc: 0.9386\n",
      "Epoch 96/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1624 - acc: 0.9382\n",
      "Epoch 97/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1665 - acc: 0.9348\n",
      "Epoch 98/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1620 - acc: 0.9378\n",
      "Epoch 99/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1597 - acc: 0.9378\n",
      "Epoch 100/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1575 - acc: 0.9412\n",
      "Epoch 101/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1553 - acc: 0.9385\n",
      "Epoch 102/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1597 - acc: 0.9386\n",
      "Epoch 103/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1619 - acc: 0.9361\n",
      "Epoch 104/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1583 - acc: 0.9407\n",
      "Epoch 105/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1536 - acc: 0.9424\n",
      "Epoch 106/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1608 - acc: 0.9381\n",
      "Epoch 107/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1649 - acc: 0.9382\n",
      "Epoch 108/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1577 - acc: 0.9378\n",
      "Epoch 109/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1575 - acc: 0.9389\n",
      "Epoch 110/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1636 - acc: 0.9391\n",
      "Epoch 111/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1590 - acc: 0.9375\n",
      "Epoch 112/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1547 - acc: 0.9417\n",
      "Epoch 113/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1562 - acc: 0.9396\n",
      "Epoch 114/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1642 - acc: 0.9411\n",
      "Epoch 115/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1530 - acc: 0.9431\n",
      "Epoch 116/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1521 - acc: 0.9424\n",
      "Epoch 117/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1496 - acc: 0.9408\n",
      "Epoch 118/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1593 - acc: 0.9396\n",
      "Epoch 119/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1466 - acc: 0.9444\n",
      "Epoch 120/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1522 - acc: 0.9405\n",
      "Epoch 121/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1531 - acc: 0.9422\n",
      "Epoch 122/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1569 - acc: 0.9389\n",
      "Epoch 123/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1554 - acc: 0.9389\n",
      "Epoch 124/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1503 - acc: 0.9409\n",
      "Epoch 125/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1517 - acc: 0.9401\n",
      "Epoch 126/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1497 - acc: 0.9411\n",
      "Epoch 127/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1455 - acc: 0.9430\n",
      "Epoch 128/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1465 - acc: 0.9434\n",
      "Epoch 129/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1504 - acc: 0.9418\n",
      "Epoch 130/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1506 - acc: 0.9435\n",
      "Epoch 131/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1538 - acc: 0.9417\n",
      "Epoch 132/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1474 - acc: 0.9425\n",
      "Epoch 133/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1637 - acc: 0.9412\n",
      "Epoch 134/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1440 - acc: 0.9407\n",
      "Epoch 135/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1489 - acc: 0.9404\n",
      "Epoch 136/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1522 - acc: 0.9424\n",
      "Epoch 137/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1422 - acc: 0.9435\n",
      "Epoch 138/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1483 - acc: 0.9415\n",
      "Epoch 139/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1464 - acc: 0.9427\n",
      "Epoch 140/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1534 - acc: 0.9398\n",
      "Epoch 141/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1500 - acc: 0.9417\n",
      "Epoch 142/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1469 - acc: 0.9430\n",
      "Epoch 143/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1464 - acc: 0.9457\n",
      "Epoch 144/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1463 - acc: 0.9451\n",
      "Epoch 145/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1498 - acc: 0.9414\n",
      "Epoch 146/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1494 - acc: 0.9427\n",
      "Epoch 147/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1451 - acc: 0.9427\n",
      "Epoch 148/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1473 - acc: 0.9457\n",
      "Epoch 149/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1475 - acc: 0.9432\n",
      "Epoch 150/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1402 - acc: 0.9468\n",
      "Epoch 151/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1472 - acc: 0.9437\n",
      "Epoch 152/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1421 - acc: 0.9477\n",
      "Epoch 153/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1417 - acc: 0.9441\n",
      "Epoch 154/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1428 - acc: 0.9463\n",
      "Epoch 155/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1492 - acc: 0.9427\n",
      "Epoch 156/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1475 - acc: 0.9432\n",
      "Epoch 157/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1471 - acc: 0.9428\n",
      "Epoch 158/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1454 - acc: 0.9424\n",
      "Epoch 159/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1414 - acc: 0.9427\n",
      "Epoch 160/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1461 - acc: 0.9442\n",
      "Epoch 161/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1485 - acc: 0.9430\n",
      "Epoch 162/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1434 - acc: 0.9424\n",
      "Epoch 163/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1408 - acc: 0.9460\n",
      "Epoch 164/200\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.1393 - acc: 0.9461\n",
      "Epoch 165/200\n",
      "55/55 [==============================] - 0s 7ms/step - loss: 0.1448 - acc: 0.9425\n",
      "Epoch 166/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1371 - acc: 0.9474\n",
      "Epoch 167/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1406 - acc: 0.9441\n",
      "Epoch 168/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1413 - acc: 0.9431\n",
      "Epoch 169/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1338 - acc: 0.9497\n",
      "Epoch 170/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1481 - acc: 0.9442\n",
      "Epoch 171/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1363 - acc: 0.9461\n",
      "Epoch 172/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1437 - acc: 0.9440\n",
      "Epoch 173/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1404 - acc: 0.9470\n",
      "Epoch 174/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1423 - acc: 0.9460\n",
      "Epoch 175/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1353 - acc: 0.9467\n",
      "Epoch 176/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1342 - acc: 0.9473\n",
      "Epoch 177/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1414 - acc: 0.9442\n",
      "Epoch 178/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1383 - acc: 0.9442\n",
      "Epoch 179/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1344 - acc: 0.9480\n",
      "Epoch 180/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1381 - acc: 0.9465\n",
      "Epoch 181/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1408 - acc: 0.9447\n",
      "Epoch 182/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1399 - acc: 0.9450\n",
      "Epoch 183/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1365 - acc: 0.9486\n",
      "Epoch 184/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1343 - acc: 0.9478\n",
      "Epoch 185/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1385 - acc: 0.9463\n",
      "Epoch 186/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1359 - acc: 0.9481\n",
      "Epoch 187/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1433 - acc: 0.9440\n",
      "Epoch 188/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1349 - acc: 0.9473\n",
      "Epoch 189/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1339 - acc: 0.9475\n",
      "Epoch 190/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1338 - acc: 0.9454\n",
      "Epoch 191/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1493 - acc: 0.9445\n",
      "Epoch 192/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1360 - acc: 0.9475\n",
      "Epoch 193/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1436 - acc: 0.9415\n",
      "Epoch 194/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1372 - acc: 0.9486\n",
      "Epoch 195/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1372 - acc: 0.9480\n",
      "Epoch 196/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1348 - acc: 0.9470\n",
      "Epoch 197/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1314 - acc: 0.9496\n",
      "Epoch 198/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1385 - acc: 0.9427\n",
      "Epoch 199/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1400 - acc: 0.9444\n",
      "Epoch 200/200\n",
      "55/55 [==============================] - 0s 6ms/step - loss: 0.1368 - acc: 0.9467\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units = 20, activation='relu', return_sequences=True, input_shape = (X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units = 20, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer = 'RMSprop', loss = 'sparse_categorical_crossentropy',  metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs = 200, batch_size = 128, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training - Loss Function')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATwAAAE/CAYAAADbkX+oAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqC0lEQVR4nO3deXxU5d338c9vJhvZWJKwhhBWFQWRRVDcaje0VaytrVbrUq1tH+3y2N697d3FLrePtau1tXpT69paa3eqtlq9RXFDEREVBNkJW0IIEAgh2+/5YyY4AkkmszCc5Pt+vfLKzJkz57rOmcw313Wus5i7IyLSG4QyXQERkcNFgScivYYCT0R6DQWeiPQaCjwR6TUUeCLSayjweigz+6eZXZbqeSU1zKzCzHabWTjTdelNTMfhHTnMbHfM03xgH9Aaff5Zd//d4a9VepmZA2PdfeVhLnceMANoiZn8fnd/IU3lrQWucvcn0rF8iU9Wpisg73D3wvbHnX1BzCzL3VsOnC7ddq2735npSsjhoy5tAJjZGWZWZWb/aWZbgLvNrL+ZPWxmNWZWF31cHvOeeWZ2VfTx5Wb2rJn9ODrvGjM7K8F5R5rZM2ZWb2ZPmNltZvbbNKxzXzO7L7p+68zsm2YWir42xsyeNrOdZrbNzP4QnW5m9jMzqzazXWb2upkd181y92+L6PPLzezZmOduZp8zs7fNbEd0/S3m9c+Y2bLo9llqZpPN7H6gAvhHtBv7NTOrjC4rK/q+oWY218y2m9lKM/tMzDK/Y2YPRbdHvZm9aWZTE922vZkCLzgGAwOAEcDVRD67u6PPK4C9wC87ef90YDlQCvwQ+E3sF7Ub8z4AvASUAN8BPpXwGnXuF0BfYBRwOnApcEX0te8DjwP9gfLovAAfAE4DxkXf+3GgNg11+zAwDZgYLeODAGZ2AZFtcilQDJwL1Lr7p4D1wDnuXujuPzzEMh8EqoChwMeA/2dmZ8a8fm50nn7AXDr/rKUDCrzgaANucPd97r7X3Wvd/c/u3uDu9cCNRIKhI+vc/dfu3grcCwwBBnVnXjOrIPJF/7a7N7n7s0S+fCkV3ZF/IfB1d69397XAT3gnXJuJBP1Qd2+M1qN9ehFwNJH908vcfXMnRd0abaXtMLNF3ajiD9x9h7uvB54CJkWnXwX80N1f9oiV7r4ujvUdDswE/jO6PouBO4kEZ7tn3f3R6GdyP3B8N+orUQq84Khx98b2J2aWb2b/E+3u7QKeAfp1Muq3pf2BuzdEHxZ2c96hwPaYaQAbOqpwdPR3d/Tn4g7X7GClQDYQGxbrgGHRx18DDHgp2r37dLSu/0uk5XMbUG1mc8ysuJNyvuju/aI/k7tRvy0xjxt4ZzsOB1Z1Yznt2rdrfcy02PU9VJl57d1hiZ8CLzgOHE7/CnAUMN3di4l05SASBOmyGRhgZvkx04Z3NLO7nxXtwhV2c4R5G++04tpVABujy93i7p9x96HAZ4FfmdmY6Gu3uvsUYDyRru1/dKNcgD1ERsjbDe7GezcAozt4rbPDITYR2a5FMdP2r6+kjgIvuIqI7LfbYWYDgBvSXWC0e7YQ+I6Z5ZjZScA5KVh0jpnltf9Epz0E3GhmRWY2ArgO+C1E9pXFDNDUEQmTNjObZmbTzSybSHA1EtkV0B2LgfOjLegxwJXdeO+dwFfNbEp0AGVMtO4AW4nsjzyIu28Angduim6DidFyUz4Y1Nsp8ILrFqAPkdbQi8C/DlO5FwMnERkM+G/gD0SOF0zGm0TCu/3nCuALREJrNfAskcGSu6LzTwMWWOS4xbnAl9x9NZGBgl8TCcF10Tr+qJt1+RnQRCSg7gXibpm6+x+J7Et9AKgH/kZkoAngJuCb0f2FXz3E2y8CKom09v5KZH+tjtlLMR14LEmJHhLylrunvYUpkiy18KRbot3G0WYWMrNZwGwiLRmRI55GeaS7BgN/IXIcXhXweXd/NbNVEomPurQi0muoSysivYYCT0R6jYztwystLfXKyspMFS8iPdQrr7yyzd3LDvVal4FnZncROVm62t07vPKEmU0DXgAudPc/dbXcyspKFi5c2NVsIiLdYmYdnr8cT5f2HmBWFwWEgZuJXMFCROSI1GXgufszwPYuZvsC8GegOhWVEhFJh6QHLcxsGPAR4PbkqyMikj6pGLS4hch1vNo6vp5khJldTeTilVRUVKSgaBE5lObmZqqqqmhsbOx65oDKy8ujvLyc7OzsuN+TisCbCjwYDbtS4Gwza3H3vx04o7vPAeYATJ06VUc8i6RJVVUVRUVFVFZW0lVDJIjcndraWqqqqhg5cmTc70s68Nx9f2lmdg/w8KHCTkQOn8bGxh4bdgBmRklJCTU1Nd16XzyHpfweOAMoNbMqItddywZw9zu6X1URORx6ati1S2T9ugw8d78o3oW5++XdroGI9EiFhYXs3r276xkPI51aJiK9RmAC7++LN/L8qm2ZroaIJGHx4sXMmDGDiRMn8pGPfIS6ujoAbr31VsaPH8/EiRO58MILAXj66aeZNGkSkyZN4oQTTqC+vr6zRcclMIH3o8eW86eFVZmuhogk4dJLL+Xmm29myZIlTJgwge9+97sA/OAHP+DVV19lyZIl3HFHZGjgxz/+MbfddhuLFy9m/vz59OnTJ+nyA3MB0HDIaNW1+0S67bv/eJOlm3aldJnjhxZzwznHdus9O3fuZMeOHZx+euT2yZdddhkXXHABABMnTuTiiy/mvPPO47zzzgNg5syZXHfddVx88cWcf/75lJeXd7TouAWmhRc2o7VNgSfSEz3yyCNcc801LFq0iGnTptHS0sL111/PnXfeyd69e5k5cyZvvfVW0uUEpoUXChltauGJdFt3W2Lp0rdvX/r378/8+fM59dRTuf/++zn99NNpa2tjw4YNvOc97+GUU07hwQcfZPfu3dTW1jJhwgQmTJjAyy+/zFtvvcXRRx+dVB0CE3hq4YkES0NDw7u6oddddx333nsvn/vc52hoaGDUqFHcfffdtLa2cskll7Bz507cnS9+8Yv069ePb33rWzz11FOEQiGOPfZYzjrrrKTrFJjAC4WM1u7eUllEMqat7dBf2BdffPGgac8+++xB037xi1+kvE7B2YcXQl1aEUlKcAJPXVoRSVJgAk+DFiKSrMAEnlp4It3T0+85ncj6BSfwQkaLAk8kLnl5edTW1vbY0Gu/Hl5eXl633heYUdpwyGhq0TCtSDzKy8upqqrq9vXigqT9isfdEajA06llIvHJzs7u1pWAe4vAdGlDZrSpSysiSQhM4KmFJyLJCkzghUxnWohIcgITeOEQ6tKKSFICFHjq0opIcgITeBq0EJFkBSbw1MITkWQFJ/B0apmIJCkwgRcKqUsrIskJTOBl6VxaEUlSYAJPl4cSkWQFJvC0D09EkhWcwAsp8EQkOYEJvJAZyjsRSUZgAi8cQi08EUlKYAIvpAOPRSRJgQm8sE4tE5EkdRl4ZnaXmVWb2RsdvH6xmS0xs9fN7HkzOz711dSpZSKSvHhaePcAszp5fQ1wurtPAL4PzElBvQ4SMsO959+JSUTSp8vAc/dngO2dvP68u9dFn74IdO+uGnEKhwzQwIWIJC7V+/CuBP7Z0YtmdrWZLTSzhd29m9L+wFMLT0QSlLLAM7P3EAm8/+xoHnef4+5T3X1qWVlZt5Yfskjgteky7yKSoJTcptHMJgJ3Ame5e20qlnmgrGgLr6WtDQinowgR6eGSbuGZWQXwF+BT7r4i+SodWiikFp6IJKfLFp6Z/R44Ayg1syrgBiAbwN3vAL4NlAC/ski3s8Xdp6a6ouFI3mkfnogkrMvAc/eLunj9KuCqlNWoAxqlFZFkBeZMi/1dWrXwRCRBgQm8sKmFJyLJCUzghdSlFZEkBSbw2lt46tKKSKKCE3hq4YlIkgITeBq0EJFkBSbw3hm0yHBFRCSwghN40ZqqSysiiQpQ4EWqqsATkUQFKPAiv3VqmYgkKjCBF9KBxyKSpMAEXlijtCKSpOAEnlp4IpKkwATeO9fDU+CJSGICE3i6p4WIJCswgadBCxFJVmACT4MWIpKs4ASeTi0TkSQFJvBCOrVMRJIUmMBTl1ZEkhWcwNOghYgkKTiBpwuAikiSFHgi0msEJvD2H4enfXgikqDABF5Yp5aJSJICF3hq4YlIogITeO1dWrXwRCRRgQk8DVqISLKCE3j7By0yXBERCazABF77qWXq0opIogITeBq0EJFkdRl4ZnaXmVWb2RsdvG5mdquZrTSzJWY2OfXV1PXwRCR58bTw7gFmdfL6WcDY6M/VwO3JV+tgOg5PRJLVZeC5+zPA9k5mmQ3c5xEvAv3MbEiqKtiufdCiRYEnIglKxT68YcCGmOdV0WkHMbOrzWyhmS2sqanpViGhkGGmy0OJSOIO66CFu89x96nuPrWsrKzb7w+baR+eiCQsFYG3ERge87w8Oi3lQiHTKK2IJCwVgTcXuDQ6WjsD2Onum1Ow3IOEzTRoISIJy+pqBjP7PXAGUGpmVcANQDaAu98BPAqcDawEGoAr0lXZcMh0Ex8RSViXgefuF3XxugPXpKxGnQhp0EJEkhCYMy2gvYWnwBORxAQv8NTCE5EEBSrwQhq0EJEkBCrw1KUVkWQEKvBCpi6tiCQuUIEXDqlLKyKJC1TgZYVMFw8QkYQFKvBCIdNxeCKSsEAFni4eICLJCFTghXRqmYgkIVCBFw7p1DIRSVywAk9dWhFJQqACT4MWIpKMQAWeWngikoxABV5Ip5aJSBICFXhhU5dWRBIXrMBTC09EkhCowIvcxCfTtRCRoApU4IUNWtt05LGIJCZYgRcK6UwLEUlYwAIPXR5KRBIWsMDTBUBFJHGBCjzd00JEkhGowFMLT0SSEazA06llIpKEQAVeSPe0EJEkBCrwwrprmYgkIVCBlxU2WnSqhYgkKFCBl58TpqGpNdPVEJGACljgZbG3uVUDFyKSkEAFXkFuGIC9zWrliUj3xRV4ZjbLzJab2Uozu/4Qr1eY2VNm9qqZLTGzs1NfVSjIzQKgYV9LOhYvIj1cl4FnZmHgNuAsYDxwkZmNP2C2bwIPufsJwIXAr1JdUYCCnEjg7dF+PBFJQDwtvBOBle6+2t2bgAeB2QfM40Bx9HFfYFPqqviO/JxIl3aPWngikoB4Am8YsCHmeVV0WqzvAJeYWRXwKPCFQy3IzK42s4VmtrCmpqbblW3v0irwRCQRqRq0uAi4x93LgbOB+83soGW7+xx3n+ruU8vKyrpdSHsLT4emiEgi4gm8jcDwmOfl0WmxrgQeAnD3F4A8oDQVFYy1v4XXpBaeiHRfPIH3MjDWzEaaWQ6RQYm5B8yzHngvgJkdQyTwut9n7cL+Ft4+tfBEpPu6DDx3bwGuBR4DlhEZjX3TzL5nZudGZ/sK8Bkzew34PXC5e+pPen1nlFYtPBHpvqx4ZnL3R4kMRsRO+3bM46XAzNRW7WD7j8PTPjwRSUCgzrTIyQqRHTaN0opIQgIVeBA5n1YtPBFJROACryAnzG618EQkAYELvPzcLBo0aCEiCQhc4BXkhNmjw1JEJAGBC7zIPjy18ESk+wIXeAW5auGJSGICGHhq4YlIYgIXePk5WboenogkJHCBV5AT1hWPRSQhgQu8/NwsGppbdUNuEem2wAVeQU4Yd93IR0S6L3CBl69r4olIggIXeAW6Jp6IJChwgVcYbeHtamzOcE1EJGgCF3hlRbkA1NTvy3BNRCRoAhd4A4vzAKhW4IlINwUu8MoKIy286l0KPBHpnsAFXk5WiP752VTXN2a6KiISMIELPICBRXnq0opItwUz8IpzFXgi0m2BDLyyolxqdqlLKyLdE8jAG1iUR83ufaTh1rci0oMFNPByaW516hp08LGIxC+YgVccPTRFI7Ui0g3BDLyi6MHHOhZPRLohoIHX3sJT4IlI/AIZeIOip5dt2bk3wzURkSAJZOD1yQkzsCiX9dsbMl0VEQmQQAYewIiSfNbWKvBEJH4BDrwC1tXuyXQ1RCRA4go8M5tlZsvNbKWZXd/BPB83s6Vm9qaZPZDaah6ssiSfrbv2sVe3bBSROGV1NYOZhYHbgPcDVcDLZjbX3ZfGzDMW+Dow093rzGxguircrqKkAID12xs4anBRuosTkR4gnhbeicBKd1/t7k3Ag8DsA+b5DHCbu9cBuHt1aqt5sMqSfADWqlsrInGKJ/CGARtinldFp8UaB4wzs+fM7EUzm5WqCnZkxIBIC0/78UQkXl12abuxnLHAGUA58IyZTXD3HbEzmdnVwNUAFRUVSRXYNz+b/vnZrNNIrYjEKZ4W3kZgeMzz8ui0WFXAXHdvdvc1wAoiAfgu7j7H3ae6+9SysrJE67xfRUmBAk9E4hZP4L0MjDWzkWaWA1wIzD1gnr8Rad1hZqVEurirU1fNQ6ssydc+PBGJW5eB5+4twLXAY8Ay4CF3f9PMvmdm50ZnewyoNbOlwFPAf7h7bboq3W5ESQGbduylqaUt3UWJSA8Q1z48d38UePSAad+OeezAddGfw2bEgHzaHKrqGhhVVng4ixaRAArsmRYAlaWRQ1O0H09E4hHowBsRPfhY+/FEJB6BDrySghwKcsJq4YlIXAIdeGamiwiISNwCHXgQ2Y+nFp6IxCPwgVcxoIANdQ20tumWjSLSucAH3rD+fWhudbbt1v0tRKRzgQ+8oX0j97fYtEP3txCRzgU+8Ib07QPA5p26R62IdC7wgTe0n1p4IhKfwAde3z7Z9MkOq4UnIl0KfOCZGUP65bFZ96gVkS4EPvAAhvbtw6YdauGJSOd6ROAN6asWnoh0rWcEXr8+VNfvo7lV18UTkY71iMAb2jcPd9i6S91aEelYjwi8If10LJ6IdK1HBJ7OthCRePSIwFMLT0Ti0SMCrzA3i6K8LDarhScinegRgQfRY/HUwhORTvSYwNPZFiLSlZ4TeH37sFlnW4hIJ3pM4A3tm0ftniYam1szXRUROUL1mMBrH6ndov14ItKBHhN4+4/F0348EelAjwm8/cfiaT+eiHSg5wRetIWnkVoR6UiPCby87DCDi/NYvnV3pqsiIkeoHhN4ANNGDuDlNdtx1z1qReRgPSrwThw5gC27Glm/vSHTVRGRI1BcgWdms8xsuZmtNLPrO5nvo2bmZjY1dVWM3/SRAwBYsGZ7JooXkSNcl4FnZmHgNuAsYDxwkZmNP8R8RcCXgAWprmS8xg4sZEBBDi8p8ETkEOJp4Z0IrHT31e7eBDwIzD7EfN8HbgYydlyImTFj1ADmLa/WGRcicpB4Am8YsCHmeVV02n5mNhkY7u6PpLBuCbl4+gi27W5i7uJNma6KiBxhkh60MLMQ8FPgK3HMe7WZLTSzhTU1NckWfUgnjy5h/JBi5sxfTVubRmtF5B3xBN5GYHjM8/LotHZFwHHAPDNbC8wA5h5q4MLd57j7VHefWlZWlnitO2FmXHnKSFZW7+altdqXJyLviCfwXgbGmtlIM8sBLgTmtr/o7jvdvdTdK929EngRONfdF6alxnE4e8IQCnOz+NMrVZmqgogcgboMPHdvAa4FHgOWAQ+5+5tm9j0zOzfdFUxEn5wwH5owhH++vpmGppZMV0dEjhBx7cNz90fdfZy7j3b3G6PTvu3ucw8x7xmZbN21++iUcvY0tfKvN7ZkuioicoToUWdaxJpW2Z+KAfnq1orIfj028MyM8ycP44XVtWzU3cxEhB4ceAAfnVyOO/xZrTwRoYcH3vAB+Zw+roxfz1/Ntt37Ml0dEcmwHh14AN/68Hgam1u56dG3Ml0VEcmwHh94YwYWcvVpo/jzoir+9cbmTFdHRDKoxwcewJfeO46J5X352p+WUL1L97wQ6a16ReDlZIX4+YUnsLe5lZ898XamqyMiGdIrAg9gZGkBF08fwUMLN7CyWve9EOmNek3gAXzhzDHkZoW44+lVma6KiGRArwq8ksJczp88jLmvbaJWh6mI9Dq9KvAALjupkqaWNr7yx9f41byVusOZSC/S6wJv7KAi3nfMIOYtr+GH/1rOqpo9ma6SiBwmvS7wAH596RSe+Y/3ADBveXWGayMih0uvDDwzo6IknzEDC3l6RXouNS8iR55eGXjtzhhXxoLV29mzTxcJFekNenXgnXnMQJpa2/j0PS/z1pZdma6OiKRZrw68k0aV8P3Zx/J29W5m//I5/vbqxq7fJCKB1asDz8z41EmVPPbl05g0vB/XPbSYp96qZvPOvTpcRaQH6tWB166sKJe7Lp/GuEFFXHHPy5x00/9y93NrM10tEUkxBV5UQW4W91xxIl95/zgmDe/HL/73beobmzNdLRFJIQVejMF98/jCe8fyvdnHUtfQzE8eX6GurUgPkpXpChyJJpb345IZFdzz/FrW1u5h1rGD+eiUcrLD+v8gEmQKvA58f/ZxDOuXz53zVzNveU3kVLQLJlKcl53pqolIgixTXbapU6f6woUZv193l9ydu59by/ceXgrArGMHc/slkzGzDNdMRA7FzF5x96mHek0tvC6YGZ8+ZSTjhxbz6Oubue+Fdcx5ZjVjBhby+JtbWVO7h+OG9uXb54zPdFVFpAsKvDjNGFXC9JEDWLNtDzf9M3IHtKLcLAYW53LXc2v45PQKxgwszHAtRaQzCrxuMDNu+cQknli2leED8pkyoj/1jS2cdNOTPLBgvVp5Ikc4DTt2U0lhLp+YVsHJo0vJzQpTWpjLrOOG8KdXNvDYm1toaW3jtQ07uP+FtbS16ZAWkSOJWngp8H/OGM2C1bV89v5XGFlawMa6vTS1trFo/Q6+f95xFOZqM4scCdTCS4FjhhTz/PVn8quLJ5MTDnHS6BK+eOYY/vrqRk6+6Ul+8vhytu9pynQ1RXq9uA5LMbNZwM+BMHCnu//ggNevA64CWoAa4NPuvq6zZQblsJRkvLZhB7fPW8VjS7cwYkA+t150Av/119eZffwwPn3KSMIhHdoikmqdHZbSZeCZWRhYAbwfqAJeBi5y96Ux87wHWODuDWb2eeAMd/9EZ8vtDYHX7rmV27jkNwsIm2EGza3O+44ZxC0XTmJl9W4qS/L56b9XkJ+TxbnHD+Wvr1bx+TPGMKAgJ9NVFwmcZI/DOxFY6e6rowt7EJgN7A88d38qZv4XgUsSr27PM3NMKZ87fTS/mb+G3141nTc37eS7/1jK5O/9m6bWNtqPYXZn/z1zdzQ086MLjs9grUV6nngCbxiwIeZ5FTC9k/mvBP6ZTKV6oq998Cg+f8ZoivOyOXHkAAxYsGY77z1mEG9u2skHxg9m594mnl9VS0ub88CC9XxsSjklhTk8vnQrn5oxgoKcLMzQWR4iCYqnS/sxYJa7XxV9/ilgurtfe4h5LwGuBU5394PudG1mVwNXA1RUVExZt67T3Xy91u59Lcy65Rmq6/eRFTIamlopLcxlb1MLRXnZnD95GNe9fxzhkGFmrK9toGpHAyePLt2/jFU1u7n/hXV8/eyjyc0KZ3BtRA6vZLu0G4HhMc/Lo9MOLOR9wDfoIOwA3H0OMAci+/DiKLtXKszN4u/XzOTrf3md+sYWrphZye8WrGdovz7U1O/jV/NW8dTyGtbX7iE3O0xdQxPucMM547li5kgAvv/wUuYtr2F0WQGfOqkysyskcoSIJ/BeBsaa2UgiQXch8MnYGczsBOB/iLQEdaPXFCgpzGXOpe/8k/rAsYP3P/7ti+v42b9XMOu4IeRkGQOL8li2eRff/Uck5KaPGsC85TXkZoW47alV5GWH2bOvhaOHFDNjVEm36rF7XwtVdQ0cPbg4ZesmkinxHpZyNnALkcNS7nL3G83se8BCd59rZk8AE4DN0besd/dzO1tmbxqlPRyaWtr4n6dXcf+L66iu30dpYQ4/OH8iV9337m38zQ8dQ8WAfMr759PQ1MJzK2upqmvgvccM5PRxA1lZvZunV1TTJyeLsycM5ssPLmbBmu1MGdGfG84Zz8TyfplZQZE4JXVYSroo8NKjrc1ZunkXRXlZjCgpYMHqWgYU5NC/IIf/+ONrPLX83TceN4t0oesbD743b3bYaG51LjpxOE8uq2bb7n1cMGU4wwf0YcXW3Rw9pIjXNuxg+sgSrphZedBgyktrtnPjo8u48bzjOG5Y37Sut0g7BZ4AsK+llWff3ka//BzW1e4hOxzijKPK6JMdZt7yGpZvrae4TzbnTBxCdf0+vv33Nzh6cDHfOfdYdjU289PHV/DAS+tpammjtDCHbbub6JefzY6GZs48eiAThvXlnOOHsLJ6D6tqdnPH06uob2xh+IA+PHztqWyoa+AvizZyxcxKhg/IP2Qdt+9pYl3tHk6o6H+Yt470FAo8SZnte5poamljUHEu23Y3UVKQwy1PrODPizayeedeYq+XUFmSz1c/eBRffnAxo8sKqa5vpK6hmZxwiOmjBrCvpY2dDc0M7ZeHA2WFuTyxbCt1Dc1cMKWca88cw4iSgv3La2xu5bmV2yjvn89Rg4sAWLxhB9/82+uUFOQy59IpuMONjyxj3KBCLp4+glDIaGlt42t/XsIxg4v5zGmj0rJd5r62iRVb6vnqB49Ky/Ilfgo8OSy27mrkkSWbGTuokCkj+tMnO4yZMf/tGr704GJCBr/85GT+vXQrz63cRl52mJKCHDbvbCQUgo11exk7sIgJ5X25+7k1tDlMGt6P8yYNpU9OmJv++RY7Gpoxg4unV3DGuIF8/nevUJyXTe2eJk4aVUJjSyuvrt8BwKljS/ne7OO49/m13PP8WrJCxuP/9zRGlUWuW3jTP5cRNuNrs47evw4rttazqno3U0b0Z2BxXlzr3drmnPbDp9i0cy8v/df7KCvKTfm2lfgp8CTjdjY009LWRklhfGGwacde5r62ib8v3sSyzbsAmDqiP9ecOYb5K7Zx9/NrcIejBhXx0GdP4i+vVvHzJ9/GgO+ceyz1jS3c+Mgy9ja3AvCRE4bx+JtbGD2wkNmThpGfE+brf3kdM/j7NTN5ZV0dr67fwcNLNtHmEA4Z/3X2MYwszac4L5sTKvrvP/e5ubWNLTsbWbS+jj8v2sjkin7c8sTbANz4keO4aFoFT75VzZC+ee/ad/nAgvWs276HK08ZycCi+MJUuk+BJ4G2Yms962obOPPogftD57mV23jgpfV84+xjGNqvzyHft2bbHh57cwvHDCnm1DGl/GlRFTc9uoy6hsj9ho8aVMTGHZFLeTW1tFFSkMOHJg5h9qRh3D5vJU8se+cIq+ED+vDND42noamFHz+2go079gLvDOz0z8+muE82xXnZmMGSqp2UFuZw9+Un8tDCDYRDxj3PrwUgLzvEJdNHcOywYvJzspj/dg1/e3UTedlhLjpxONMqB/Dq+h1kZxmXzBjBhu0N7Gxo5uQxkQPL3Z07nl7NE8u2MrG8LwOL8pgwrC/TRvYnJxxi3ooaHntjCzNGlXDeCcO63L7Lt9TzyJJNfPl94wil+YIWtzyxgtrdTXz/vOPSVoYCTyTG+toG/rFkE2cdN5hHlmzmF0+t5CcXHM85xw/dP09bm/PEsq0U98lm665Gfv7E26zetgeIBOVlJ1cyrH8fxg4s5DP3LeRDE4dQ39jC7fNWMaAghytPGclP/72CNndCZpFu77gyvvWhY/jVvFX8bfFG2r96WSHj3OOHsntfC48v3fquup5Q0Y8VW+rZ09TKp2aM4NihxSxcV8efXqliVFkBVXV7aWppA2BYvz4cPbiIJ9+qJiccoqm1jY+cMIzLT66ktCiXax9YxNiBhdz80Yls2L6Xmt2NTBjWj6vuW8gzK2r46ceP55V1dZQU5HDiyBI21DVwwZRyssIhdjU2s7624V0t1ubWNjZsb2Bovz7kZR98Ns+9z6+lf0EO50a3646GJqb/vyfZ19LGfZ8+kb8v3sRlJ4/o8FAnd0/oNEoFnkgH3J2GplYKurhIa2NzK/OW1zCwOJeJw/qSdYh7FG/d1cicZ1Zz1akjGdK3D7c8sYI/vLyBuy6fRmFuFoOK88jJiryvbk8TdQ1NNDS1UlKYw5C+kVbqi6tr2bW3mdPGlfHwks189Y+vMaxfH04ZU8ofFkZOac8OG588sYIbzjkWM6jf18KLq2r54WPLWbNtD1/9wFFcMbOSW598m7ueW0Njc9v+9zW3OqPLClhVEwnv48v78lrVzv0t59YDrtJ94bThfONDx3DRr1/kjY27mFbZn+vPOoZF6+q489nVbN0VOf3xY1PK+ezpo1lXu4e5r22iOC+be55fSzhk/OHqGUytHMCd81fz348sIzcrRHNrG20ORXlZfPa0UQzu24cPHjuIwtwsbn96FQ8sWM+efS18euZILp9ZSVE3bo+qwBPJkERbKe3mv13DmIGFDOnbh/rGZnbubWZg0TvBGauppY3te5oY3Ped/YM79zbz5LKtLN9Sz0enlPOb+Wt4fOkWrj5tNG3u/Oix5eRlh7j5oxP50oOLueY9o/nwxKFsrNvLK+vruH3eKrJCRps7V54ykr8t3kRNfeTM0VPGlPKhiUN4c9NO/vDyBppbI1mSkxWiqaWNM48eyKqa3Wze2cio0gI27tjLuEFFnDSqhF8+tZIbzhnP/S+s299yzs8Jc/LoUp5YtpWZY0rICYd4ankNd10+lTOPHhT3NlPgiQgQCWB39u+r+92CdWSHQ3x86nCqdzVSVpS7P6DdnX8s2cyidXWcMqaU940fxK7GZn4zfw2TKvrxnqMG7l/ulp2NPLxkEwCXzBjBss27GD+0mM07GrnvhXWs395AmztXnzaKEysHsKGugRElBbS2OU0tbby1ZRe/eXYNDy/ZzLnHD+WWT0wiFDKWbd7F0YOLuvVPQ4EnIoGwZWcjA4tykxo80Y24RSQQYrvj6aCb+IhIr6HAE5FeQ4EnIr2GAk9Eeg0Fnoj0Ggo8Eek1FHgi0mso8ESk11DgiUivocATkV4jY+fSmlkNsK6bbysFtqWhOir/yC5b5av87pQ/wt3LDvVCxgIvEWa2sKOTglV+zy1b5av8VJWvLq2I9BoKPBHpNYIWeHNUfq8sW+Wr/JSUH6h9eCIiyQhaC09EJGGBCDwzm2Vmy81spZldfxjKG25mT5nZUjN708y+FJ3+HTPbaGaLoz9np7EOa83s9Wg5C6PTBpjZv83s7ejv/mkq+6iYdVxsZrvM7MvpXH8zu8vMqs3sjZhph1xfi7g1+vewxMwmp6n8H5nZW9Ey/mpm/aLTK81sb8x2uCNN5Xe4vc3s69H1X25mH0xD2X+IKXetmS2OTk/Hunf0fUv95x+5qceR+wOEgVXAKCAHeA0Yn+YyhwCTo4+LgBXAeOA7wFcP03qvBUoPmPZD4Pro4+uBmw/T9t8CjEjn+gOnAZOBN7paX+Bs4J+AATOABWkq/wNAVvTxzTHlV8bOl8b1P+T2jv4tvgbkAiOj349wKss+4PWfAN9O47p39H1L+ecfhBbeicBKd1/t7k3Ag8DsdBbo7pvdfVH0cT2wDOj6Fu7pNxu4N/r4XuC8w1Dme4FV7t7dg8S7xd2fAbYfMLmj9Z0N3OcRLwL9zGxIqst398fdvSX69EWgPJkyult+J2YDD7r7PndfA6wk8j1JedkWuV3Yx4HfJ7r8OMrv6PuW8s8/CIE3DNgQ87yKwxg+ZlYJnAAsiE66NtqMvitdXcooBx43s1fM7OrotEHuvjn6eAsQ/806E3ch7/5jP1zrDx2vbyb+Jj5NpFXRbqSZvWpmT5vZqWks91Db+3Cu/6nAVnd/O2Za2tb9gO9byj//IARexphZIfBn4Mvuvgu4HRgNTAI2E2nqp8sp7j4ZOAu4xsxOi33RI237tA6xm1kOcC7wx+ikw7n+73I41rcjZvYNoAX4XXTSZqDC3U8ArgMeMLPiNBSdse0d4yLe/Q8vbet+iO/bfqn6/IMQeBuB4THPy6PT0srMsols/N+5+18A3H2ru7e6exvwa5LoRnTF3TdGf1cDf42WtbW96R79XZ2u8qPOAha5+9ZoXQ7b+kd1tL6H7W/CzC4HPgxcHP3SEe1K1kYfv0JkH9q4VJfdyfY+LOtvZlnA+cAfYuqUlnU/1PeNNHz+QQi8l4GxZjYy2uK4EJibzgKj+y1+Ayxz95/GTI/dT/AR4I0D35ui8gvMrKj9MZGd528QWe/LorNdBvw9HeXHeNd/98O1/jE6Wt+5wKXR0boZwM6Yrk/KmNks4GvAue7eEDO9zMzC0cejgLHA6jSU39H2ngtcaGa5ZjYyWv5LqS4feB/wlrtXxdQp5eve0feNdHz+qRxtSdcPkVGZFUT+m3zjMJR3CpHm8xJgcfTnbOB+4PXo9LnAkDSVP4rIKNxrwJvt6wyUAE8CbwNPAAPSuA0KgFqgb8y0tK0/kWDdDDQT2SdzZUfrS2R07rbo38PrwNQ0lb+SyL6i9r+BO6LzfjT6uSwGFgHnpKn8Drc38I3o+i8Hzkp12dHp9wCfO2DedKx7R9+3lH/+OtNCRHqNIHRpRURSQoEnIr2GAk9Eeg0Fnoj0Ggo8Eek1FHgi0mso8ESk11DgiUiv8f8BcV3Ve/8ELW4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.subplot()\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Training - Loss Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.9212030e-03, 1.7071757e-02, 9.7500420e-01, 2.8124256e-07,\n",
       "        2.6210228e-06],\n",
       "       [5.6985944e-10, 1.5294478e-05, 1.7466332e-06, 1.1778819e-02,\n",
       "        9.8820418e-01],\n",
       "       [9.9999994e-01, 4.4733679e-16, 2.2780045e-14, 9.4090610e-36,\n",
       "        0.0000000e+00],\n",
       "       ...,\n",
       "       [9.9993110e-01, 6.1152074e-05, 7.7730574e-06, 8.1523669e-15,\n",
       "        5.8316567e-16],\n",
       "       [1.9796031e-02, 3.7728788e-04, 9.7982669e-01, 3.0075397e-11,\n",
       "        3.2104086e-10],\n",
       "       [1.0000000e+00, 4.8242114e-21, 1.0268602e-20, 0.0000000e+00,\n",
       "        0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob = model.predict(X_test)\n",
    "y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mac_hash</th>\n",
       "      <th>C0</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b882f1d44602a25349a08f4a0af32977</td>\n",
       "      <td>7.921203e-03</td>\n",
       "      <td>1.707176e-02</td>\n",
       "      <td>9.750042e-01</td>\n",
       "      <td>2.812426e-07</td>\n",
       "      <td>2.621023e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86e644b498922f3a2fd0e6e1a1525de3</td>\n",
       "      <td>5.698594e-10</td>\n",
       "      <td>1.529448e-05</td>\n",
       "      <td>1.746633e-06</td>\n",
       "      <td>1.177882e-02</td>\n",
       "      <td>9.882042e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>691662b04ee08015062d901a4c5628b1</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>4.473368e-16</td>\n",
       "      <td>2.278004e-14</td>\n",
       "      <td>9.409061e-36</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52b5c510a28774237f4f118764c2ed6f</td>\n",
       "      <td>2.721664e-03</td>\n",
       "      <td>9.972550e-01</td>\n",
       "      <td>1.870805e-05</td>\n",
       "      <td>4.653631e-06</td>\n",
       "      <td>6.256593e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9f3c995e53d109f532056b6eae29a0b5</td>\n",
       "      <td>7.714346e-04</td>\n",
       "      <td>2.712694e-01</td>\n",
       "      <td>7.279003e-01</td>\n",
       "      <td>4.724672e-05</td>\n",
       "      <td>1.155017e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>5001c0dfe522886ed884c0dc4e5848df</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.385725e-18</td>\n",
       "      <td>1.984960e-18</td>\n",
       "      <td>4.387067e-37</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425</th>\n",
       "      <td>f1c8781e701e89068fa0b8a62ecbc564</td>\n",
       "      <td>2.944204e-08</td>\n",
       "      <td>1.782065e-04</td>\n",
       "      <td>2.747556e-05</td>\n",
       "      <td>1.256109e-02</td>\n",
       "      <td>9.872332e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426</th>\n",
       "      <td>646136b402e136422466a2acd8636630</td>\n",
       "      <td>9.999311e-01</td>\n",
       "      <td>6.115207e-05</td>\n",
       "      <td>7.773057e-06</td>\n",
       "      <td>8.152367e-15</td>\n",
       "      <td>5.831657e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427</th>\n",
       "      <td>c7bea6491029f86ab3d1f0f9b599ca2c</td>\n",
       "      <td>1.979603e-02</td>\n",
       "      <td>3.772879e-04</td>\n",
       "      <td>9.798267e-01</td>\n",
       "      <td>3.007540e-11</td>\n",
       "      <td>3.210409e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428</th>\n",
       "      <td>822fd3104f38cc6374f2996d3d6920a2</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.824211e-21</td>\n",
       "      <td>1.026860e-20</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3429 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              mac_hash            C0            C1  \\\n",
       "0     b882f1d44602a25349a08f4a0af32977  7.921203e-03  1.707176e-02   \n",
       "1     86e644b498922f3a2fd0e6e1a1525de3  5.698594e-10  1.529448e-05   \n",
       "2     691662b04ee08015062d901a4c5628b1  9.999999e-01  4.473368e-16   \n",
       "3     52b5c510a28774237f4f118764c2ed6f  2.721664e-03  9.972550e-01   \n",
       "4     9f3c995e53d109f532056b6eae29a0b5  7.714346e-04  2.712694e-01   \n",
       "...                                ...           ...           ...   \n",
       "3424  5001c0dfe522886ed884c0dc4e5848df  1.000000e+00  1.385725e-18   \n",
       "3425  f1c8781e701e89068fa0b8a62ecbc564  2.944204e-08  1.782065e-04   \n",
       "3426  646136b402e136422466a2acd8636630  9.999311e-01  6.115207e-05   \n",
       "3427  c7bea6491029f86ab3d1f0f9b599ca2c  1.979603e-02  3.772879e-04   \n",
       "3428  822fd3104f38cc6374f2996d3d6920a2  1.000000e+00  4.824211e-21   \n",
       "\n",
       "                C2            C3            C4  \n",
       "0     9.750042e-01  2.812426e-07  2.621023e-06  \n",
       "1     1.746633e-06  1.177882e-02  9.882042e-01  \n",
       "2     2.278004e-14  9.409061e-36  0.000000e+00  \n",
       "3     1.870805e-05  4.653631e-06  6.256593e-10  \n",
       "4     7.279003e-01  4.724672e-05  1.155017e-05  \n",
       "...            ...           ...           ...  \n",
       "3424  1.984960e-18  4.387067e-37  0.000000e+00  \n",
       "3425  2.747556e-05  1.256109e-02  9.872332e-01  \n",
       "3426  7.773057e-06  8.152367e-15  5.831657e-16  \n",
       "3427  9.798267e-01  3.007540e-11  3.210409e-10  \n",
       "3428  1.026860e-20  0.000000e+00  0.000000e+00  \n",
       "\n",
       "[3429 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame({'mac_hash':sample['mac_hash'], 'C0':y_prob[:,0], 'C1':y_prob[:,1], 'C2':y_prob[:,2], 'C3':y_prob[:,3], 'C4':y_prob[:,4]})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"output_LSTM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#FF0000> RNN (w/o group)</font> \t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (with validation set spiltted from training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 14, 30)            960       \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 14, 30)            0         \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 14, 30)            1830      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 14, 30)            0         \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 30)                1830      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5)                 155       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,775\n",
      "Trainable params: 4,775\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()#Adding the first SimpleRNN layer and some Dropout regularisation\n",
    "model.add(SimpleRNN(units = 30, activation='relu', return_sequences=True, input_shape = (X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))# Adding a second SimpleRNN layer and some Dropout regularisation\n",
    "model.add(SimpleRNN(units = 30, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))# Adding a third SimpleRNN layer and some Dropout regularisation\n",
    "model.add(SimpleRNN(units = 30, activation='relu'))\n",
    "model.add(Dense(units = 5, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "49/49 [==============================] - 2s 10ms/step - loss: 1.3341 - acc: 0.5207 - val_loss: 0.9029 - val_acc: 0.6408\n",
      "Epoch 2/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.8565 - acc: 0.6411 - val_loss: 0.6453 - val_acc: 0.7586\n",
      "Epoch 3/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.6744 - acc: 0.7009 - val_loss: 0.5218 - val_acc: 0.7902\n",
      "Epoch 4/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.5852 - acc: 0.7378 - val_loss: 0.5043 - val_acc: 0.8089\n",
      "Epoch 5/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.5266 - acc: 0.7659 - val_loss: 0.4476 - val_acc: 0.8448\n",
      "Epoch 6/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.4810 - acc: 0.7939 - val_loss: 0.3901 - val_acc: 0.8520\n",
      "Epoch 7/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.4548 - acc: 0.8119 - val_loss: 0.4225 - val_acc: 0.8376\n",
      "Epoch 8/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.4244 - acc: 0.8159 - val_loss: 0.3592 - val_acc: 0.8707\n",
      "Epoch 9/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.4048 - acc: 0.8331 - val_loss: 0.3649 - val_acc: 0.8707\n",
      "Epoch 10/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.3967 - acc: 0.8325 - val_loss: 0.3614 - val_acc: 0.8491\n",
      "Epoch 11/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.3769 - acc: 0.8440 - val_loss: 0.3696 - val_acc: 0.8448\n",
      "Epoch 12/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.3560 - acc: 0.8555 - val_loss: 0.2941 - val_acc: 0.8764\n",
      "Epoch 13/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.3555 - acc: 0.8582 - val_loss: 0.2724 - val_acc: 0.8951\n",
      "Epoch 14/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.3425 - acc: 0.8611 - val_loss: 0.2658 - val_acc: 0.8922\n",
      "Epoch 15/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.3289 - acc: 0.8616 - val_loss: 0.2871 - val_acc: 0.8922\n",
      "Epoch 16/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.3099 - acc: 0.8777 - val_loss: 0.2669 - val_acc: 0.9009\n",
      "Epoch 17/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.3064 - acc: 0.8802 - val_loss: 0.2430 - val_acc: 0.8966\n",
      "Epoch 18/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2934 - acc: 0.8870 - val_loss: 0.3264 - val_acc: 0.8649\n",
      "Epoch 19/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2839 - acc: 0.8890 - val_loss: 0.2232 - val_acc: 0.9167\n",
      "Epoch 20/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2761 - acc: 0.8933 - val_loss: 0.2138 - val_acc: 0.9167\n",
      "Epoch 21/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2620 - acc: 0.8980 - val_loss: 0.2129 - val_acc: 0.9167\n",
      "Epoch 22/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2550 - acc: 0.8999 - val_loss: 0.2083 - val_acc: 0.9095\n",
      "Epoch 23/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2479 - acc: 0.9045 - val_loss: 0.1857 - val_acc: 0.9195\n",
      "Epoch 24/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2425 - acc: 0.9069 - val_loss: 0.1887 - val_acc: 0.9195\n",
      "Epoch 25/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2407 - acc: 0.9068 - val_loss: 0.2629 - val_acc: 0.8908\n",
      "Epoch 26/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2385 - acc: 0.9119 - val_loss: 0.1833 - val_acc: 0.9267\n",
      "Epoch 27/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2281 - acc: 0.9109 - val_loss: 0.1672 - val_acc: 0.9325\n",
      "Epoch 28/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2279 - acc: 0.9141 - val_loss: 0.2170 - val_acc: 0.9109\n",
      "Epoch 29/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2179 - acc: 0.9176 - val_loss: 0.1738 - val_acc: 0.9296\n",
      "Epoch 30/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2036 - acc: 0.9224 - val_loss: 0.1667 - val_acc: 0.9397\n",
      "Epoch 31/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2231 - acc: 0.9163 - val_loss: 0.1657 - val_acc: 0.9310\n",
      "Epoch 32/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2107 - acc: 0.9216 - val_loss: 0.1611 - val_acc: 0.9440\n",
      "Epoch 33/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2105 - acc: 0.9187 - val_loss: 0.1813 - val_acc: 0.9411\n",
      "Epoch 34/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2037 - acc: 0.9205 - val_loss: 0.1775 - val_acc: 0.9296\n",
      "Epoch 35/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.2045 - acc: 0.9216 - val_loss: 0.1535 - val_acc: 0.9382\n",
      "Epoch 36/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1942 - acc: 0.9266 - val_loss: 0.1566 - val_acc: 0.9411\n",
      "Epoch 37/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1866 - acc: 0.9291 - val_loss: 0.1635 - val_acc: 0.9339\n",
      "Epoch 38/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1920 - acc: 0.9283 - val_loss: 0.1499 - val_acc: 0.9325\n",
      "Epoch 39/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1941 - acc: 0.9274 - val_loss: 0.1540 - val_acc: 0.9368\n",
      "Epoch 40/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1858 - acc: 0.9315 - val_loss: 0.1713 - val_acc: 0.9339\n",
      "Epoch 41/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1869 - acc: 0.9299 - val_loss: 0.1803 - val_acc: 0.9210\n",
      "Epoch 42/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1884 - acc: 0.9280 - val_loss: 0.1434 - val_acc: 0.9425\n",
      "Epoch 43/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1783 - acc: 0.9310 - val_loss: 0.1426 - val_acc: 0.9425\n",
      "Epoch 44/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1926 - acc: 0.9277 - val_loss: 0.1613 - val_acc: 0.9339\n",
      "Epoch 45/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1825 - acc: 0.9299 - val_loss: 0.1623 - val_acc: 0.9368\n",
      "Epoch 46/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1781 - acc: 0.9336 - val_loss: 0.1435 - val_acc: 0.9511\n",
      "Epoch 47/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1779 - acc: 0.9341 - val_loss: 0.1407 - val_acc: 0.9483\n",
      "Epoch 48/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1744 - acc: 0.9333 - val_loss: 0.1515 - val_acc: 0.9397\n",
      "Epoch 49/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1828 - acc: 0.9318 - val_loss: 0.1415 - val_acc: 0.9454\n",
      "Epoch 50/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1675 - acc: 0.9363 - val_loss: 0.1472 - val_acc: 0.9353\n",
      "Epoch 51/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1655 - acc: 0.9379 - val_loss: 0.1432 - val_acc: 0.9468\n",
      "Epoch 52/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1817 - acc: 0.9309 - val_loss: 0.1395 - val_acc: 0.9511\n",
      "Epoch 53/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1691 - acc: 0.9341 - val_loss: 0.1380 - val_acc: 0.9468\n",
      "Epoch 54/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1686 - acc: 0.9353 - val_loss: 0.1386 - val_acc: 0.9511\n",
      "Epoch 55/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1582 - acc: 0.9368 - val_loss: 0.1374 - val_acc: 0.9511\n",
      "Epoch 56/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1581 - acc: 0.9424 - val_loss: 0.1419 - val_acc: 0.9425\n",
      "Epoch 57/200\n",
      "49/49 [==============================] - 0s 6ms/step - loss: 0.1698 - acc: 0.9353 - val_loss: 0.1388 - val_acc: 0.9425\n",
      "Epoch 58/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1648 - acc: 0.9379 - val_loss: 0.1787 - val_acc: 0.9282\n",
      "Epoch 59/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1629 - acc: 0.9395 - val_loss: 0.1617 - val_acc: 0.9425\n",
      "Epoch 60/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1635 - acc: 0.9384 - val_loss: 0.1374 - val_acc: 0.9468\n",
      "Epoch 61/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1562 - acc: 0.9392 - val_loss: 0.1343 - val_acc: 0.9497\n",
      "Epoch 62/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1640 - acc: 0.9380 - val_loss: 0.1400 - val_acc: 0.9382\n",
      "Epoch 63/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1548 - acc: 0.9390 - val_loss: 0.1726 - val_acc: 0.9253\n",
      "Epoch 64/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1632 - acc: 0.9361 - val_loss: 0.1313 - val_acc: 0.9440\n",
      "Epoch 65/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1559 - acc: 0.9404 - val_loss: 0.1516 - val_acc: 0.9382\n",
      "Epoch 66/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1565 - acc: 0.9395 - val_loss: 0.1312 - val_acc: 0.9483\n",
      "Epoch 67/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1580 - acc: 0.9390 - val_loss: 0.1349 - val_acc: 0.9540\n",
      "Epoch 68/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1556 - acc: 0.9419 - val_loss: 0.1319 - val_acc: 0.9540\n",
      "Epoch 69/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1563 - acc: 0.9428 - val_loss: 0.1388 - val_acc: 0.9411\n",
      "Epoch 70/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1522 - acc: 0.9417 - val_loss: 0.1252 - val_acc: 0.9483\n",
      "Epoch 71/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1530 - acc: 0.9396 - val_loss: 0.1330 - val_acc: 0.9511\n",
      "Epoch 72/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1501 - acc: 0.9425 - val_loss: 0.1232 - val_acc: 0.9598\n",
      "Epoch 73/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1476 - acc: 0.9408 - val_loss: 0.1444 - val_acc: 0.9382\n",
      "Epoch 74/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1531 - acc: 0.9428 - val_loss: 0.1281 - val_acc: 0.9483\n",
      "Epoch 75/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1497 - acc: 0.9438 - val_loss: 0.1242 - val_acc: 0.9526\n",
      "Epoch 76/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1450 - acc: 0.9462 - val_loss: 0.1283 - val_acc: 0.9454\n",
      "Epoch 77/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1428 - acc: 0.9438 - val_loss: 0.1256 - val_acc: 0.9555\n",
      "Epoch 78/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1486 - acc: 0.9419 - val_loss: 0.1441 - val_acc: 0.9397\n",
      "Epoch 79/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1463 - acc: 0.9416 - val_loss: 0.1292 - val_acc: 0.9526\n",
      "Epoch 80/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1453 - acc: 0.9462 - val_loss: 0.1439 - val_acc: 0.9483\n",
      "Epoch 81/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1440 - acc: 0.9452 - val_loss: 0.1296 - val_acc: 0.9497\n",
      "Epoch 82/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1378 - acc: 0.9492 - val_loss: 0.1324 - val_acc: 0.9483\n",
      "Epoch 83/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1442 - acc: 0.9451 - val_loss: 0.1249 - val_acc: 0.9511\n",
      "Epoch 84/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1423 - acc: 0.9465 - val_loss: 0.1556 - val_acc: 0.9325\n",
      "Epoch 85/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1460 - acc: 0.9430 - val_loss: 0.1309 - val_acc: 0.9454\n",
      "Epoch 86/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1427 - acc: 0.9451 - val_loss: 0.1333 - val_acc: 0.9440\n",
      "Epoch 87/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1402 - acc: 0.9441 - val_loss: 0.1272 - val_acc: 0.9526\n",
      "Epoch 88/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1365 - acc: 0.9465 - val_loss: 0.1331 - val_acc: 0.9511\n",
      "Epoch 89/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1385 - acc: 0.9464 - val_loss: 0.1450 - val_acc: 0.9425\n",
      "Epoch 90/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1408 - acc: 0.9471 - val_loss: 0.1294 - val_acc: 0.9540\n",
      "Epoch 91/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1389 - acc: 0.9457 - val_loss: 0.1254 - val_acc: 0.9540\n",
      "Epoch 92/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1352 - acc: 0.9478 - val_loss: 0.1273 - val_acc: 0.9497\n",
      "Epoch 93/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1414 - acc: 0.9422 - val_loss: 0.1222 - val_acc: 0.9555\n",
      "Epoch 94/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1422 - acc: 0.9416 - val_loss: 0.1323 - val_acc: 0.9511\n",
      "Epoch 95/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1435 - acc: 0.9470 - val_loss: 0.1267 - val_acc: 0.9526\n",
      "Epoch 96/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1388 - acc: 0.9464 - val_loss: 0.1197 - val_acc: 0.9540\n",
      "Epoch 97/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1342 - acc: 0.9470 - val_loss: 0.1278 - val_acc: 0.9454\n",
      "Epoch 98/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1356 - acc: 0.9476 - val_loss: 0.1226 - val_acc: 0.9526\n",
      "Epoch 99/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1370 - acc: 0.9470 - val_loss: 0.1343 - val_acc: 0.9411\n",
      "Epoch 100/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1313 - acc: 0.9487 - val_loss: 0.1215 - val_acc: 0.9540\n",
      "Epoch 101/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1373 - acc: 0.9460 - val_loss: 0.1316 - val_acc: 0.9397\n",
      "Epoch 102/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1307 - acc: 0.9495 - val_loss: 0.1234 - val_acc: 0.9497\n",
      "Epoch 103/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1315 - acc: 0.9481 - val_loss: 0.1372 - val_acc: 0.9497\n",
      "Epoch 104/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1334 - acc: 0.9481 - val_loss: 0.1225 - val_acc: 0.9526\n",
      "Epoch 105/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1302 - acc: 0.9476 - val_loss: 0.1320 - val_acc: 0.9454\n",
      "Epoch 106/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1308 - acc: 0.9497 - val_loss: 0.1260 - val_acc: 0.9483\n",
      "Epoch 107/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1270 - acc: 0.9476 - val_loss: 0.1343 - val_acc: 0.9468\n",
      "Epoch 108/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1286 - acc: 0.9521 - val_loss: 0.1228 - val_acc: 0.9583\n",
      "Epoch 109/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1338 - acc: 0.9484 - val_loss: 0.1207 - val_acc: 0.9540\n",
      "Epoch 110/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1248 - acc: 0.9540 - val_loss: 0.1259 - val_acc: 0.9454\n",
      "Epoch 111/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1298 - acc: 0.9508 - val_loss: 0.1159 - val_acc: 0.9555\n",
      "Epoch 112/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1322 - acc: 0.9491 - val_loss: 0.1262 - val_acc: 0.9483\n",
      "Epoch 113/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1292 - acc: 0.9495 - val_loss: 0.1330 - val_acc: 0.9569\n",
      "Epoch 114/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1295 - acc: 0.9484 - val_loss: 0.1189 - val_acc: 0.9540\n",
      "Epoch 115/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1300 - acc: 0.9526 - val_loss: 0.1342 - val_acc: 0.9468\n",
      "Epoch 116/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1327 - acc: 0.9470 - val_loss: 0.1395 - val_acc: 0.9454\n",
      "Epoch 117/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1225 - acc: 0.9505 - val_loss: 0.1212 - val_acc: 0.9526\n",
      "Epoch 118/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1345 - acc: 0.9478 - val_loss: 0.1330 - val_acc: 0.9526\n",
      "Epoch 119/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1287 - acc: 0.9497 - val_loss: 0.1253 - val_acc: 0.9425\n",
      "Epoch 120/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1230 - acc: 0.9534 - val_loss: 0.1318 - val_acc: 0.9483\n",
      "Epoch 121/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1288 - acc: 0.9511 - val_loss: 0.1435 - val_acc: 0.9497\n",
      "Epoch 122/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1375 - acc: 0.9459 - val_loss: 0.1269 - val_acc: 0.9454\n",
      "Epoch 123/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1261 - acc: 0.9508 - val_loss: 0.1199 - val_acc: 0.9555\n",
      "Epoch 124/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1226 - acc: 0.9492 - val_loss: 0.1232 - val_acc: 0.9526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1218 - acc: 0.9515 - val_loss: 0.1620 - val_acc: 0.9397\n",
      "Epoch 126/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1337 - acc: 0.9470 - val_loss: 0.1185 - val_acc: 0.9598\n",
      "Epoch 127/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1244 - acc: 0.9508 - val_loss: 0.1180 - val_acc: 0.9626\n",
      "Epoch 128/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1244 - acc: 0.9539 - val_loss: 0.1178 - val_acc: 0.9526\n",
      "Epoch 129/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1234 - acc: 0.9535 - val_loss: 0.1147 - val_acc: 0.9598\n",
      "Epoch 130/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1314 - acc: 0.9492 - val_loss: 0.1190 - val_acc: 0.9526\n",
      "Epoch 131/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1290 - acc: 0.9519 - val_loss: 0.1419 - val_acc: 0.9483\n",
      "Epoch 132/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1260 - acc: 0.9510 - val_loss: 0.1287 - val_acc: 0.9425\n",
      "Epoch 133/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1248 - acc: 0.9508 - val_loss: 0.1305 - val_acc: 0.9526\n",
      "Epoch 134/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1219 - acc: 0.9526 - val_loss: 0.1242 - val_acc: 0.9569\n",
      "Epoch 135/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1264 - acc: 0.9500 - val_loss: 0.1258 - val_acc: 0.9454\n",
      "Epoch 136/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1187 - acc: 0.9542 - val_loss: 0.1228 - val_acc: 0.9540\n",
      "Epoch 137/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1233 - acc: 0.9518 - val_loss: 0.1291 - val_acc: 0.9540\n",
      "Epoch 138/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1250 - acc: 0.9497 - val_loss: 0.1333 - val_acc: 0.9483\n",
      "Epoch 139/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1255 - acc: 0.9516 - val_loss: 0.1162 - val_acc: 0.9483\n",
      "Epoch 140/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1235 - acc: 0.9511 - val_loss: 0.1128 - val_acc: 0.9612\n",
      "Epoch 141/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1205 - acc: 0.9510 - val_loss: 0.1436 - val_acc: 0.9368\n",
      "Epoch 142/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1183 - acc: 0.9529 - val_loss: 0.1256 - val_acc: 0.9583\n",
      "Epoch 143/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1213 - acc: 0.9524 - val_loss: 0.1221 - val_acc: 0.9583\n",
      "Epoch 144/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1189 - acc: 0.9518 - val_loss: 0.1477 - val_acc: 0.9368\n",
      "Epoch 145/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1190 - acc: 0.9532 - val_loss: 0.1233 - val_acc: 0.9468\n",
      "Epoch 146/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1179 - acc: 0.9551 - val_loss: 0.1286 - val_acc: 0.9483\n",
      "Epoch 147/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1242 - acc: 0.9524 - val_loss: 0.1210 - val_acc: 0.9583\n",
      "Epoch 148/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1291 - acc: 0.9491 - val_loss: 0.1236 - val_acc: 0.9526\n",
      "Epoch 149/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1232 - acc: 0.9511 - val_loss: 0.1377 - val_acc: 0.9497\n",
      "Epoch 150/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1237 - acc: 0.9497 - val_loss: 0.1201 - val_acc: 0.9526\n",
      "Epoch 151/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1234 - acc: 0.9483 - val_loss: 0.1193 - val_acc: 0.9555\n",
      "Epoch 152/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1227 - acc: 0.9529 - val_loss: 0.1205 - val_acc: 0.9540\n",
      "Epoch 153/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1174 - acc: 0.9503 - val_loss: 0.1379 - val_acc: 0.9497\n",
      "Epoch 154/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1205 - acc: 0.9511 - val_loss: 0.1126 - val_acc: 0.9540\n",
      "Epoch 155/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1193 - acc: 0.9497 - val_loss: 0.1278 - val_acc: 0.9511\n",
      "Epoch 156/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1202 - acc: 0.9500 - val_loss: 0.1253 - val_acc: 0.9511\n",
      "Epoch 157/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1165 - acc: 0.9548 - val_loss: 0.1123 - val_acc: 0.9569\n",
      "Epoch 158/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1174 - acc: 0.9543 - val_loss: 0.1249 - val_acc: 0.9497\n",
      "Epoch 159/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1154 - acc: 0.9532 - val_loss: 0.1146 - val_acc: 0.9598\n",
      "Epoch 160/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1165 - acc: 0.9547 - val_loss: 0.1606 - val_acc: 0.9382\n",
      "Epoch 161/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1166 - acc: 0.9537 - val_loss: 0.1237 - val_acc: 0.9468\n",
      "Epoch 162/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1146 - acc: 0.9563 - val_loss: 0.1152 - val_acc: 0.9569\n",
      "Epoch 163/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1195 - acc: 0.9547 - val_loss: 0.1265 - val_acc: 0.9511\n",
      "Epoch 164/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1197 - acc: 0.9550 - val_loss: 0.1228 - val_acc: 0.9540\n",
      "Epoch 165/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1109 - acc: 0.9553 - val_loss: 0.1282 - val_acc: 0.9526\n",
      "Epoch 166/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1215 - acc: 0.9519 - val_loss: 0.1183 - val_acc: 0.9555\n",
      "Epoch 167/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1110 - acc: 0.9564 - val_loss: 0.1398 - val_acc: 0.9425\n",
      "Epoch 168/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1121 - acc: 0.9572 - val_loss: 0.1099 - val_acc: 0.9626\n",
      "Epoch 169/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1192 - acc: 0.9535 - val_loss: 0.1170 - val_acc: 0.9540\n",
      "Epoch 170/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1139 - acc: 0.9564 - val_loss: 0.1050 - val_acc: 0.9555\n",
      "Epoch 171/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1098 - acc: 0.9585 - val_loss: 0.1088 - val_acc: 0.9626\n",
      "Epoch 172/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1190 - acc: 0.9526 - val_loss: 0.1148 - val_acc: 0.9626\n",
      "Epoch 173/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1174 - acc: 0.9542 - val_loss: 0.1098 - val_acc: 0.9612\n",
      "Epoch 174/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1103 - acc: 0.9558 - val_loss: 0.1193 - val_acc: 0.9526\n",
      "Epoch 175/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1160 - acc: 0.9550 - val_loss: 0.1182 - val_acc: 0.9612\n",
      "Epoch 176/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1134 - acc: 0.9556 - val_loss: 0.1268 - val_acc: 0.9497\n",
      "Epoch 177/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1110 - acc: 0.9580 - val_loss: 0.1200 - val_acc: 0.9468\n",
      "Epoch 178/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1183 - acc: 0.9534 - val_loss: 0.1215 - val_acc: 0.9511\n",
      "Epoch 179/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1136 - acc: 0.9548 - val_loss: 0.1274 - val_acc: 0.9511\n",
      "Epoch 180/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1085 - acc: 0.9582 - val_loss: 0.1285 - val_acc: 0.9511\n",
      "Epoch 181/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1093 - acc: 0.9572 - val_loss: 0.1282 - val_acc: 0.9540\n",
      "Epoch 182/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1077 - acc: 0.9588 - val_loss: 0.1061 - val_acc: 0.9612\n",
      "Epoch 183/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1124 - acc: 0.9572 - val_loss: 0.1272 - val_acc: 0.9468\n",
      "Epoch 184/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1085 - acc: 0.9583 - val_loss: 0.1242 - val_acc: 0.9483\n",
      "Epoch 185/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1145 - acc: 0.9555 - val_loss: 0.1134 - val_acc: 0.9598\n",
      "Epoch 186/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1139 - acc: 0.9559 - val_loss: 0.1142 - val_acc: 0.9526\n",
      "Epoch 187/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1073 - acc: 0.9578 - val_loss: 0.1101 - val_acc: 0.9598\n",
      "Epoch 188/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1128 - acc: 0.9550 - val_loss: 0.1136 - val_acc: 0.9583\n",
      "Epoch 189/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1093 - acc: 0.9545 - val_loss: 0.1086 - val_acc: 0.9612\n",
      "Epoch 190/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1155 - acc: 0.9523 - val_loss: 0.1032 - val_acc: 0.9655\n",
      "Epoch 191/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1052 - acc: 0.9591 - val_loss: 0.1106 - val_acc: 0.9555\n",
      "Epoch 192/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1101 - acc: 0.9569 - val_loss: 0.1217 - val_acc: 0.9569\n",
      "Epoch 193/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1089 - acc: 0.9564 - val_loss: 0.1062 - val_acc: 0.9612\n",
      "Epoch 194/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1047 - acc: 0.9580 - val_loss: 0.1116 - val_acc: 0.9583\n",
      "Epoch 195/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1053 - acc: 0.9575 - val_loss: 0.1122 - val_acc: 0.9626\n",
      "Epoch 196/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1102 - acc: 0.9574 - val_loss: 0.1112 - val_acc: 0.9626\n",
      "Epoch 197/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1104 - acc: 0.9583 - val_loss: 0.1221 - val_acc: 0.9555\n",
      "Epoch 198/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1104 - acc: 0.9548 - val_loss: 0.1035 - val_acc: 0.9598\n",
      "Epoch 199/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1050 - acc: 0.9588 - val_loss: 0.1165 - val_acc: 0.9454\n",
      "Epoch 200/200\n",
      "49/49 [==============================] - 0s 5ms/step - loss: 0.1115 - acc: 0.9569 - val_loss: 0.1063 - val_acc: 0.9641\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'RMSprop', loss = 'sparse_categorical_crossentropy',  metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs = 200, batch_size = 128, verbose = 1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training - Loss Function')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.subplot()\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Training - Loss Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (train with  whole training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "55/55 [==============================] - 2s 5ms/step - loss: 1.3291 - acc: 0.5189\n",
      "Epoch 2/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.7583 - acc: 0.6676\n",
      "Epoch 3/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.5770 - acc: 0.7288\n",
      "Epoch 4/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.5224 - acc: 0.7642\n",
      "Epoch 5/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4706 - acc: 0.7957\n",
      "Epoch 6/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4498 - acc: 0.8050\n",
      "Epoch 7/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.4243 - acc: 0.8227\n",
      "Epoch 8/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3949 - acc: 0.8323\n",
      "Epoch 9/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3755 - acc: 0.8416\n",
      "Epoch 10/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3581 - acc: 0.8510\n",
      "Epoch 11/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3495 - acc: 0.8552\n",
      "Epoch 12/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3245 - acc: 0.8700\n",
      "Epoch 13/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.3066 - acc: 0.8740\n",
      "Epoch 14/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.2972 - acc: 0.8809\n",
      "Epoch 15/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.2821 - acc: 0.8860\n",
      "Epoch 16/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.2667 - acc: 0.8957\n",
      "Epoch 17/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.2511 - acc: 0.9050\n",
      "Epoch 18/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.2520 - acc: 0.9001\n",
      "Epoch 19/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.2426 - acc: 0.9040\n",
      "Epoch 20/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.2296 - acc: 0.9098\n",
      "Epoch 21/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.2330 - acc: 0.9085\n",
      "Epoch 22/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.2307 - acc: 0.9092\n",
      "Epoch 23/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.2276 - acc: 0.9121\n",
      "Epoch 24/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.2170 - acc: 0.9169\n",
      "Epoch 25/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.2126 - acc: 0.9213\n",
      "Epoch 26/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.2096 - acc: 0.9231\n",
      "Epoch 27/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.2156 - acc: 0.9155\n",
      "Epoch 28/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.2071 - acc: 0.9215\n",
      "Epoch 29/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.2069 - acc: 0.9202\n",
      "Epoch 30/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1985 - acc: 0.9228\n",
      "Epoch 31/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.2032 - acc: 0.9234\n",
      "Epoch 32/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1923 - acc: 0.9266\n",
      "Epoch 33/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1893 - acc: 0.9263\n",
      "Epoch 34/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1843 - acc: 0.9289\n",
      "Epoch 35/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1919 - acc: 0.9274\n",
      "Epoch 36/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1935 - acc: 0.9267\n",
      "Epoch 37/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1911 - acc: 0.9276\n",
      "Epoch 38/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1858 - acc: 0.9304\n",
      "Epoch 39/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1819 - acc: 0.9289\n",
      "Epoch 40/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1854 - acc: 0.9296\n",
      "Epoch 41/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1830 - acc: 0.9320\n",
      "Epoch 42/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1791 - acc: 0.9315\n",
      "Epoch 43/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1802 - acc: 0.9326\n",
      "Epoch 44/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1817 - acc: 0.9290\n",
      "Epoch 45/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1730 - acc: 0.9346\n",
      "Epoch 46/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1761 - acc: 0.9332\n",
      "Epoch 47/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1702 - acc: 0.9362\n",
      "Epoch 48/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1700 - acc: 0.9338\n",
      "Epoch 49/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1725 - acc: 0.9313\n",
      "Epoch 50/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1694 - acc: 0.9365\n",
      "Epoch 51/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1674 - acc: 0.9372\n",
      "Epoch 52/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1610 - acc: 0.9386\n",
      "Epoch 53/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1635 - acc: 0.9398\n",
      "Epoch 54/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1664 - acc: 0.9336\n",
      "Epoch 55/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1643 - acc: 0.9361\n",
      "Epoch 56/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1661 - acc: 0.9386\n",
      "Epoch 57/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1609 - acc: 0.9369\n",
      "Epoch 58/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1581 - acc: 0.9402\n",
      "Epoch 59/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1575 - acc: 0.9404\n",
      "Epoch 60/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1629 - acc: 0.9381\n",
      "Epoch 61/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1572 - acc: 0.9407\n",
      "Epoch 62/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1572 - acc: 0.9389\n",
      "Epoch 63/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1566 - acc: 0.9399\n",
      "Epoch 64/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1559 - acc: 0.9415\n",
      "Epoch 65/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1606 - acc: 0.9382\n",
      "Epoch 66/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1630 - acc: 0.9384\n",
      "Epoch 67/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1549 - acc: 0.9419\n",
      "Epoch 68/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1576 - acc: 0.9424\n",
      "Epoch 69/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1560 - acc: 0.9399\n",
      "Epoch 70/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1561 - acc: 0.9388\n",
      "Epoch 71/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1504 - acc: 0.9409\n",
      "Epoch 72/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1559 - acc: 0.9382\n",
      "Epoch 73/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1482 - acc: 0.9394\n",
      "Epoch 74/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1552 - acc: 0.9391\n",
      "Epoch 75/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1618 - acc: 0.9379\n",
      "Epoch 76/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1501 - acc: 0.9407\n",
      "Epoch 77/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1513 - acc: 0.9407\n",
      "Epoch 78/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1474 - acc: 0.9425\n",
      "Epoch 79/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1540 - acc: 0.9417\n",
      "Epoch 80/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1474 - acc: 0.9428\n",
      "Epoch 81/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1478 - acc: 0.9425\n",
      "Epoch 82/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1491 - acc: 0.9412\n",
      "Epoch 83/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1437 - acc: 0.9428\n",
      "Epoch 84/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1496 - acc: 0.9418\n",
      "Epoch 85/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1505 - acc: 0.9396\n",
      "Epoch 86/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1460 - acc: 0.9430\n",
      "Epoch 87/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1431 - acc: 0.9461\n",
      "Epoch 88/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1443 - acc: 0.9460\n",
      "Epoch 89/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1346 - acc: 0.9481\n",
      "Epoch 90/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1412 - acc: 0.9463\n",
      "Epoch 91/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1430 - acc: 0.9442\n",
      "Epoch 92/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1445 - acc: 0.9448\n",
      "Epoch 93/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1402 - acc: 0.9435\n",
      "Epoch 94/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1409 - acc: 0.9455\n",
      "Epoch 95/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1445 - acc: 0.9442\n",
      "Epoch 96/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1313 - acc: 0.9510\n",
      "Epoch 97/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1346 - acc: 0.9484\n",
      "Epoch 98/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1423 - acc: 0.9445\n",
      "Epoch 99/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1362 - acc: 0.9454\n",
      "Epoch 100/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1379 - acc: 0.9464\n",
      "Epoch 101/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1383 - acc: 0.9471\n",
      "Epoch 102/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1340 - acc: 0.9473\n",
      "Epoch 103/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1310 - acc: 0.9477\n",
      "Epoch 104/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1385 - acc: 0.9480\n",
      "Epoch 105/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1340 - acc: 0.9474\n",
      "Epoch 106/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1352 - acc: 0.9464\n",
      "Epoch 107/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1399 - acc: 0.9448\n",
      "Epoch 108/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1331 - acc: 0.9491\n",
      "Epoch 109/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1343 - acc: 0.9511\n",
      "Epoch 110/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1353 - acc: 0.9488\n",
      "Epoch 111/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1350 - acc: 0.9471\n",
      "Epoch 112/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1345 - acc: 0.9460\n",
      "Epoch 113/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1385 - acc: 0.9461\n",
      "Epoch 114/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1329 - acc: 0.9480\n",
      "Epoch 115/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1321 - acc: 0.9524\n",
      "Epoch 116/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1332 - acc: 0.9507\n",
      "Epoch 117/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1296 - acc: 0.9501\n",
      "Epoch 118/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1334 - acc: 0.9470\n",
      "Epoch 119/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1319 - acc: 0.9511\n",
      "Epoch 120/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1285 - acc: 0.9501\n",
      "Epoch 121/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1328 - acc: 0.9486\n",
      "Epoch 122/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1290 - acc: 0.9513\n",
      "Epoch 123/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1307 - acc: 0.9487\n",
      "Epoch 124/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1258 - acc: 0.9514\n",
      "Epoch 125/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1277 - acc: 0.9487\n",
      "Epoch 126/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1291 - acc: 0.9507\n",
      "Epoch 127/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1303 - acc: 0.9488\n",
      "Epoch 128/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1306 - acc: 0.9480\n",
      "Epoch 129/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1339 - acc: 0.9504\n",
      "Epoch 130/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1311 - acc: 0.9460\n",
      "Epoch 131/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1288 - acc: 0.9503\n",
      "Epoch 132/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1260 - acc: 0.9509\n",
      "Epoch 133/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1304 - acc: 0.9497\n",
      "Epoch 134/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1332 - acc: 0.9487\n",
      "Epoch 135/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1251 - acc: 0.9519\n",
      "Epoch 136/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1252 - acc: 0.9520\n",
      "Epoch 137/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1228 - acc: 0.9504\n",
      "Epoch 138/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1248 - acc: 0.9503\n",
      "Epoch 139/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1251 - acc: 0.9540\n",
      "Epoch 140/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1267 - acc: 0.9516\n",
      "Epoch 141/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1257 - acc: 0.9509\n",
      "Epoch 142/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1247 - acc: 0.9506\n",
      "Epoch 143/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1257 - acc: 0.9529\n",
      "Epoch 144/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1295 - acc: 0.9486\n",
      "Epoch 145/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1234 - acc: 0.9540\n",
      "Epoch 146/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1261 - acc: 0.9503\n",
      "Epoch 147/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1236 - acc: 0.9513\n",
      "Epoch 148/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1257 - acc: 0.9530\n",
      "Epoch 149/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1245 - acc: 0.9529\n",
      "Epoch 150/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1245 - acc: 0.9497\n",
      "Epoch 151/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1233 - acc: 0.9523\n",
      "Epoch 152/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1227 - acc: 0.9549\n",
      "Epoch 153/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1284 - acc: 0.9483\n",
      "Epoch 154/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1265 - acc: 0.9520\n",
      "Epoch 155/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1236 - acc: 0.9532\n",
      "Epoch 156/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1265 - acc: 0.9497\n",
      "Epoch 157/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1302 - acc: 0.9490\n",
      "Epoch 158/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1232 - acc: 0.9516\n",
      "Epoch 159/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1227 - acc: 0.9529\n",
      "Epoch 160/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1195 - acc: 0.9524\n",
      "Epoch 161/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1183 - acc: 0.9540\n",
      "Epoch 162/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1215 - acc: 0.9509\n",
      "Epoch 163/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1226 - acc: 0.9534\n",
      "Epoch 164/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1231 - acc: 0.9519\n",
      "Epoch 165/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1178 - acc: 0.9537\n",
      "Epoch 166/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1191 - acc: 0.9529\n",
      "Epoch 167/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1143 - acc: 0.9540\n",
      "Epoch 168/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1220 - acc: 0.9526\n",
      "Epoch 169/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1208 - acc: 0.9537\n",
      "Epoch 170/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1188 - acc: 0.9524\n",
      "Epoch 171/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1156 - acc: 0.9547\n",
      "Epoch 172/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1200 - acc: 0.9532\n",
      "Epoch 173/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1210 - acc: 0.9532\n",
      "Epoch 174/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1178 - acc: 0.9559\n",
      "Epoch 175/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1209 - acc: 0.9513\n",
      "Epoch 176/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1198 - acc: 0.9529\n",
      "Epoch 177/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1130 - acc: 0.9565\n",
      "Epoch 178/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1201 - acc: 0.9550\n",
      "Epoch 179/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1210 - acc: 0.9523\n",
      "Epoch 180/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1158 - acc: 0.9526\n",
      "Epoch 181/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1167 - acc: 0.9553\n",
      "Epoch 182/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1176 - acc: 0.9546\n",
      "Epoch 183/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1188 - acc: 0.9533\n",
      "Epoch 184/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1162 - acc: 0.9530\n",
      "Epoch 185/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1169 - acc: 0.9559\n",
      "Epoch 186/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1135 - acc: 0.9557\n",
      "Epoch 187/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1167 - acc: 0.9553\n",
      "Epoch 188/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1132 - acc: 0.9563\n",
      "Epoch 189/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1194 - acc: 0.9543\n",
      "Epoch 190/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1170 - acc: 0.9543\n",
      "Epoch 191/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1109 - acc: 0.9544\n",
      "Epoch 192/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1120 - acc: 0.9557\n",
      "Epoch 193/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1146 - acc: 0.9557\n",
      "Epoch 194/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1141 - acc: 0.9546\n",
      "Epoch 195/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1129 - acc: 0.9553\n",
      "Epoch 196/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1087 - acc: 0.9570\n",
      "Epoch 197/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1094 - acc: 0.9572\n",
      "Epoch 198/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1109 - acc: 0.9546\n",
      "Epoch 199/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1129 - acc: 0.9550\n",
      "Epoch 200/200\n",
      "55/55 [==============================] - 0s 5ms/step - loss: 0.1098 - acc: 0.9546\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()#Adding the first SimpleRNN layer and some Dropout regularisation\n",
    "model.add(SimpleRNN(units = 30, activation='relu', return_sequences=True, input_shape = (X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))# Adding a second SimpleRNN layer and some Dropout regularisation\n",
    "model.add(SimpleRNN(units = 30, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.2))# Adding a third SimpleRNN layer and some Dropout regularisation\n",
    "model.add(SimpleRNN(units = 30, activation='relu'))\n",
    "model.add(Dense(units = 5, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer = 'RMSprop', loss = 'sparse_categorical_crossentropy',  metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs = 200, batch_size = 128, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training - Loss Function')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.subplot()\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Training - Loss Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.09488860e-02, 4.86623915e-03, 9.34169531e-01, 1.43453944e-05,\n",
       "        9.82873985e-07],\n",
       "       [1.87880706e-26, 9.08546086e-11, 4.38092829e-05, 5.80884865e-04,\n",
       "        9.99375284e-01],\n",
       "       [9.99999940e-01, 7.91249932e-14, 8.63226117e-14, 1.49177453e-24,\n",
       "        4.60048202e-30],\n",
       "       ...,\n",
       "       [9.99999881e-01, 6.08180386e-08, 3.46723148e-08, 1.35479515e-14,\n",
       "        1.58159195e-21],\n",
       "       [7.33036315e-03, 8.50714514e-06, 9.92661119e-01, 1.06530426e-13,\n",
       "        5.31296969e-15],\n",
       "       [1.00000000e+00, 5.47971113e-10, 4.02268756e-12, 3.12057890e-17,\n",
       "        1.48731488e-23]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob = model.predict(X_test)\n",
    "y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mac_hash</th>\n",
       "      <th>C0</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b882f1d44602a25349a08f4a0af32977</td>\n",
       "      <td>6.094889e-02</td>\n",
       "      <td>4.866239e-03</td>\n",
       "      <td>9.341695e-01</td>\n",
       "      <td>1.434539e-05</td>\n",
       "      <td>9.828740e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86e644b498922f3a2fd0e6e1a1525de3</td>\n",
       "      <td>1.878807e-26</td>\n",
       "      <td>9.085461e-11</td>\n",
       "      <td>4.380928e-05</td>\n",
       "      <td>5.808849e-04</td>\n",
       "      <td>9.993753e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>691662b04ee08015062d901a4c5628b1</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>7.912499e-14</td>\n",
       "      <td>8.632261e-14</td>\n",
       "      <td>1.491775e-24</td>\n",
       "      <td>4.600482e-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52b5c510a28774237f4f118764c2ed6f</td>\n",
       "      <td>1.861883e-03</td>\n",
       "      <td>9.980972e-01</td>\n",
       "      <td>8.035893e-06</td>\n",
       "      <td>3.280407e-05</td>\n",
       "      <td>3.557997e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9f3c995e53d109f532056b6eae29a0b5</td>\n",
       "      <td>4.105323e-08</td>\n",
       "      <td>1.495906e-03</td>\n",
       "      <td>9.983988e-01</td>\n",
       "      <td>3.995387e-05</td>\n",
       "      <td>6.525678e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>5001c0dfe522886ed884c0dc4e5848df</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.974770e-10</td>\n",
       "      <td>3.287433e-12</td>\n",
       "      <td>1.114474e-17</td>\n",
       "      <td>5.139043e-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425</th>\n",
       "      <td>f1c8781e701e89068fa0b8a62ecbc564</td>\n",
       "      <td>1.937212e-23</td>\n",
       "      <td>1.269270e-08</td>\n",
       "      <td>3.701468e-05</td>\n",
       "      <td>1.126766e-03</td>\n",
       "      <td>9.988362e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426</th>\n",
       "      <td>646136b402e136422466a2acd8636630</td>\n",
       "      <td>9.999999e-01</td>\n",
       "      <td>6.081804e-08</td>\n",
       "      <td>3.467231e-08</td>\n",
       "      <td>1.354795e-14</td>\n",
       "      <td>1.581592e-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427</th>\n",
       "      <td>c7bea6491029f86ab3d1f0f9b599ca2c</td>\n",
       "      <td>7.330363e-03</td>\n",
       "      <td>8.507145e-06</td>\n",
       "      <td>9.926611e-01</td>\n",
       "      <td>1.065304e-13</td>\n",
       "      <td>5.312970e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428</th>\n",
       "      <td>822fd3104f38cc6374f2996d3d6920a2</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.479711e-10</td>\n",
       "      <td>4.022688e-12</td>\n",
       "      <td>3.120579e-17</td>\n",
       "      <td>1.487315e-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3429 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              mac_hash            C0            C1  \\\n",
       "0     b882f1d44602a25349a08f4a0af32977  6.094889e-02  4.866239e-03   \n",
       "1     86e644b498922f3a2fd0e6e1a1525de3  1.878807e-26  9.085461e-11   \n",
       "2     691662b04ee08015062d901a4c5628b1  9.999999e-01  7.912499e-14   \n",
       "3     52b5c510a28774237f4f118764c2ed6f  1.861883e-03  9.980972e-01   \n",
       "4     9f3c995e53d109f532056b6eae29a0b5  4.105323e-08  1.495906e-03   \n",
       "...                                ...           ...           ...   \n",
       "3424  5001c0dfe522886ed884c0dc4e5848df  1.000000e+00  2.974770e-10   \n",
       "3425  f1c8781e701e89068fa0b8a62ecbc564  1.937212e-23  1.269270e-08   \n",
       "3426  646136b402e136422466a2acd8636630  9.999999e-01  6.081804e-08   \n",
       "3427  c7bea6491029f86ab3d1f0f9b599ca2c  7.330363e-03  8.507145e-06   \n",
       "3428  822fd3104f38cc6374f2996d3d6920a2  1.000000e+00  5.479711e-10   \n",
       "\n",
       "                C2            C3            C4  \n",
       "0     9.341695e-01  1.434539e-05  9.828740e-07  \n",
       "1     4.380928e-05  5.808849e-04  9.993753e-01  \n",
       "2     8.632261e-14  1.491775e-24  4.600482e-30  \n",
       "3     8.035893e-06  3.280407e-05  3.557997e-16  \n",
       "4     9.983988e-01  3.995387e-05  6.525678e-05  \n",
       "...            ...           ...           ...  \n",
       "3424  3.287433e-12  1.114474e-17  5.139043e-24  \n",
       "3425  3.701468e-05  1.126766e-03  9.988362e-01  \n",
       "3426  3.467231e-08  1.354795e-14  1.581592e-21  \n",
       "3427  9.926611e-01  1.065304e-13  5.312970e-15  \n",
       "3428  4.022688e-12  3.120579e-17  1.487315e-23  \n",
       "\n",
       "[3429 rows x 6 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame({'mac_hash':sample['mac_hash'], 'C0':y_prob[:,0], 'C1':y_prob[:,1], 'C2':y_prob[:,2], 'C3':y_prob[:,3], 'C4':y_prob[:,4]})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"output_RNN.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#FF0000> Transformer Bert (w/o group)</font> \t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (with validation set spiltted from training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_onehot = np.eye(5)[y_train]\n",
    "y_train_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_seq (InputLayer)         [(None, 14)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 14)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  40160       ['input_seq[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 14,                                                \n",
      "                                16),                                                              \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 16),                                                           \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (Slic  (None, 16)          0           ['tf_bert_model[0][0]']          \n",
      " ingOpLambda)                                                                                     \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 32)           544         ['tf.__operators__.getitem[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 5)            165         ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 40,869\n",
      "Trainable params: 40,869\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig(vocab_size=15, hidden_size=16, num_hidden_layer=8, num_attention_heads=4,\n",
    "                    intermediate_size=64, max_position_embeddings=14)\n",
    "bert = TFBertModel(config)\n",
    "\n",
    "input_seq = tf.keras.layers.Input(shape=(14,), name='input_seq', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(14,), name='attention_mask', dtype='int32')\n",
    "\n",
    "encoder = bert(input_seq, attention_mask=mask)[0][:,0] \n",
    "\n",
    "x = tf.keras.layers.Dense(32, activation='relu')(encoder)\n",
    "y = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_seq, mask], outputs=y)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, decay=1e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "7/7 [==============================] - 14s 558ms/step - loss: 1.6325 - accuracy: 0.2646 - val_loss: 1.4857 - val_accuracy: 0.4023\n",
      "Epoch 2/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 1.4855 - accuracy: 0.3883 - val_loss: 1.4406 - val_accuracy: 0.4023\n",
      "Epoch 3/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 1.4227 - accuracy: 0.3896 - val_loss: 1.3116 - val_accuracy: 0.4368\n",
      "Epoch 4/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 1.2666 - accuracy: 0.5307 - val_loss: 1.0727 - val_accuracy: 0.6322\n",
      "Epoch 5/500\n",
      "7/7 [==============================] - 2s 239ms/step - loss: 1.0398 - accuracy: 0.6222 - val_loss: 0.9159 - val_accuracy: 0.6552\n",
      "Epoch 6/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.8983 - accuracy: 0.6467 - val_loss: 0.8151 - val_accuracy: 0.6911\n",
      "Epoch 7/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.8008 - accuracy: 0.6957 - val_loss: 0.7356 - val_accuracy: 0.7428\n",
      "Epoch 8/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.7277 - accuracy: 0.7324 - val_loss: 0.6766 - val_accuracy: 0.7514\n",
      "Epoch 9/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.6679 - accuracy: 0.7380 - val_loss: 0.6171 - val_accuracy: 0.7644\n",
      "Epoch 10/500\n",
      "7/7 [==============================] - 2s 238ms/step - loss: 0.6142 - accuracy: 0.7520 - val_loss: 0.5708 - val_accuracy: 0.7859\n",
      "Epoch 11/500\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 0.5787 - accuracy: 0.7565[CV] END ......................................., score=0.950 total time=   2.0s\n",
      "7/7 [==============================] - 2s 239ms/step - loss: 0.5778 - accuracy: 0.7571 - val_loss: 0.5370 - val_accuracy: 0.7773\n",
      "Epoch 12/500\n",
      "[CV] END ......................................., score=0.945 total time=   2.2s\n",
      "[CV] END ......................................., score=0.955 total time=   2.1s\n",
      "[CV] END ......................................., score=0.959 total time=   2.1s\n",
      "1/7 [===>..........................] - ETA: 1s - loss: 0.5740 - accuracy: 0.7480[CV] END ......................................., score=0.958 total time=   2.1s\n",
      "7/7 [==============================] - 2s 245ms/step - loss: 0.5444 - accuracy: 0.7656 - val_loss: 0.5140 - val_accuracy: 0.7859\n",
      "Epoch 13/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.5225 - accuracy: 0.7717 - val_loss: 0.5040 - val_accuracy: 0.7945\n",
      "Epoch 14/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.4996 - accuracy: 0.7856 - val_loss: 0.4820 - val_accuracy: 0.8003\n",
      "Epoch 15/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.4818 - accuracy: 0.7881 - val_loss: 0.4804 - val_accuracy: 0.8060\n",
      "Epoch 16/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.4768 - accuracy: 0.7979 - val_loss: 0.4597 - val_accuracy: 0.8147\n",
      "Epoch 17/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.4609 - accuracy: 0.7996 - val_loss: 0.4382 - val_accuracy: 0.8405\n",
      "Epoch 18/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.4415 - accuracy: 0.8164 - val_loss: 0.4676 - val_accuracy: 0.8290\n",
      "Epoch 19/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.4287 - accuracy: 0.8276 - val_loss: 0.3964 - val_accuracy: 0.8348\n",
      "Epoch 20/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.4008 - accuracy: 0.8394 - val_loss: 0.3693 - val_accuracy: 0.8779\n",
      "Epoch 21/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.3776 - accuracy: 0.8549 - val_loss: 0.3344 - val_accuracy: 0.8764\n",
      "Epoch 22/500\n",
      "7/7 [==============================] - 2s 238ms/step - loss: 0.3518 - accuracy: 0.8605 - val_loss: 0.3422 - val_accuracy: 0.8520\n",
      "Epoch 23/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.3240 - accuracy: 0.8719 - val_loss: 0.2680 - val_accuracy: 0.9052\n",
      "Epoch 24/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.2980 - accuracy: 0.8889 - val_loss: 0.2522 - val_accuracy: 0.9210\n",
      "Epoch 25/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.2626 - accuracy: 0.9048 - val_loss: 0.1996 - val_accuracy: 0.9440\n",
      "Epoch 26/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.2321 - accuracy: 0.9213 - val_loss: 0.1660 - val_accuracy: 0.9598\n",
      "Epoch 27/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.2004 - accuracy: 0.9369 - val_loss: 0.1393 - val_accuracy: 0.9598\n",
      "Epoch 28/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.1797 - accuracy: 0.9428 - val_loss: 0.1407 - val_accuracy: 0.9626\n",
      "Epoch 29/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.1592 - accuracy: 0.9523 - val_loss: 0.1274 - val_accuracy: 0.9684\n",
      "Epoch 30/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.1593 - accuracy: 0.9534 - val_loss: 0.1171 - val_accuracy: 0.9612\n",
      "Epoch 31/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.1526 - accuracy: 0.9518 - val_loss: 0.1096 - val_accuracy: 0.9684\n",
      "Epoch 32/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.1471 - accuracy: 0.9529 - val_loss: 0.1173 - val_accuracy: 0.9612\n",
      "Epoch 33/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.1351 - accuracy: 0.9582 - val_loss: 0.1257 - val_accuracy: 0.9555\n",
      "Epoch 34/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.1368 - accuracy: 0.9561 - val_loss: 0.1050 - val_accuracy: 0.9713\n",
      "Epoch 35/500\n",
      "7/7 [==============================] - 2s 238ms/step - loss: 0.1339 - accuracy: 0.9561 - val_loss: 0.1207 - val_accuracy: 0.9612\n",
      "Epoch 36/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.1373 - accuracy: 0.9529 - val_loss: 0.1014 - val_accuracy: 0.9655\n",
      "Epoch 37/500\n",
      "7/7 [==============================] - 2s 239ms/step - loss: 0.1292 - accuracy: 0.9561 - val_loss: 0.1034 - val_accuracy: 0.9612\n",
      "Epoch 38/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.1290 - accuracy: 0.9577 - val_loss: 0.1113 - val_accuracy: 0.9583\n",
      "Epoch 39/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.1273 - accuracy: 0.9575 - val_loss: 0.1100 - val_accuracy: 0.9612\n",
      "Epoch 40/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.1186 - accuracy: 0.9628 - val_loss: 0.0894 - val_accuracy: 0.9684\n",
      "Epoch 41/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.1192 - accuracy: 0.9612 - val_loss: 0.1085 - val_accuracy: 0.9598\n",
      "Epoch 42/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.1193 - accuracy: 0.9618 - val_loss: 0.0915 - val_accuracy: 0.9698\n",
      "Epoch 43/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.1068 - accuracy: 0.9681 - val_loss: 0.1013 - val_accuracy: 0.9655\n",
      "Epoch 44/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.1117 - accuracy: 0.9652 - val_loss: 0.0908 - val_accuracy: 0.9698\n",
      "Epoch 45/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.1275 - accuracy: 0.9558 - val_loss: 0.0975 - val_accuracy: 0.9626\n",
      "Epoch 46/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.1149 - accuracy: 0.9612 - val_loss: 0.0865 - val_accuracy: 0.9741\n",
      "Epoch 47/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.1069 - accuracy: 0.9654 - val_loss: 0.1106 - val_accuracy: 0.9612\n",
      "Epoch 48/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.1092 - accuracy: 0.9647 - val_loss: 0.0961 - val_accuracy: 0.9626\n",
      "Epoch 49/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.1043 - accuracy: 0.9671 - val_loss: 0.0898 - val_accuracy: 0.9598\n",
      "Epoch 50/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.1022 - accuracy: 0.9669 - val_loss: 0.0828 - val_accuracy: 0.9770\n",
      "Epoch 51/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0994 - accuracy: 0.9697 - val_loss: 0.0760 - val_accuracy: 0.9770\n",
      "Epoch 52/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0942 - accuracy: 0.9735 - val_loss: 0.0735 - val_accuracy: 0.9784\n",
      "Epoch 53/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0924 - accuracy: 0.9749 - val_loss: 0.0688 - val_accuracy: 0.9842\n",
      "Epoch 54/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0941 - accuracy: 0.9714 - val_loss: 0.0796 - val_accuracy: 0.9741\n",
      "Epoch 55/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0989 - accuracy: 0.9677 - val_loss: 0.0692 - val_accuracy: 0.9799\n",
      "Epoch 56/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0942 - accuracy: 0.9705 - val_loss: 0.0797 - val_accuracy: 0.9756\n",
      "Epoch 57/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0966 - accuracy: 0.9692 - val_loss: 0.1296 - val_accuracy: 0.9511\n",
      "Epoch 58/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.1073 - accuracy: 0.9649 - val_loss: 0.0963 - val_accuracy: 0.9598\n",
      "Epoch 59/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0984 - accuracy: 0.9698 - val_loss: 0.0673 - val_accuracy: 0.9756\n",
      "Epoch 60/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0923 - accuracy: 0.9711 - val_loss: 0.0686 - val_accuracy: 0.9756\n",
      "Epoch 61/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0971 - accuracy: 0.9682 - val_loss: 0.0769 - val_accuracy: 0.9784\n",
      "Epoch 62/500\n",
      "7/7 [==============================] - 2s 238ms/step - loss: 0.0901 - accuracy: 0.9727 - val_loss: 0.0908 - val_accuracy: 0.9698\n",
      "Epoch 63/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0939 - accuracy: 0.9697 - val_loss: 0.0771 - val_accuracy: 0.9670\n",
      "Epoch 64/500\n",
      "7/7 [==============================] - 2s 238ms/step - loss: 0.0902 - accuracy: 0.9722 - val_loss: 0.0708 - val_accuracy: 0.9713\n",
      "Epoch 65/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0895 - accuracy: 0.9713 - val_loss: 0.0817 - val_accuracy: 0.9684\n",
      "Epoch 66/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0948 - accuracy: 0.9701 - val_loss: 0.0837 - val_accuracy: 0.9655\n",
      "Epoch 67/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0942 - accuracy: 0.9693 - val_loss: 0.0716 - val_accuracy: 0.9813\n",
      "Epoch 68/500\n",
      "7/7 [==============================] - 2s 238ms/step - loss: 0.0899 - accuracy: 0.9708 - val_loss: 0.0633 - val_accuracy: 0.9784\n",
      "Epoch 69/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0873 - accuracy: 0.9727 - val_loss: 0.0641 - val_accuracy: 0.9756\n",
      "Epoch 70/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0927 - accuracy: 0.9681 - val_loss: 0.0684 - val_accuracy: 0.9784\n",
      "Epoch 71/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0891 - accuracy: 0.9727 - val_loss: 0.0805 - val_accuracy: 0.9698\n",
      "Epoch 72/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0918 - accuracy: 0.9719 - val_loss: 0.0717 - val_accuracy: 0.9799\n",
      "Epoch 73/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0836 - accuracy: 0.9746 - val_loss: 0.0675 - val_accuracy: 0.9770\n",
      "Epoch 74/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0820 - accuracy: 0.9727 - val_loss: 0.0642 - val_accuracy: 0.9770\n",
      "Epoch 75/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0823 - accuracy: 0.9740 - val_loss: 0.0768 - val_accuracy: 0.9713\n",
      "Epoch 76/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0830 - accuracy: 0.9743 - val_loss: 0.0855 - val_accuracy: 0.9626\n",
      "Epoch 77/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0867 - accuracy: 0.9721 - val_loss: 0.0746 - val_accuracy: 0.9698\n",
      "Epoch 78/500\n",
      "7/7 [==============================] - 2s 238ms/step - loss: 0.0793 - accuracy: 0.9746 - val_loss: 0.0638 - val_accuracy: 0.9784\n",
      "Epoch 79/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0910 - accuracy: 0.9722 - val_loss: 0.0582 - val_accuracy: 0.9813\n",
      "Epoch 80/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0845 - accuracy: 0.9727 - val_loss: 0.0736 - val_accuracy: 0.9813\n",
      "Epoch 81/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0874 - accuracy: 0.9716 - val_loss: 0.0588 - val_accuracy: 0.9799\n",
      "Epoch 82/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0822 - accuracy: 0.9764 - val_loss: 0.0665 - val_accuracy: 0.9741\n",
      "Epoch 83/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0812 - accuracy: 0.9748 - val_loss: 0.0849 - val_accuracy: 0.9655\n",
      "Epoch 84/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0769 - accuracy: 0.9765 - val_loss: 0.0592 - val_accuracy: 0.9799\n",
      "Epoch 85/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0793 - accuracy: 0.9764 - val_loss: 0.0757 - val_accuracy: 0.9698\n",
      "Epoch 86/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0881 - accuracy: 0.9692 - val_loss: 0.0837 - val_accuracy: 0.9684\n",
      "Epoch 87/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0838 - accuracy: 0.9753 - val_loss: 0.0968 - val_accuracy: 0.9598\n",
      "Epoch 88/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0831 - accuracy: 0.9751 - val_loss: 0.0570 - val_accuracy: 0.9828\n",
      "Epoch 89/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0786 - accuracy: 0.9760 - val_loss: 0.0753 - val_accuracy: 0.9727\n",
      "Epoch 90/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0862 - accuracy: 0.9716 - val_loss: 0.0608 - val_accuracy: 0.9813\n",
      "Epoch 91/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0800 - accuracy: 0.9743 - val_loss: 0.0556 - val_accuracy: 0.9813\n",
      "Epoch 92/500\n",
      "7/7 [==============================] - 2s 242ms/step - loss: 0.0793 - accuracy: 0.9749 - val_loss: 0.0588 - val_accuracy: 0.9828\n",
      "Epoch 93/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0789 - accuracy: 0.9765 - val_loss: 0.0619 - val_accuracy: 0.9784\n",
      "Epoch 94/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0767 - accuracy: 0.9753 - val_loss: 0.0673 - val_accuracy: 0.9684\n",
      "Epoch 95/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0806 - accuracy: 0.9748 - val_loss: 0.0755 - val_accuracy: 0.9698\n",
      "Epoch 96/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0766 - accuracy: 0.9770 - val_loss: 0.0656 - val_accuracy: 0.9784\n",
      "Epoch 97/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0792 - accuracy: 0.9746 - val_loss: 0.0555 - val_accuracy: 0.9799\n",
      "Epoch 98/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0735 - accuracy: 0.9772 - val_loss: 0.0582 - val_accuracy: 0.9784\n",
      "Epoch 99/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0717 - accuracy: 0.9778 - val_loss: 0.0546 - val_accuracy: 0.9784\n",
      "Epoch 100/500\n",
      "7/7 [==============================] - 2s 255ms/step - loss: 0.0708 - accuracy: 0.9784 - val_loss: 0.0772 - val_accuracy: 0.9684\n",
      "Epoch 101/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0748 - accuracy: 0.9745 - val_loss: 0.0871 - val_accuracy: 0.9641\n",
      "Epoch 102/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0808 - accuracy: 0.9741 - val_loss: 0.0549 - val_accuracy: 0.9828\n",
      "Epoch 103/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0789 - accuracy: 0.9732 - val_loss: 0.0569 - val_accuracy: 0.9813\n",
      "Epoch 104/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0772 - accuracy: 0.9719 - val_loss: 0.0612 - val_accuracy: 0.9813\n",
      "Epoch 105/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0784 - accuracy: 0.9746 - val_loss: 0.0587 - val_accuracy: 0.9770\n",
      "Epoch 106/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0789 - accuracy: 0.9749 - val_loss: 0.0575 - val_accuracy: 0.9813\n",
      "Epoch 107/500\n",
      "7/7 [==============================] - 2s 238ms/step - loss: 0.0781 - accuracy: 0.9733 - val_loss: 0.0526 - val_accuracy: 0.9842\n",
      "Epoch 108/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0754 - accuracy: 0.9760 - val_loss: 0.0589 - val_accuracy: 0.9770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0739 - accuracy: 0.9762 - val_loss: 0.0724 - val_accuracy: 0.9655\n",
      "Epoch 110/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0765 - accuracy: 0.9756 - val_loss: 0.0800 - val_accuracy: 0.9626\n",
      "Epoch 111/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0880 - accuracy: 0.9721 - val_loss: 0.0661 - val_accuracy: 0.9698\n",
      "Epoch 112/500\n",
      "7/7 [==============================] - 2s 238ms/step - loss: 0.0806 - accuracy: 0.9741 - val_loss: 0.0656 - val_accuracy: 0.9741\n",
      "Epoch 113/500\n",
      "7/7 [==============================] - 2s 238ms/step - loss: 0.0876 - accuracy: 0.9717 - val_loss: 0.0589 - val_accuracy: 0.9727\n",
      "Epoch 114/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0754 - accuracy: 0.9740 - val_loss: 0.0786 - val_accuracy: 0.9727\n",
      "Epoch 115/500\n",
      "7/7 [==============================] - 2s 241ms/step - loss: 0.0753 - accuracy: 0.9751 - val_loss: 0.0576 - val_accuracy: 0.9784\n",
      "Epoch 116/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0688 - accuracy: 0.9783 - val_loss: 0.0601 - val_accuracy: 0.9756\n",
      "Epoch 117/500\n",
      "7/7 [==============================] - 2s 238ms/step - loss: 0.0759 - accuracy: 0.9762 - val_loss: 0.0503 - val_accuracy: 0.9813\n",
      "Epoch 118/500\n",
      "7/7 [==============================] - 2s 238ms/step - loss: 0.0820 - accuracy: 0.9721 - val_loss: 0.0499 - val_accuracy: 0.9842\n",
      "Epoch 119/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0693 - accuracy: 0.9788 - val_loss: 0.0588 - val_accuracy: 0.9770\n",
      "Epoch 120/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0706 - accuracy: 0.9783 - val_loss: 0.0521 - val_accuracy: 0.9828\n",
      "Epoch 121/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0737 - accuracy: 0.9770 - val_loss: 0.0557 - val_accuracy: 0.9799\n",
      "Epoch 122/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0738 - accuracy: 0.9772 - val_loss: 0.0498 - val_accuracy: 0.9813\n",
      "Epoch 123/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0737 - accuracy: 0.9759 - val_loss: 0.0504 - val_accuracy: 0.9871\n",
      "Epoch 124/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0777 - accuracy: 0.9756 - val_loss: 0.0624 - val_accuracy: 0.9784\n",
      "Epoch 125/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0718 - accuracy: 0.9791 - val_loss: 0.0591 - val_accuracy: 0.9727\n",
      "Epoch 126/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0723 - accuracy: 0.9784 - val_loss: 0.0511 - val_accuracy: 0.9784\n",
      "Epoch 127/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0694 - accuracy: 0.9786 - val_loss: 0.0518 - val_accuracy: 0.9799\n",
      "Epoch 128/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0668 - accuracy: 0.9789 - val_loss: 0.0489 - val_accuracy: 0.9842\n",
      "Epoch 129/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0716 - accuracy: 0.9778 - val_loss: 0.0557 - val_accuracy: 0.9784\n",
      "Epoch 130/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0736 - accuracy: 0.9764 - val_loss: 0.0447 - val_accuracy: 0.9871\n",
      "Epoch 131/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0699 - accuracy: 0.9781 - val_loss: 0.0541 - val_accuracy: 0.9813\n",
      "Epoch 132/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0742 - accuracy: 0.9772 - val_loss: 0.0486 - val_accuracy: 0.9828\n",
      "Epoch 133/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0718 - accuracy: 0.9784 - val_loss: 0.0526 - val_accuracy: 0.9856\n",
      "Epoch 134/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0655 - accuracy: 0.9776 - val_loss: 0.0435 - val_accuracy: 0.9871\n",
      "Epoch 135/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0689 - accuracy: 0.9786 - val_loss: 0.0607 - val_accuracy: 0.9799\n",
      "Epoch 136/500\n",
      "7/7 [==============================] - 2s 238ms/step - loss: 0.0722 - accuracy: 0.9781 - val_loss: 0.0462 - val_accuracy: 0.9842\n",
      "Epoch 137/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0685 - accuracy: 0.9797 - val_loss: 0.0538 - val_accuracy: 0.9813\n",
      "Epoch 138/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0712 - accuracy: 0.9776 - val_loss: 0.0565 - val_accuracy: 0.9741\n",
      "Epoch 139/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0676 - accuracy: 0.9789 - val_loss: 0.0668 - val_accuracy: 0.9799\n",
      "Epoch 140/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0759 - accuracy: 0.9749 - val_loss: 0.0767 - val_accuracy: 0.9655\n",
      "Epoch 141/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0692 - accuracy: 0.9780 - val_loss: 0.0802 - val_accuracy: 0.9626\n",
      "Epoch 142/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0778 - accuracy: 0.9741 - val_loss: 0.0552 - val_accuracy: 0.9842\n",
      "Epoch 143/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0707 - accuracy: 0.9778 - val_loss: 0.0566 - val_accuracy: 0.9741\n",
      "Epoch 144/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0657 - accuracy: 0.9799 - val_loss: 0.0575 - val_accuracy: 0.9784\n",
      "Epoch 145/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0717 - accuracy: 0.9784 - val_loss: 0.0714 - val_accuracy: 0.9670\n",
      "Epoch 146/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0692 - accuracy: 0.9800 - val_loss: 0.0558 - val_accuracy: 0.9756\n",
      "Epoch 147/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0661 - accuracy: 0.9797 - val_loss: 0.0616 - val_accuracy: 0.9784\n",
      "Epoch 148/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0732 - accuracy: 0.9773 - val_loss: 0.0611 - val_accuracy: 0.9713\n",
      "Epoch 149/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0749 - accuracy: 0.9760 - val_loss: 0.0717 - val_accuracy: 0.9784\n",
      "Epoch 150/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0746 - accuracy: 0.9757 - val_loss: 0.0492 - val_accuracy: 0.9828\n",
      "Epoch 151/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0629 - accuracy: 0.9813 - val_loss: 0.0473 - val_accuracy: 0.9842\n",
      "Epoch 152/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0629 - accuracy: 0.9808 - val_loss: 0.0509 - val_accuracy: 0.9813\n",
      "Epoch 153/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0654 - accuracy: 0.9797 - val_loss: 0.0489 - val_accuracy: 0.9813\n",
      "Epoch 154/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0680 - accuracy: 0.9804 - val_loss: 0.0466 - val_accuracy: 0.9856\n",
      "Epoch 155/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0689 - accuracy: 0.9780 - val_loss: 0.0585 - val_accuracy: 0.9741\n",
      "Epoch 156/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0613 - accuracy: 0.9823 - val_loss: 0.0518 - val_accuracy: 0.9828\n",
      "Epoch 157/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0645 - accuracy: 0.9794 - val_loss: 0.0513 - val_accuracy: 0.9770\n",
      "Epoch 158/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0643 - accuracy: 0.9786 - val_loss: 0.0605 - val_accuracy: 0.9784\n",
      "Epoch 159/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0649 - accuracy: 0.9807 - val_loss: 0.0470 - val_accuracy: 0.9842\n",
      "Epoch 160/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0645 - accuracy: 0.9818 - val_loss: 0.0493 - val_accuracy: 0.9856\n",
      "Epoch 161/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0658 - accuracy: 0.9796 - val_loss: 0.0483 - val_accuracy: 0.9828\n",
      "Epoch 162/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0648 - accuracy: 0.9799 - val_loss: 0.0469 - val_accuracy: 0.9828\n",
      "Epoch 163/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0614 - accuracy: 0.9804 - val_loss: 0.0514 - val_accuracy: 0.9856\n",
      "Epoch 164/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0693 - accuracy: 0.9784 - val_loss: 0.0463 - val_accuracy: 0.9813\n",
      "Epoch 165/500\n",
      "7/7 [==============================] - 2s 233ms/step - loss: 0.0659 - accuracy: 0.9813 - val_loss: 0.0665 - val_accuracy: 0.9741\n",
      "Epoch 166/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0680 - accuracy: 0.9784 - val_loss: 0.0476 - val_accuracy: 0.9799\n",
      "Epoch 167/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0648 - accuracy: 0.9807 - val_loss: 0.0426 - val_accuracy: 0.9885\n",
      "Epoch 168/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0634 - accuracy: 0.9805 - val_loss: 0.0539 - val_accuracy: 0.9842\n",
      "Epoch 169/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0633 - accuracy: 0.9815 - val_loss: 0.0489 - val_accuracy: 0.9770\n",
      "Epoch 170/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0625 - accuracy: 0.9820 - val_loss: 0.0472 - val_accuracy: 0.9842\n",
      "Epoch 171/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0635 - accuracy: 0.9797 - val_loss: 0.0610 - val_accuracy: 0.9713\n",
      "Epoch 172/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0649 - accuracy: 0.9802 - val_loss: 0.0514 - val_accuracy: 0.9799\n",
      "Epoch 173/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0688 - accuracy: 0.9783 - val_loss: 0.0597 - val_accuracy: 0.9799\n",
      "Epoch 174/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0672 - accuracy: 0.9783 - val_loss: 0.0544 - val_accuracy: 0.9799\n",
      "Epoch 175/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0662 - accuracy: 0.9794 - val_loss: 0.0479 - val_accuracy: 0.9828\n",
      "Epoch 176/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0653 - accuracy: 0.9796 - val_loss: 0.0604 - val_accuracy: 0.9813\n",
      "Epoch 177/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0703 - accuracy: 0.9788 - val_loss: 0.0546 - val_accuracy: 0.9842\n",
      "Epoch 178/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0653 - accuracy: 0.9794 - val_loss: 0.0455 - val_accuracy: 0.9828\n",
      "Epoch 179/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0671 - accuracy: 0.9800 - val_loss: 0.0809 - val_accuracy: 0.9655\n",
      "Epoch 180/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0754 - accuracy: 0.9762 - val_loss: 0.0449 - val_accuracy: 0.9871\n",
      "Epoch 181/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0688 - accuracy: 0.9773 - val_loss: 0.0552 - val_accuracy: 0.9799\n",
      "Epoch 182/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0688 - accuracy: 0.9775 - val_loss: 0.0463 - val_accuracy: 0.9842\n",
      "Epoch 183/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0825 - accuracy: 0.9721 - val_loss: 0.0582 - val_accuracy: 0.9770\n",
      "Epoch 184/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0689 - accuracy: 0.9784 - val_loss: 0.0695 - val_accuracy: 0.9698\n",
      "Epoch 185/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0669 - accuracy: 0.9794 - val_loss: 0.0575 - val_accuracy: 0.9756\n",
      "Epoch 186/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0643 - accuracy: 0.9807 - val_loss: 0.0466 - val_accuracy: 0.9828\n",
      "Epoch 187/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0597 - accuracy: 0.9823 - val_loss: 0.0452 - val_accuracy: 0.9828\n",
      "Epoch 188/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0630 - accuracy: 0.9812 - val_loss: 0.0535 - val_accuracy: 0.9727\n",
      "Epoch 189/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0601 - accuracy: 0.9824 - val_loss: 0.0435 - val_accuracy: 0.9813\n",
      "Epoch 190/500\n",
      "7/7 [==============================] - 2s 246ms/step - loss: 0.0605 - accuracy: 0.9823 - val_loss: 0.0409 - val_accuracy: 0.9871\n",
      "Epoch 191/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0621 - accuracy: 0.9807 - val_loss: 0.0478 - val_accuracy: 0.9813\n",
      "Epoch 192/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0654 - accuracy: 0.9804 - val_loss: 0.0519 - val_accuracy: 0.9770\n",
      "Epoch 193/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0704 - accuracy: 0.9775 - val_loss: 0.0797 - val_accuracy: 0.9770\n",
      "Epoch 194/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0666 - accuracy: 0.9796 - val_loss: 0.0515 - val_accuracy: 0.9756\n",
      "Epoch 195/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0607 - accuracy: 0.9818 - val_loss: 0.0431 - val_accuracy: 0.9856\n",
      "Epoch 196/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0658 - accuracy: 0.9797 - val_loss: 0.0503 - val_accuracy: 0.9813\n",
      "Epoch 197/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0629 - accuracy: 0.9804 - val_loss: 0.0490 - val_accuracy: 0.9784\n",
      "Epoch 198/500\n",
      "7/7 [==============================] - 2s 238ms/step - loss: 0.0603 - accuracy: 0.9823 - val_loss: 0.0525 - val_accuracy: 0.9813\n",
      "Epoch 199/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0624 - accuracy: 0.9804 - val_loss: 0.0432 - val_accuracy: 0.9842\n",
      "Epoch 200/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0617 - accuracy: 0.9821 - val_loss: 0.0429 - val_accuracy: 0.9914\n",
      "Epoch 201/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0618 - accuracy: 0.9804 - val_loss: 0.0533 - val_accuracy: 0.9756\n",
      "Epoch 202/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0614 - accuracy: 0.9821 - val_loss: 0.0572 - val_accuracy: 0.9813\n",
      "Epoch 203/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0583 - accuracy: 0.9831 - val_loss: 0.0438 - val_accuracy: 0.9842\n",
      "Epoch 204/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0611 - accuracy: 0.9820 - val_loss: 0.0493 - val_accuracy: 0.9856\n",
      "Epoch 205/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0656 - accuracy: 0.9788 - val_loss: 0.0525 - val_accuracy: 0.9871\n",
      "Epoch 206/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0648 - accuracy: 0.9788 - val_loss: 0.0410 - val_accuracy: 0.9856\n",
      "Epoch 207/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0628 - accuracy: 0.9824 - val_loss: 0.0463 - val_accuracy: 0.9799\n",
      "Epoch 208/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0591 - accuracy: 0.9826 - val_loss: 0.0456 - val_accuracy: 0.9842\n",
      "Epoch 209/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0610 - accuracy: 0.9805 - val_loss: 0.0415 - val_accuracy: 0.9856\n",
      "Epoch 210/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0614 - accuracy: 0.9800 - val_loss: 0.0686 - val_accuracy: 0.9684\n",
      "Epoch 211/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0659 - accuracy: 0.9786 - val_loss: 0.0439 - val_accuracy: 0.9856\n",
      "Epoch 212/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0607 - accuracy: 0.9813 - val_loss: 0.0562 - val_accuracy: 0.9784\n",
      "Epoch 213/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0610 - accuracy: 0.9807 - val_loss: 0.0449 - val_accuracy: 0.9856\n",
      "Epoch 214/500\n",
      "7/7 [==============================] - 2s 233ms/step - loss: 0.0594 - accuracy: 0.9823 - val_loss: 0.0507 - val_accuracy: 0.9756\n",
      "Epoch 215/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0606 - accuracy: 0.9812 - val_loss: 0.0431 - val_accuracy: 0.9856\n",
      "Epoch 216/500\n",
      "7/7 [==============================] - 2s 257ms/step - loss: 0.0579 - accuracy: 0.9828 - val_loss: 0.0430 - val_accuracy: 0.9856\n",
      "Epoch 217/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0584 - accuracy: 0.9832 - val_loss: 0.0412 - val_accuracy: 0.9871\n",
      "Epoch 218/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0624 - accuracy: 0.9805 - val_loss: 0.0495 - val_accuracy: 0.9813\n",
      "Epoch 219/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0588 - accuracy: 0.9824 - val_loss: 0.0534 - val_accuracy: 0.9770\n",
      "Epoch 220/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0589 - accuracy: 0.9808 - val_loss: 0.0527 - val_accuracy: 0.9741\n",
      "Epoch 221/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0632 - accuracy: 0.9808 - val_loss: 0.0412 - val_accuracy: 0.9871\n",
      "Epoch 222/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0603 - accuracy: 0.9818 - val_loss: 0.0419 - val_accuracy: 0.9885\n",
      "Epoch 223/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0578 - accuracy: 0.9820 - val_loss: 0.0401 - val_accuracy: 0.9842\n",
      "Epoch 224/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0549 - accuracy: 0.9839 - val_loss: 0.0402 - val_accuracy: 0.9828\n",
      "Epoch 225/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0573 - accuracy: 0.9815 - val_loss: 0.0450 - val_accuracy: 0.9856\n",
      "Epoch 226/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0516 - accuracy: 0.9844 - val_loss: 0.0384 - val_accuracy: 0.9871\n",
      "Epoch 227/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0565 - accuracy: 0.9823 - val_loss: 0.0410 - val_accuracy: 0.9885\n",
      "Epoch 228/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0565 - accuracy: 0.9832 - val_loss: 0.0422 - val_accuracy: 0.9828\n",
      "Epoch 229/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0577 - accuracy: 0.9821 - val_loss: 0.0482 - val_accuracy: 0.9799\n",
      "Epoch 230/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0617 - accuracy: 0.9813 - val_loss: 0.0436 - val_accuracy: 0.9842\n",
      "Epoch 231/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0593 - accuracy: 0.9826 - val_loss: 0.0414 - val_accuracy: 0.9828\n",
      "Epoch 232/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0544 - accuracy: 0.9840 - val_loss: 0.0458 - val_accuracy: 0.9842\n",
      "Epoch 233/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0551 - accuracy: 0.9828 - val_loss: 0.0417 - val_accuracy: 0.9842\n",
      "Epoch 234/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0564 - accuracy: 0.9820 - val_loss: 0.0395 - val_accuracy: 0.9842\n",
      "Epoch 235/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0555 - accuracy: 0.9842 - val_loss: 0.0417 - val_accuracy: 0.9871\n",
      "Epoch 236/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0591 - accuracy: 0.9816 - val_loss: 0.0420 - val_accuracy: 0.9828\n",
      "Epoch 237/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0593 - accuracy: 0.9818 - val_loss: 0.0434 - val_accuracy: 0.9828\n",
      "Epoch 238/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0566 - accuracy: 0.9815 - val_loss: 0.0431 - val_accuracy: 0.9871\n",
      "Epoch 239/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0575 - accuracy: 0.9828 - val_loss: 0.0462 - val_accuracy: 0.9842\n",
      "Epoch 240/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0675 - accuracy: 0.9775 - val_loss: 0.0456 - val_accuracy: 0.9828\n",
      "Epoch 241/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0593 - accuracy: 0.9818 - val_loss: 0.0479 - val_accuracy: 0.9813\n",
      "Epoch 242/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0628 - accuracy: 0.9812 - val_loss: 0.0470 - val_accuracy: 0.9770\n",
      "Epoch 243/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0588 - accuracy: 0.9826 - val_loss: 0.0448 - val_accuracy: 0.9842\n",
      "Epoch 244/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0555 - accuracy: 0.9826 - val_loss: 0.0426 - val_accuracy: 0.9871\n",
      "Epoch 245/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0561 - accuracy: 0.9832 - val_loss: 0.0411 - val_accuracy: 0.9856\n",
      "Epoch 246/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0661 - accuracy: 0.9789 - val_loss: 0.0752 - val_accuracy: 0.9612\n",
      "Epoch 247/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0690 - accuracy: 0.9776 - val_loss: 0.0734 - val_accuracy: 0.9741\n",
      "Epoch 248/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0649 - accuracy: 0.9802 - val_loss: 0.0385 - val_accuracy: 0.9856\n",
      "Epoch 249/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0566 - accuracy: 0.9828 - val_loss: 0.0394 - val_accuracy: 0.9828\n",
      "Epoch 250/500\n",
      "7/7 [==============================] - 2s 238ms/step - loss: 0.0546 - accuracy: 0.9848 - val_loss: 0.0396 - val_accuracy: 0.9842\n",
      "Epoch 251/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0511 - accuracy: 0.9844 - val_loss: 0.0473 - val_accuracy: 0.9799\n",
      "Epoch 252/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0564 - accuracy: 0.9826 - val_loss: 0.0401 - val_accuracy: 0.9842\n",
      "Epoch 253/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0589 - accuracy: 0.9815 - val_loss: 0.0373 - val_accuracy: 0.9871\n",
      "Epoch 254/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0593 - accuracy: 0.9815 - val_loss: 0.0424 - val_accuracy: 0.9856\n",
      "Epoch 255/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0665 - accuracy: 0.9794 - val_loss: 0.0537 - val_accuracy: 0.9799\n",
      "Epoch 256/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0569 - accuracy: 0.9829 - val_loss: 0.0421 - val_accuracy: 0.9842\n",
      "Epoch 257/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0568 - accuracy: 0.9820 - val_loss: 0.0522 - val_accuracy: 0.9784\n",
      "Epoch 258/500\n",
      "7/7 [==============================] - 2s 242ms/step - loss: 0.0546 - accuracy: 0.9844 - val_loss: 0.0526 - val_accuracy: 0.9828\n",
      "Epoch 259/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0625 - accuracy: 0.9808 - val_loss: 0.0367 - val_accuracy: 0.9871\n",
      "Epoch 260/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0538 - accuracy: 0.9842 - val_loss: 0.0354 - val_accuracy: 0.9914\n",
      "Epoch 261/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0592 - accuracy: 0.9823 - val_loss: 0.0471 - val_accuracy: 0.9813\n",
      "Epoch 262/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0529 - accuracy: 0.9837 - val_loss: 0.0434 - val_accuracy: 0.9856\n",
      "Epoch 263/500\n",
      "7/7 [==============================] - 2s 238ms/step - loss: 0.0542 - accuracy: 0.9839 - val_loss: 0.0436 - val_accuracy: 0.9828\n",
      "Epoch 264/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0557 - accuracy: 0.9834 - val_loss: 0.0426 - val_accuracy: 0.9842\n",
      "Epoch 265/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0500 - accuracy: 0.9852 - val_loss: 0.0423 - val_accuracy: 0.9828\n",
      "Epoch 266/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0561 - accuracy: 0.9837 - val_loss: 0.0409 - val_accuracy: 0.9871\n",
      "Epoch 267/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0538 - accuracy: 0.9855 - val_loss: 0.0497 - val_accuracy: 0.9784\n",
      "Epoch 268/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0564 - accuracy: 0.9818 - val_loss: 0.0400 - val_accuracy: 0.9871\n",
      "Epoch 269/500\n",
      "7/7 [==============================] - 2s 238ms/step - loss: 0.0532 - accuracy: 0.9855 - val_loss: 0.0387 - val_accuracy: 0.9885\n",
      "Epoch 270/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0530 - accuracy: 0.9839 - val_loss: 0.0469 - val_accuracy: 0.9799\n",
      "Epoch 271/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0574 - accuracy: 0.9815 - val_loss: 0.0403 - val_accuracy: 0.9871\n",
      "Epoch 272/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0511 - accuracy: 0.9856 - val_loss: 0.0407 - val_accuracy: 0.9856\n",
      "Epoch 273/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0566 - accuracy: 0.9826 - val_loss: 0.0421 - val_accuracy: 0.9871\n",
      "Epoch 274/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0556 - accuracy: 0.9839 - val_loss: 0.0445 - val_accuracy: 0.9842\n",
      "Epoch 275/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0544 - accuracy: 0.9828 - val_loss: 0.0468 - val_accuracy: 0.9799\n",
      "Epoch 276/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0559 - accuracy: 0.9839 - val_loss: 0.0417 - val_accuracy: 0.9871\n",
      "Epoch 277/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0546 - accuracy: 0.9839 - val_loss: 0.0467 - val_accuracy: 0.9856\n",
      "Epoch 278/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0559 - accuracy: 0.9834 - val_loss: 0.0397 - val_accuracy: 0.9842\n",
      "Epoch 279/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0581 - accuracy: 0.9818 - val_loss: 0.0487 - val_accuracy: 0.9813\n",
      "Epoch 280/500\n",
      "7/7 [==============================] - 2s 245ms/step - loss: 0.0636 - accuracy: 0.9789 - val_loss: 0.0389 - val_accuracy: 0.9856\n",
      "Epoch 281/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0557 - accuracy: 0.9828 - val_loss: 0.0370 - val_accuracy: 0.9885\n",
      "Epoch 282/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0561 - accuracy: 0.9820 - val_loss: 0.0488 - val_accuracy: 0.9799\n",
      "Epoch 283/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0555 - accuracy: 0.9832 - val_loss: 0.0377 - val_accuracy: 0.9871\n",
      "Epoch 284/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0573 - accuracy: 0.9824 - val_loss: 0.0438 - val_accuracy: 0.9813\n",
      "Epoch 285/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0575 - accuracy: 0.9813 - val_loss: 0.0410 - val_accuracy: 0.9871\n",
      "Epoch 286/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0544 - accuracy: 0.9831 - val_loss: 0.0377 - val_accuracy: 0.9871\n",
      "Epoch 287/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0574 - accuracy: 0.9837 - val_loss: 0.0467 - val_accuracy: 0.9871\n",
      "Epoch 288/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0575 - accuracy: 0.9828 - val_loss: 0.0444 - val_accuracy: 0.9842\n",
      "Epoch 289/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0543 - accuracy: 0.9828 - val_loss: 0.0440 - val_accuracy: 0.9871\n",
      "Epoch 290/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0519 - accuracy: 0.9864 - val_loss: 0.0403 - val_accuracy: 0.9842\n",
      "Epoch 291/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0524 - accuracy: 0.9848 - val_loss: 0.0392 - val_accuracy: 0.9842\n",
      "Epoch 292/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0536 - accuracy: 0.9840 - val_loss: 0.0456 - val_accuracy: 0.9784\n",
      "Epoch 293/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0558 - accuracy: 0.9839 - val_loss: 0.0409 - val_accuracy: 0.9871\n",
      "Epoch 294/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0504 - accuracy: 0.9848 - val_loss: 0.0412 - val_accuracy: 0.9856\n",
      "Epoch 295/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0534 - accuracy: 0.9850 - val_loss: 0.0387 - val_accuracy: 0.9828\n",
      "Epoch 296/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0520 - accuracy: 0.9836 - val_loss: 0.0498 - val_accuracy: 0.9813\n",
      "Epoch 297/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0611 - accuracy: 0.9815 - val_loss: 0.0371 - val_accuracy: 0.9856\n",
      "Epoch 298/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0542 - accuracy: 0.9839 - val_loss: 0.0367 - val_accuracy: 0.9885\n",
      "Epoch 299/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0515 - accuracy: 0.9842 - val_loss: 0.0422 - val_accuracy: 0.9799\n",
      "Epoch 300/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0481 - accuracy: 0.9871 - val_loss: 0.0381 - val_accuracy: 0.9842\n",
      "Epoch 301/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0486 - accuracy: 0.9871 - val_loss: 0.0432 - val_accuracy: 0.9828\n",
      "Epoch 302/500\n",
      "7/7 [==============================] - 2s 233ms/step - loss: 0.0525 - accuracy: 0.9837 - val_loss: 0.0403 - val_accuracy: 0.9842\n",
      "Epoch 303/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0524 - accuracy: 0.9844 - val_loss: 0.0350 - val_accuracy: 0.9899\n",
      "Epoch 304/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0565 - accuracy: 0.9815 - val_loss: 0.0560 - val_accuracy: 0.9799\n",
      "Epoch 305/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0601 - accuracy: 0.9800 - val_loss: 0.0442 - val_accuracy: 0.9828\n",
      "Epoch 306/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0562 - accuracy: 0.9832 - val_loss: 0.0416 - val_accuracy: 0.9856\n",
      "Epoch 307/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0555 - accuracy: 0.9820 - val_loss: 0.0418 - val_accuracy: 0.9871\n",
      "Epoch 308/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0535 - accuracy: 0.9836 - val_loss: 0.0416 - val_accuracy: 0.9856\n",
      "Epoch 309/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0553 - accuracy: 0.9840 - val_loss: 0.0403 - val_accuracy: 0.9813\n",
      "Epoch 310/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0517 - accuracy: 0.9842 - val_loss: 0.0375 - val_accuracy: 0.9885\n",
      "Epoch 311/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0628 - accuracy: 0.9797 - val_loss: 0.0464 - val_accuracy: 0.9828\n",
      "Epoch 312/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0576 - accuracy: 0.9823 - val_loss: 0.0384 - val_accuracy: 0.9885\n",
      "Epoch 313/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0545 - accuracy: 0.9821 - val_loss: 0.0427 - val_accuracy: 0.9799\n",
      "Epoch 314/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0511 - accuracy: 0.9842 - val_loss: 0.0440 - val_accuracy: 0.9856\n",
      "Epoch 315/500\n",
      "7/7 [==============================] - 2s 233ms/step - loss: 0.0553 - accuracy: 0.9826 - val_loss: 0.0410 - val_accuracy: 0.9813\n",
      "Epoch 316/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0536 - accuracy: 0.9842 - val_loss: 0.0412 - val_accuracy: 0.9813\n",
      "Epoch 317/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0518 - accuracy: 0.9839 - val_loss: 0.0392 - val_accuracy: 0.9842\n",
      "Epoch 318/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0470 - accuracy: 0.9866 - val_loss: 0.0386 - val_accuracy: 0.9871\n",
      "Epoch 319/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0538 - accuracy: 0.9828 - val_loss: 0.0411 - val_accuracy: 0.9828\n",
      "Epoch 320/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0550 - accuracy: 0.9840 - val_loss: 0.0529 - val_accuracy: 0.9828\n",
      "Epoch 321/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0597 - accuracy: 0.9813 - val_loss: 0.0474 - val_accuracy: 0.9828\n",
      "Epoch 322/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0507 - accuracy: 0.9853 - val_loss: 0.0436 - val_accuracy: 0.9871\n",
      "Epoch 323/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0603 - accuracy: 0.9815 - val_loss: 0.0739 - val_accuracy: 0.9698\n",
      "Epoch 324/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0630 - accuracy: 0.9800 - val_loss: 0.0450 - val_accuracy: 0.9799\n",
      "Epoch 325/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0589 - accuracy: 0.9812 - val_loss: 0.0403 - val_accuracy: 0.9813\n",
      "Epoch 326/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0569 - accuracy: 0.9812 - val_loss: 0.0490 - val_accuracy: 0.9813\n",
      "Epoch 327/500\n",
      "7/7 [==============================] - 2s 233ms/step - loss: 0.0556 - accuracy: 0.9826 - val_loss: 0.0375 - val_accuracy: 0.9828\n",
      "Epoch 328/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0475 - accuracy: 0.9855 - val_loss: 0.0374 - val_accuracy: 0.9899\n",
      "Epoch 329/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0503 - accuracy: 0.9855 - val_loss: 0.0381 - val_accuracy: 0.9885\n",
      "Epoch 330/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0511 - accuracy: 0.9837 - val_loss: 0.0493 - val_accuracy: 0.9828\n",
      "Epoch 331/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0574 - accuracy: 0.9824 - val_loss: 0.0451 - val_accuracy: 0.9799\n",
      "Epoch 332/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0555 - accuracy: 0.9810 - val_loss: 0.0425 - val_accuracy: 0.9784\n",
      "Epoch 333/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0539 - accuracy: 0.9837 - val_loss: 0.0354 - val_accuracy: 0.9856\n",
      "Epoch 334/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0565 - accuracy: 0.9824 - val_loss: 0.0334 - val_accuracy: 0.9871\n",
      "Epoch 335/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0569 - accuracy: 0.9831 - val_loss: 0.0414 - val_accuracy: 0.9813\n",
      "Epoch 336/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0548 - accuracy: 0.9832 - val_loss: 0.0428 - val_accuracy: 0.9813\n",
      "Epoch 337/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0500 - accuracy: 0.9845 - val_loss: 0.0425 - val_accuracy: 0.9856\n",
      "Epoch 338/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0493 - accuracy: 0.9845 - val_loss: 0.0394 - val_accuracy: 0.9885\n",
      "Epoch 339/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0523 - accuracy: 0.9836 - val_loss: 0.0384 - val_accuracy: 0.9856\n",
      "Epoch 340/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0534 - accuracy: 0.9837 - val_loss: 0.0391 - val_accuracy: 0.9828\n",
      "Epoch 341/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0523 - accuracy: 0.9837 - val_loss: 0.0369 - val_accuracy: 0.9885\n",
      "Epoch 342/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0526 - accuracy: 0.9850 - val_loss: 0.0403 - val_accuracy: 0.9828\n",
      "Epoch 343/500\n",
      "7/7 [==============================] - 2s 305ms/step - loss: 0.0528 - accuracy: 0.9845 - val_loss: 0.0352 - val_accuracy: 0.9871\n",
      "Epoch 344/500\n",
      "7/7 [==============================] - 2s 258ms/step - loss: 0.0528 - accuracy: 0.9847 - val_loss: 0.0430 - val_accuracy: 0.9856\n",
      "Epoch 345/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0534 - accuracy: 0.9836 - val_loss: 0.0406 - val_accuracy: 0.9871\n",
      "Epoch 346/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0453 - accuracy: 0.9871 - val_loss: 0.0437 - val_accuracy: 0.9828\n",
      "Epoch 347/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0539 - accuracy: 0.9832 - val_loss: 0.0339 - val_accuracy: 0.9871\n",
      "Epoch 348/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0513 - accuracy: 0.9831 - val_loss: 0.0340 - val_accuracy: 0.9871\n",
      "Epoch 349/500\n",
      "7/7 [==============================] - 2s 259ms/step - loss: 0.0492 - accuracy: 0.9848 - val_loss: 0.0333 - val_accuracy: 0.9842\n",
      "Epoch 350/500\n",
      "7/7 [==============================] - 2s 247ms/step - loss: 0.0482 - accuracy: 0.9859 - val_loss: 0.0401 - val_accuracy: 0.9856\n",
      "Epoch 351/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0508 - accuracy: 0.9859 - val_loss: 0.0378 - val_accuracy: 0.9856\n",
      "Epoch 352/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0531 - accuracy: 0.9828 - val_loss: 0.0364 - val_accuracy: 0.9856\n",
      "Epoch 353/500\n",
      "7/7 [==============================] - 2s 233ms/step - loss: 0.0553 - accuracy: 0.9824 - val_loss: 0.0409 - val_accuracy: 0.9828\n",
      "Epoch 354/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0609 - accuracy: 0.9824 - val_loss: 0.0791 - val_accuracy: 0.9641\n",
      "Epoch 355/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0661 - accuracy: 0.9786 - val_loss: 0.0424 - val_accuracy: 0.9799\n",
      "Epoch 356/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0556 - accuracy: 0.9828 - val_loss: 0.0368 - val_accuracy: 0.9871\n",
      "Epoch 357/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0539 - accuracy: 0.9842 - val_loss: 0.0395 - val_accuracy: 0.9828\n",
      "Epoch 358/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0508 - accuracy: 0.9848 - val_loss: 0.0363 - val_accuracy: 0.9871\n",
      "Epoch 359/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0498 - accuracy: 0.9847 - val_loss: 0.0360 - val_accuracy: 0.9856\n",
      "Epoch 360/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0503 - accuracy: 0.9850 - val_loss: 0.0332 - val_accuracy: 0.9871\n",
      "Epoch 361/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0496 - accuracy: 0.9864 - val_loss: 0.0348 - val_accuracy: 0.9856\n",
      "Epoch 362/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0490 - accuracy: 0.9856 - val_loss: 0.0368 - val_accuracy: 0.9871\n",
      "Epoch 363/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0480 - accuracy: 0.9859 - val_loss: 0.0369 - val_accuracy: 0.9842\n",
      "Epoch 364/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0457 - accuracy: 0.9869 - val_loss: 0.0390 - val_accuracy: 0.9856\n",
      "Epoch 365/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0495 - accuracy: 0.9852 - val_loss: 0.0380 - val_accuracy: 0.9856\n",
      "Epoch 366/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0511 - accuracy: 0.9847 - val_loss: 0.0419 - val_accuracy: 0.9813\n",
      "Epoch 367/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0512 - accuracy: 0.9837 - val_loss: 0.0416 - val_accuracy: 0.9871\n",
      "Epoch 368/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0503 - accuracy: 0.9844 - val_loss: 0.0446 - val_accuracy: 0.9813\n",
      "Epoch 369/500\n",
      "7/7 [==============================] - 2s 258ms/step - loss: 0.0521 - accuracy: 0.9840 - val_loss: 0.0412 - val_accuracy: 0.9856\n",
      "Epoch 370/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0561 - accuracy: 0.9818 - val_loss: 0.0474 - val_accuracy: 0.9828\n",
      "Epoch 371/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0597 - accuracy: 0.9812 - val_loss: 0.0506 - val_accuracy: 0.9828\n",
      "Epoch 372/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0523 - accuracy: 0.9834 - val_loss: 0.0549 - val_accuracy: 0.9784\n",
      "Epoch 373/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0535 - accuracy: 0.9848 - val_loss: 0.0482 - val_accuracy: 0.9828\n",
      "Epoch 374/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0516 - accuracy: 0.9834 - val_loss: 0.0440 - val_accuracy: 0.9813\n",
      "Epoch 375/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0522 - accuracy: 0.9844 - val_loss: 0.0364 - val_accuracy: 0.9871\n",
      "Epoch 376/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0505 - accuracy: 0.9839 - val_loss: 0.0427 - val_accuracy: 0.9799\n",
      "Epoch 377/500\n",
      "7/7 [==============================] - 2s 239ms/step - loss: 0.0515 - accuracy: 0.9832 - val_loss: 0.0389 - val_accuracy: 0.9842\n",
      "Epoch 378/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0495 - accuracy: 0.9845 - val_loss: 0.0372 - val_accuracy: 0.9856\n",
      "Epoch 379/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0505 - accuracy: 0.9836 - val_loss: 0.0455 - val_accuracy: 0.9842\n",
      "Epoch 380/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0512 - accuracy: 0.9840 - val_loss: 0.0368 - val_accuracy: 0.9871\n",
      "Epoch 381/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0487 - accuracy: 0.9864 - val_loss: 0.0335 - val_accuracy: 0.9856\n",
      "Epoch 382/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0488 - accuracy: 0.9828 - val_loss: 0.0537 - val_accuracy: 0.9784\n",
      "Epoch 383/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0506 - accuracy: 0.9832 - val_loss: 0.0373 - val_accuracy: 0.9842\n",
      "Epoch 384/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0528 - accuracy: 0.9839 - val_loss: 0.0414 - val_accuracy: 0.9842\n",
      "Epoch 385/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0523 - accuracy: 0.9842 - val_loss: 0.0376 - val_accuracy: 0.9871\n",
      "Epoch 386/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0473 - accuracy: 0.9859 - val_loss: 0.0356 - val_accuracy: 0.9899\n",
      "Epoch 387/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0488 - accuracy: 0.9837 - val_loss: 0.0410 - val_accuracy: 0.9828\n",
      "Epoch 388/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0514 - accuracy: 0.9837 - val_loss: 0.0371 - val_accuracy: 0.9842\n",
      "Epoch 389/500\n",
      "7/7 [==============================] - 2s 233ms/step - loss: 0.0492 - accuracy: 0.9858 - val_loss: 0.0395 - val_accuracy: 0.9842\n",
      "Epoch 390/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0481 - accuracy: 0.9850 - val_loss: 0.0408 - val_accuracy: 0.9842\n",
      "Epoch 391/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0488 - accuracy: 0.9853 - val_loss: 0.0361 - val_accuracy: 0.9842\n",
      "Epoch 392/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0466 - accuracy: 0.9863 - val_loss: 0.0411 - val_accuracy: 0.9828\n",
      "Epoch 393/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0506 - accuracy: 0.9840 - val_loss: 0.0325 - val_accuracy: 0.9842\n",
      "Epoch 394/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0494 - accuracy: 0.9848 - val_loss: 0.0453 - val_accuracy: 0.9856\n",
      "Epoch 395/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0530 - accuracy: 0.9832 - val_loss: 0.0345 - val_accuracy: 0.9885\n",
      "Epoch 396/500\n",
      "7/7 [==============================] - 2s 248ms/step - loss: 0.0458 - accuracy: 0.9858 - val_loss: 0.0380 - val_accuracy: 0.9871\n",
      "Epoch 397/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0513 - accuracy: 0.9844 - val_loss: 0.0383 - val_accuracy: 0.9828\n",
      "Epoch 398/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0512 - accuracy: 0.9840 - val_loss: 0.0368 - val_accuracy: 0.9856\n",
      "Epoch 399/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0457 - accuracy: 0.9855 - val_loss: 0.0352 - val_accuracy: 0.9871\n",
      "Epoch 400/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0475 - accuracy: 0.9856 - val_loss: 0.0345 - val_accuracy: 0.9885\n",
      "Epoch 401/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0442 - accuracy: 0.9863 - val_loss: 0.0334 - val_accuracy: 0.9899\n",
      "Epoch 402/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0459 - accuracy: 0.9858 - val_loss: 0.0367 - val_accuracy: 0.9828\n",
      "Epoch 403/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0459 - accuracy: 0.9872 - val_loss: 0.0372 - val_accuracy: 0.9842\n",
      "Epoch 404/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0456 - accuracy: 0.9875 - val_loss: 0.0399 - val_accuracy: 0.9842\n",
      "Epoch 405/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0504 - accuracy: 0.9853 - val_loss: 0.0330 - val_accuracy: 0.9871\n",
      "Epoch 406/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0483 - accuracy: 0.9856 - val_loss: 0.0340 - val_accuracy: 0.9885\n",
      "Epoch 407/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0497 - accuracy: 0.9840 - val_loss: 0.0464 - val_accuracy: 0.9842\n",
      "Epoch 408/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0519 - accuracy: 0.9845 - val_loss: 0.0297 - val_accuracy: 0.9885\n",
      "Epoch 409/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0470 - accuracy: 0.9855 - val_loss: 0.0407 - val_accuracy: 0.9871\n",
      "Epoch 410/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0514 - accuracy: 0.9832 - val_loss: 0.0346 - val_accuracy: 0.9856\n",
      "Epoch 411/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0452 - accuracy: 0.9871 - val_loss: 0.0323 - val_accuracy: 0.9885\n",
      "Epoch 412/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0490 - accuracy: 0.9848 - val_loss: 0.0391 - val_accuracy: 0.9828\n",
      "Epoch 413/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0500 - accuracy: 0.9847 - val_loss: 0.0354 - val_accuracy: 0.9871\n",
      "Epoch 414/500\n",
      "7/7 [==============================] - 2s 233ms/step - loss: 0.0524 - accuracy: 0.9831 - val_loss: 0.0381 - val_accuracy: 0.9856\n",
      "Epoch 415/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0484 - accuracy: 0.9852 - val_loss: 0.0378 - val_accuracy: 0.9856\n",
      "Epoch 416/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0457 - accuracy: 0.9866 - val_loss: 0.0351 - val_accuracy: 0.9856\n",
      "Epoch 417/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0456 - accuracy: 0.9861 - val_loss: 0.0311 - val_accuracy: 0.9885\n",
      "Epoch 418/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0463 - accuracy: 0.9853 - val_loss: 0.0348 - val_accuracy: 0.9871\n",
      "Epoch 419/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0517 - accuracy: 0.9844 - val_loss: 0.0491 - val_accuracy: 0.9828\n",
      "Epoch 420/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0499 - accuracy: 0.9837 - val_loss: 0.0398 - val_accuracy: 0.9842\n",
      "Epoch 421/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0523 - accuracy: 0.9844 - val_loss: 0.0404 - val_accuracy: 0.9828\n",
      "Epoch 422/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0465 - accuracy: 0.9853 - val_loss: 0.0359 - val_accuracy: 0.9856\n",
      "Epoch 423/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0469 - accuracy: 0.9850 - val_loss: 0.0353 - val_accuracy: 0.9885\n",
      "Epoch 424/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0500 - accuracy: 0.9844 - val_loss: 0.0333 - val_accuracy: 0.9871\n",
      "Epoch 425/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0493 - accuracy: 0.9852 - val_loss: 0.0340 - val_accuracy: 0.9856\n",
      "Epoch 426/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0463 - accuracy: 0.9871 - val_loss: 0.0343 - val_accuracy: 0.9856\n",
      "Epoch 427/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0435 - accuracy: 0.9864 - val_loss: 0.0307 - val_accuracy: 0.9885\n",
      "Epoch 428/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0438 - accuracy: 0.9869 - val_loss: 0.0303 - val_accuracy: 0.9885\n",
      "Epoch 429/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0539 - accuracy: 0.9844 - val_loss: 0.0379 - val_accuracy: 0.9842\n",
      "Epoch 430/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0523 - accuracy: 0.9832 - val_loss: 0.0318 - val_accuracy: 0.9871\n",
      "Epoch 431/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0491 - accuracy: 0.9832 - val_loss: 0.0321 - val_accuracy: 0.9871\n",
      "Epoch 432/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0487 - accuracy: 0.9856 - val_loss: 0.0424 - val_accuracy: 0.9842\n",
      "Epoch 433/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0482 - accuracy: 0.9859 - val_loss: 0.0299 - val_accuracy: 0.9899\n",
      "Epoch 434/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0544 - accuracy: 0.9826 - val_loss: 0.0358 - val_accuracy: 0.9842\n",
      "Epoch 435/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0583 - accuracy: 0.9808 - val_loss: 0.0449 - val_accuracy: 0.9856\n",
      "Epoch 436/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0566 - accuracy: 0.9815 - val_loss: 0.0660 - val_accuracy: 0.9741\n",
      "Epoch 437/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0586 - accuracy: 0.9818 - val_loss: 0.0444 - val_accuracy: 0.9871\n",
      "Epoch 438/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0466 - accuracy: 0.9861 - val_loss: 0.0422 - val_accuracy: 0.9842\n",
      "Epoch 439/500\n",
      "7/7 [==============================] - 2s 260ms/step - loss: 0.0551 - accuracy: 0.9829 - val_loss: 0.0341 - val_accuracy: 0.9899\n",
      "Epoch 440/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0468 - accuracy: 0.9861 - val_loss: 0.0335 - val_accuracy: 0.9871\n",
      "Epoch 441/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0472 - accuracy: 0.9847 - val_loss: 0.0527 - val_accuracy: 0.9784\n",
      "Epoch 442/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0568 - accuracy: 0.9816 - val_loss: 0.0305 - val_accuracy: 0.9871\n",
      "Epoch 443/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0504 - accuracy: 0.9845 - val_loss: 0.0304 - val_accuracy: 0.9899\n",
      "Epoch 444/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0481 - accuracy: 0.9853 - val_loss: 0.0337 - val_accuracy: 0.9856\n",
      "Epoch 445/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0438 - accuracy: 0.9864 - val_loss: 0.0321 - val_accuracy: 0.9871\n",
      "Epoch 446/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0454 - accuracy: 0.9861 - val_loss: 0.0289 - val_accuracy: 0.9885\n",
      "Epoch 447/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0458 - accuracy: 0.9861 - val_loss: 0.0353 - val_accuracy: 0.9842\n",
      "Epoch 448/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0483 - accuracy: 0.9850 - val_loss: 0.0416 - val_accuracy: 0.9828\n",
      "Epoch 449/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0443 - accuracy: 0.9859 - val_loss: 0.0332 - val_accuracy: 0.9856\n",
      "Epoch 450/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0474 - accuracy: 0.9848 - val_loss: 0.0412 - val_accuracy: 0.9828\n",
      "Epoch 451/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0474 - accuracy: 0.9834 - val_loss: 0.0302 - val_accuracy: 0.9871\n",
      "Epoch 452/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0433 - accuracy: 0.9859 - val_loss: 0.0288 - val_accuracy: 0.9914\n",
      "Epoch 453/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0465 - accuracy: 0.9852 - val_loss: 0.0334 - val_accuracy: 0.9871\n",
      "Epoch 454/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0460 - accuracy: 0.9871 - val_loss: 0.0311 - val_accuracy: 0.9899\n",
      "Epoch 455/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0440 - accuracy: 0.9863 - val_loss: 0.0309 - val_accuracy: 0.9899\n",
      "Epoch 456/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0433 - accuracy: 0.9887 - val_loss: 0.0400 - val_accuracy: 0.9842\n",
      "Epoch 457/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0466 - accuracy: 0.9840 - val_loss: 0.0429 - val_accuracy: 0.9856\n",
      "Epoch 458/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0557 - accuracy: 0.9828 - val_loss: 0.0620 - val_accuracy: 0.9713\n",
      "Epoch 459/500\n",
      "7/7 [==============================] - 2s 237ms/step - loss: 0.0502 - accuracy: 0.9831 - val_loss: 0.0518 - val_accuracy: 0.9842\n",
      "Epoch 460/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0536 - accuracy: 0.9831 - val_loss: 0.0328 - val_accuracy: 0.9871\n",
      "Epoch 461/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0455 - accuracy: 0.9858 - val_loss: 0.0550 - val_accuracy: 0.9799\n",
      "Epoch 462/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0496 - accuracy: 0.9832 - val_loss: 0.0395 - val_accuracy: 0.9828\n",
      "Epoch 463/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0463 - accuracy: 0.9859 - val_loss: 0.0373 - val_accuracy: 0.9871\n",
      "Epoch 464/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0470 - accuracy: 0.9853 - val_loss: 0.0306 - val_accuracy: 0.9871\n",
      "Epoch 465/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0462 - accuracy: 0.9850 - val_loss: 0.0407 - val_accuracy: 0.9828\n",
      "Epoch 466/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0454 - accuracy: 0.9861 - val_loss: 0.0307 - val_accuracy: 0.9871\n",
      "Epoch 467/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0432 - accuracy: 0.9871 - val_loss: 0.0322 - val_accuracy: 0.9856\n",
      "Epoch 468/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0451 - accuracy: 0.9866 - val_loss: 0.0411 - val_accuracy: 0.9784\n",
      "Epoch 469/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0485 - accuracy: 0.9839 - val_loss: 0.0314 - val_accuracy: 0.9871\n",
      "Epoch 470/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0467 - accuracy: 0.9853 - val_loss: 0.0375 - val_accuracy: 0.9842\n",
      "Epoch 471/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0428 - accuracy: 0.9869 - val_loss: 0.0344 - val_accuracy: 0.9871\n",
      "Epoch 472/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0432 - accuracy: 0.9861 - val_loss: 0.0328 - val_accuracy: 0.9885\n",
      "Epoch 473/500\n",
      "7/7 [==============================] - 2s 240ms/step - loss: 0.0401 - accuracy: 0.9879 - val_loss: 0.0381 - val_accuracy: 0.9871\n",
      "Epoch 474/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0518 - accuracy: 0.9840 - val_loss: 0.0473 - val_accuracy: 0.9828\n",
      "Epoch 475/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0443 - accuracy: 0.9856 - val_loss: 0.0446 - val_accuracy: 0.9813\n",
      "Epoch 476/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0461 - accuracy: 0.9858 - val_loss: 0.0318 - val_accuracy: 0.9842\n",
      "Epoch 477/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0450 - accuracy: 0.9847 - val_loss: 0.0389 - val_accuracy: 0.9828\n",
      "Epoch 478/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0456 - accuracy: 0.9847 - val_loss: 0.0300 - val_accuracy: 0.9885\n",
      "Epoch 479/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0375 - accuracy: 0.9880 - val_loss: 0.0365 - val_accuracy: 0.9871\n",
      "Epoch 480/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0420 - accuracy: 0.9864 - val_loss: 0.0338 - val_accuracy: 0.9871\n",
      "Epoch 481/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0405 - accuracy: 0.9867 - val_loss: 0.0373 - val_accuracy: 0.9871\n",
      "Epoch 482/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0480 - accuracy: 0.9848 - val_loss: 0.0371 - val_accuracy: 0.9828\n",
      "Epoch 483/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0487 - accuracy: 0.9850 - val_loss: 0.0359 - val_accuracy: 0.9885\n",
      "Epoch 484/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0543 - accuracy: 0.9829 - val_loss: 0.0357 - val_accuracy: 0.9899\n",
      "Epoch 485/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0456 - accuracy: 0.9853 - val_loss: 0.0354 - val_accuracy: 0.9871\n",
      "Epoch 486/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0443 - accuracy: 0.9867 - val_loss: 0.0525 - val_accuracy: 0.9784\n",
      "Epoch 487/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0430 - accuracy: 0.9864 - val_loss: 0.0340 - val_accuracy: 0.9885\n",
      "Epoch 488/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0439 - accuracy: 0.9867 - val_loss: 0.0333 - val_accuracy: 0.9899\n",
      "Epoch 489/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0459 - accuracy: 0.9859 - val_loss: 0.0419 - val_accuracy: 0.9871\n",
      "Epoch 490/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0502 - accuracy: 0.9840 - val_loss: 0.0354 - val_accuracy: 0.9871\n",
      "Epoch 491/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0443 - accuracy: 0.9874 - val_loss: 0.0349 - val_accuracy: 0.9856\n",
      "Epoch 492/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0442 - accuracy: 0.9866 - val_loss: 0.0331 - val_accuracy: 0.9856\n",
      "Epoch 493/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0405 - accuracy: 0.9890 - val_loss: 0.0292 - val_accuracy: 0.9914\n",
      "Epoch 494/500\n",
      "7/7 [==============================] - 2s 236ms/step - loss: 0.0414 - accuracy: 0.9877 - val_loss: 0.0351 - val_accuracy: 0.9842\n",
      "Epoch 495/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0409 - accuracy: 0.9880 - val_loss: 0.0329 - val_accuracy: 0.9871\n",
      "Epoch 496/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0423 - accuracy: 0.9874 - val_loss: 0.0320 - val_accuracy: 0.9856\n",
      "Epoch 497/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0423 - accuracy: 0.9872 - val_loss: 0.0431 - val_accuracy: 0.9828\n",
      "Epoch 498/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0408 - accuracy: 0.9871 - val_loss: 0.0323 - val_accuracy: 0.9856\n",
      "Epoch 499/500\n",
      "7/7 [==============================] - 2s 234ms/step - loss: 0.0437 - accuracy: 0.9855 - val_loss: 0.0393 - val_accuracy: 0.9842\n",
      "Epoch 500/500\n",
      "7/7 [==============================] - 2s 235ms/step - loss: 0.0400 - accuracy: 0.9875 - val_loss: 0.0353 - val_accuracy: 0.9856\n"
     ]
    }
   ],
   "source": [
    "X_mask = np.ones((len(X_train),14))\n",
    "history = model.fit([X_train, X_mask], y_train_onehot, batch_size=1024, epochs=500, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training - Loss Function')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.subplot()\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Training - Loss Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (train with  whole training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "7/7 [==============================] - 12s 254ms/step - loss: 1.6239 - accuracy: 0.2470\n",
      "Epoch 2/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 1.5023 - accuracy: 0.3878\n",
      "Epoch 3/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 1.3894 - accuracy: 0.4595\n",
      "Epoch 4/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 1.1518 - accuracy: 0.6078\n",
      "Epoch 5/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.9672 - accuracy: 0.6412\n",
      "Epoch 6/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.8756 - accuracy: 0.6704\n",
      "Epoch 7/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.8074 - accuracy: 0.6834\n",
      "Epoch 8/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.7455 - accuracy: 0.7071\n",
      "Epoch 9/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.6777 - accuracy: 0.7392\n",
      "Epoch 10/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.6236 - accuracy: 0.7520\n",
      "Epoch 11/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.5803 - accuracy: 0.7526\n",
      "Epoch 12/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.5510 - accuracy: 0.7439\n",
      "Epoch 13/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.5374 - accuracy: 0.7441\n",
      "Epoch 14/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.5152 - accuracy: 0.7702\n",
      "Epoch 15/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.5073 - accuracy: 0.7663\n",
      "Epoch 16/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.4949 - accuracy: 0.7724\n",
      "Epoch 17/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.4889 - accuracy: 0.7711\n",
      "Epoch 18/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.4835 - accuracy: 0.7697\n",
      "Epoch 19/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.4709 - accuracy: 0.7790\n",
      "Epoch 20/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.4662 - accuracy: 0.7804\n",
      "Epoch 21/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.4542 - accuracy: 0.7888\n",
      "Epoch 22/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.4385 - accuracy: 0.8092\n",
      "Epoch 23/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.4080 - accuracy: 0.8404\n",
      "Epoch 24/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.3590 - accuracy: 0.8698\n",
      "Epoch 25/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.3184 - accuracy: 0.8820\n",
      "Epoch 26/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.2827 - accuracy: 0.9017\n",
      "Epoch 27/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.2597 - accuracy: 0.9093\n",
      "Epoch 28/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.2310 - accuracy: 0.9207\n",
      "Epoch 29/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.2091 - accuracy: 0.9299\n",
      "Epoch 30/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.1972 - accuracy: 0.9327\n",
      "Epoch 31/500\n",
      "7/7 [==============================] - 2s 255ms/step - loss: 0.1742 - accuracy: 0.9447\n",
      "Epoch 32/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.1523 - accuracy: 0.9513\n",
      "Epoch 33/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.1396 - accuracy: 0.9575\n",
      "Epoch 34/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.1356 - accuracy: 0.9583\n",
      "Epoch 35/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.1275 - accuracy: 0.9601\n",
      "Epoch 36/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.1222 - accuracy: 0.9634\n",
      "Epoch 37/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.1142 - accuracy: 0.9652\n",
      "Epoch 38/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.1170 - accuracy: 0.9626\n",
      "Epoch 39/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.1128 - accuracy: 0.9631\n",
      "Epoch 40/500\n",
      "7/7 [==============================] - 2s 262ms/step - loss: 0.1103 - accuracy: 0.9644\n",
      "Epoch 41/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.1062 - accuracy: 0.9658\n",
      "Epoch 42/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.1041 - accuracy: 0.9657\n",
      "Epoch 43/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.1028 - accuracy: 0.9698\n",
      "Epoch 44/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.1029 - accuracy: 0.9664\n",
      "Epoch 45/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0982 - accuracy: 0.9677\n",
      "Epoch 46/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.1002 - accuracy: 0.9694\n",
      "Epoch 47/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0964 - accuracy: 0.9684\n",
      "Epoch 48/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0944 - accuracy: 0.9687\n",
      "Epoch 49/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0939 - accuracy: 0.9684\n",
      "Epoch 50/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0913 - accuracy: 0.9708\n",
      "Epoch 51/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0911 - accuracy: 0.9733\n",
      "Epoch 52/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0859 - accuracy: 0.9728\n",
      "Epoch 53/500\n",
      "7/7 [==============================] - 2s 250ms/step - loss: 0.0869 - accuracy: 0.9715\n",
      "Epoch 54/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0883 - accuracy: 0.9720\n",
      "Epoch 55/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0887 - accuracy: 0.9733\n",
      "Epoch 56/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0812 - accuracy: 0.9744\n",
      "Epoch 57/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0868 - accuracy: 0.9713\n",
      "Epoch 58/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0912 - accuracy: 0.9701\n",
      "Epoch 59/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0815 - accuracy: 0.9749\n",
      "Epoch 60/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0844 - accuracy: 0.9730\n",
      "Epoch 61/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0840 - accuracy: 0.9720\n",
      "Epoch 62/500\n",
      "7/7 [==============================] - 2s 256ms/step - loss: 0.0831 - accuracy: 0.9718\n",
      "Epoch 63/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0841 - accuracy: 0.9736\n",
      "Epoch 64/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0832 - accuracy: 0.9721\n",
      "Epoch 65/500\n",
      "7/7 [==============================] - 2s 265ms/step - loss: 0.0830 - accuracy: 0.9733\n",
      "Epoch 66/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0775 - accuracy: 0.9763\n",
      "Epoch 67/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0776 - accuracy: 0.9759\n",
      "Epoch 68/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0833 - accuracy: 0.9731\n",
      "Epoch 69/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0776 - accuracy: 0.9746\n",
      "Epoch 70/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0756 - accuracy: 0.9767\n",
      "Epoch 71/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0744 - accuracy: 0.9772\n",
      "Epoch 72/500\n",
      "7/7 [==============================] - 2s 255ms/step - loss: 0.0792 - accuracy: 0.9746\n",
      "Epoch 73/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0815 - accuracy: 0.9730\n",
      "Epoch 74/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0766 - accuracy: 0.9763\n",
      "Epoch 75/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0786 - accuracy: 0.9743\n",
      "Epoch 76/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0774 - accuracy: 0.9749\n",
      "Epoch 77/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0760 - accuracy: 0.9764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0788 - accuracy: 0.9749\n",
      "Epoch 79/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0773 - accuracy: 0.9770\n",
      "Epoch 80/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0776 - accuracy: 0.9746\n",
      "Epoch 81/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0772 - accuracy: 0.9737\n",
      "Epoch 82/500\n",
      "7/7 [==============================] - 2s 250ms/step - loss: 0.0715 - accuracy: 0.9782\n",
      "Epoch 83/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0714 - accuracy: 0.9767\n",
      "Epoch 84/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0713 - accuracy: 0.9786\n",
      "Epoch 85/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0722 - accuracy: 0.9770\n",
      "Epoch 86/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0704 - accuracy: 0.9787\n",
      "Epoch 87/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0718 - accuracy: 0.9766\n",
      "Epoch 88/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0741 - accuracy: 0.9754\n",
      "Epoch 89/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0658 - accuracy: 0.9787\n",
      "Epoch 90/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0658 - accuracy: 0.9793\n",
      "Epoch 91/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0685 - accuracy: 0.9787\n",
      "Epoch 92/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0690 - accuracy: 0.9784\n",
      "Epoch 93/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0710 - accuracy: 0.9769\n",
      "Epoch 94/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0718 - accuracy: 0.9766\n",
      "Epoch 95/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0740 - accuracy: 0.9767\n",
      "Epoch 96/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0839 - accuracy: 0.9711\n",
      "Epoch 97/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0878 - accuracy: 0.9707\n",
      "Epoch 98/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0771 - accuracy: 0.9734\n",
      "Epoch 99/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0707 - accuracy: 0.9773\n",
      "Epoch 100/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0695 - accuracy: 0.9777\n",
      "Epoch 101/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0732 - accuracy: 0.9760\n",
      "Epoch 102/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0704 - accuracy: 0.9773\n",
      "Epoch 103/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0691 - accuracy: 0.9780\n",
      "Epoch 104/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0671 - accuracy: 0.9779\n",
      "Epoch 105/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0710 - accuracy: 0.9767\n",
      "Epoch 106/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0702 - accuracy: 0.9761\n",
      "Epoch 107/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0726 - accuracy: 0.9782\n",
      "Epoch 108/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0704 - accuracy: 0.9761\n",
      "Epoch 109/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0695 - accuracy: 0.9773\n",
      "Epoch 110/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0667 - accuracy: 0.9796\n",
      "Epoch 111/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0650 - accuracy: 0.9796\n",
      "Epoch 112/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0624 - accuracy: 0.9802\n",
      "Epoch 113/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0658 - accuracy: 0.9780\n",
      "Epoch 114/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0686 - accuracy: 0.9784\n",
      "Epoch 115/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0691 - accuracy: 0.9780\n",
      "Epoch 116/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0723 - accuracy: 0.9767\n",
      "Epoch 117/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0634 - accuracy: 0.9795\n",
      "Epoch 118/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0652 - accuracy: 0.9786\n",
      "Epoch 119/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0607 - accuracy: 0.9796\n",
      "Epoch 120/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0664 - accuracy: 0.9796\n",
      "Epoch 121/500\n",
      "7/7 [==============================] - 2s 255ms/step - loss: 0.0632 - accuracy: 0.9812\n",
      "Epoch 122/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0634 - accuracy: 0.9786\n",
      "Epoch 123/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0666 - accuracy: 0.9787\n",
      "Epoch 124/500\n",
      "7/7 [==============================] - 2s 255ms/step - loss: 0.0635 - accuracy: 0.9787\n",
      "Epoch 125/500\n",
      "7/7 [==============================] - 2s 281ms/step - loss: 0.0603 - accuracy: 0.9816\n",
      "Epoch 126/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0608 - accuracy: 0.9810\n",
      "Epoch 127/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0706 - accuracy: 0.9779\n",
      "Epoch 128/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0644 - accuracy: 0.9800\n",
      "Epoch 129/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0623 - accuracy: 0.9809\n",
      "Epoch 130/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0651 - accuracy: 0.9786\n",
      "Epoch 131/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0602 - accuracy: 0.9802\n",
      "Epoch 132/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0587 - accuracy: 0.9815\n",
      "Epoch 133/500\n",
      "7/7 [==============================] - 2s 255ms/step - loss: 0.0573 - accuracy: 0.9839\n",
      "Epoch 134/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0593 - accuracy: 0.9809\n",
      "Epoch 135/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0565 - accuracy: 0.9815\n",
      "Epoch 136/500\n",
      "7/7 [==============================] - 2s 255ms/step - loss: 0.0567 - accuracy: 0.9825\n",
      "Epoch 137/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0585 - accuracy: 0.9830\n",
      "Epoch 138/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0593 - accuracy: 0.9819\n",
      "Epoch 139/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0634 - accuracy: 0.9796\n",
      "Epoch 140/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0591 - accuracy: 0.9822\n",
      "Epoch 141/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0567 - accuracy: 0.9825\n",
      "Epoch 142/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0568 - accuracy: 0.9830\n",
      "Epoch 143/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0593 - accuracy: 0.9816\n",
      "Epoch 144/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0578 - accuracy: 0.9828\n",
      "Epoch 145/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0583 - accuracy: 0.9828\n",
      "Epoch 146/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0586 - accuracy: 0.9816\n",
      "Epoch 147/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0580 - accuracy: 0.9815\n",
      "Epoch 148/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0572 - accuracy: 0.9836\n",
      "Epoch 149/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0532 - accuracy: 0.9830\n",
      "Epoch 150/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0598 - accuracy: 0.9815\n",
      "Epoch 151/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0581 - accuracy: 0.9799\n",
      "Epoch 152/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0558 - accuracy: 0.9840\n",
      "Epoch 153/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0562 - accuracy: 0.9839\n",
      "Epoch 154/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0575 - accuracy: 0.9815\n",
      "Epoch 155/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0538 - accuracy: 0.9839\n",
      "Epoch 156/500\n",
      "7/7 [==============================] - 2s 255ms/step - loss: 0.0567 - accuracy: 0.9822\n",
      "Epoch 157/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0597 - accuracy: 0.9818\n",
      "Epoch 158/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0591 - accuracy: 0.9818\n",
      "Epoch 159/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0564 - accuracy: 0.9826\n",
      "Epoch 160/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0531 - accuracy: 0.9846\n",
      "Epoch 161/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0529 - accuracy: 0.9842\n",
      "Epoch 162/500\n",
      "7/7 [==============================] - 2s 255ms/step - loss: 0.0537 - accuracy: 0.9840\n",
      "Epoch 163/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0535 - accuracy: 0.9848\n",
      "Epoch 164/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0523 - accuracy: 0.9845\n",
      "Epoch 165/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0532 - accuracy: 0.9852\n",
      "Epoch 166/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0543 - accuracy: 0.9832\n",
      "Epoch 167/500\n",
      "7/7 [==============================] - 2s 255ms/step - loss: 0.0532 - accuracy: 0.9836\n",
      "Epoch 168/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0550 - accuracy: 0.9829\n",
      "Epoch 169/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0510 - accuracy: 0.9849\n",
      "Epoch 170/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0540 - accuracy: 0.9825\n",
      "Epoch 171/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0532 - accuracy: 0.9845\n",
      "Epoch 172/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0587 - accuracy: 0.9812\n",
      "Epoch 173/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0519 - accuracy: 0.9852\n",
      "Epoch 174/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0541 - accuracy: 0.9838\n",
      "Epoch 175/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0507 - accuracy: 0.9861\n",
      "Epoch 176/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0509 - accuracy: 0.9846\n",
      "Epoch 177/500\n",
      "7/7 [==============================] - 2s 279ms/step - loss: 0.0552 - accuracy: 0.9829\n",
      "Epoch 178/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0571 - accuracy: 0.9820\n",
      "Epoch 179/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0509 - accuracy: 0.9848\n",
      "Epoch 180/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0509 - accuracy: 0.9848\n",
      "Epoch 181/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0485 - accuracy: 0.9859\n",
      "Epoch 182/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0489 - accuracy: 0.9846\n",
      "Epoch 183/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0522 - accuracy: 0.9836\n",
      "Epoch 184/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0502 - accuracy: 0.9846\n",
      "Epoch 185/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0496 - accuracy: 0.9852\n",
      "Epoch 186/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0535 - accuracy: 0.9826\n",
      "Epoch 187/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0499 - accuracy: 0.9842\n",
      "Epoch 188/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0547 - accuracy: 0.9840\n",
      "Epoch 189/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0490 - accuracy: 0.9851\n",
      "Epoch 190/500\n",
      "7/7 [==============================] - 2s 257ms/step - loss: 0.0478 - accuracy: 0.9866\n",
      "Epoch 191/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0499 - accuracy: 0.9853\n",
      "Epoch 192/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0500 - accuracy: 0.9858\n",
      "Epoch 193/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0467 - accuracy: 0.9858\n",
      "Epoch 194/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0498 - accuracy: 0.9858\n",
      "Epoch 195/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0503 - accuracy: 0.9848\n",
      "Epoch 196/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0522 - accuracy: 0.9835\n",
      "Epoch 197/500\n",
      "7/7 [==============================] - 2s 255ms/step - loss: 0.0506 - accuracy: 0.9842\n",
      "Epoch 198/500\n",
      "7/7 [==============================] - 2s 272ms/step - loss: 0.0487 - accuracy: 0.9843\n",
      "Epoch 199/500\n",
      "7/7 [==============================] - 2s 255ms/step - loss: 0.0479 - accuracy: 0.9863\n",
      "Epoch 200/500\n",
      "7/7 [==============================] - 2s 256ms/step - loss: 0.0483 - accuracy: 0.9858\n",
      "Epoch 201/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0464 - accuracy: 0.9872\n",
      "Epoch 202/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0478 - accuracy: 0.9855\n",
      "Epoch 203/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0481 - accuracy: 0.9849\n",
      "Epoch 204/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0469 - accuracy: 0.9872\n",
      "Epoch 205/500\n",
      "7/7 [==============================] - 2s 272ms/step - loss: 0.0558 - accuracy: 0.9829\n",
      "Epoch 206/500\n",
      "7/7 [==============================] - 2s 250ms/step - loss: 0.0459 - accuracy: 0.9853\n",
      "Epoch 207/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0514 - accuracy: 0.9835\n",
      "Epoch 208/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0472 - accuracy: 0.9856\n",
      "Epoch 209/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0500 - accuracy: 0.9843\n",
      "Epoch 210/500\n",
      "7/7 [==============================] - 2s 265ms/step - loss: 0.0521 - accuracy: 0.9845\n",
      "Epoch 211/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0495 - accuracy: 0.9851\n",
      "Epoch 212/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0469 - accuracy: 0.9858\n",
      "Epoch 213/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0488 - accuracy: 0.9852\n",
      "Epoch 214/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0534 - accuracy: 0.9840\n",
      "Epoch 215/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0557 - accuracy: 0.9828\n",
      "Epoch 216/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0497 - accuracy: 0.9849\n",
      "Epoch 217/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0518 - accuracy: 0.9848\n",
      "Epoch 218/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0499 - accuracy: 0.9849\n",
      "Epoch 219/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0498 - accuracy: 0.9855\n",
      "Epoch 220/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0452 - accuracy: 0.9868\n",
      "Epoch 221/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0462 - accuracy: 0.9859\n",
      "Epoch 222/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0463 - accuracy: 0.9851\n",
      "Epoch 223/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0476 - accuracy: 0.9851\n",
      "Epoch 224/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0478 - accuracy: 0.9855\n",
      "Epoch 225/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0487 - accuracy: 0.9843\n",
      "Epoch 226/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0464 - accuracy: 0.9856\n",
      "Epoch 227/500\n",
      "7/7 [==============================] - 2s 263ms/step - loss: 0.0478 - accuracy: 0.9861\n",
      "Epoch 228/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0465 - accuracy: 0.9858\n",
      "Epoch 229/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0465 - accuracy: 0.9874\n",
      "Epoch 230/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0437 - accuracy: 0.9871\n",
      "Epoch 231/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0460 - accuracy: 0.9856\n",
      "Epoch 232/500\n",
      "7/7 [==============================] - 2s 260ms/step - loss: 0.0445 - accuracy: 0.9859\n",
      "Epoch 233/500\n",
      "7/7 [==============================] - 2s 266ms/step - loss: 0.0466 - accuracy: 0.9868\n",
      "Epoch 234/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0466 - accuracy: 0.9852\n",
      "Epoch 235/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0481 - accuracy: 0.9853\n",
      "Epoch 236/500\n",
      "7/7 [==============================] - 2s 250ms/step - loss: 0.0477 - accuracy: 0.9848\n",
      "Epoch 237/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0439 - accuracy: 0.9868\n",
      "Epoch 238/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0472 - accuracy: 0.9863\n",
      "Epoch 239/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0494 - accuracy: 0.9849\n",
      "Epoch 240/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0481 - accuracy: 0.9851\n",
      "Epoch 241/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0497 - accuracy: 0.9842\n",
      "Epoch 242/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0482 - accuracy: 0.9861\n",
      "Epoch 243/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0510 - accuracy: 0.9842\n",
      "Epoch 244/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0546 - accuracy: 0.9825\n",
      "Epoch 245/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0497 - accuracy: 0.9840\n",
      "Epoch 246/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0485 - accuracy: 0.9840\n",
      "Epoch 247/500\n",
      "7/7 [==============================] - 2s 250ms/step - loss: 0.0468 - accuracy: 0.9856\n",
      "Epoch 248/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0448 - accuracy: 0.9865\n",
      "Epoch 249/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0484 - accuracy: 0.9853\n",
      "Epoch 250/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0462 - accuracy: 0.9852\n",
      "Epoch 251/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0425 - accuracy: 0.9875\n",
      "Epoch 252/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0466 - accuracy: 0.9852\n",
      "Epoch 253/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0479 - accuracy: 0.9859\n",
      "Epoch 254/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0443 - accuracy: 0.9879\n",
      "Epoch 255/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0508 - accuracy: 0.9835\n",
      "Epoch 256/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0478 - accuracy: 0.9842\n",
      "Epoch 257/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0471 - accuracy: 0.9855\n",
      "Epoch 258/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0404 - accuracy: 0.9882\n",
      "Epoch 259/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0463 - accuracy: 0.9859\n",
      "Epoch 260/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0457 - accuracy: 0.9855\n",
      "Epoch 261/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0425 - accuracy: 0.9879\n",
      "Epoch 262/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0447 - accuracy: 0.9868\n",
      "Epoch 263/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0471 - accuracy: 0.9866\n",
      "Epoch 264/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0446 - accuracy: 0.9871\n",
      "Epoch 265/500\n",
      "7/7 [==============================] - 2s 261ms/step - loss: 0.0443 - accuracy: 0.9861\n",
      "Epoch 266/500\n",
      "7/7 [==============================] - 2s 255ms/step - loss: 0.0435 - accuracy: 0.9858\n",
      "Epoch 267/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0462 - accuracy: 0.9868\n",
      "Epoch 268/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0446 - accuracy: 0.9856\n",
      "Epoch 269/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0441 - accuracy: 0.9865\n",
      "Epoch 270/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0462 - accuracy: 0.9859\n",
      "Epoch 271/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0447 - accuracy: 0.9874\n",
      "Epoch 272/500\n",
      "7/7 [==============================] - 2s 266ms/step - loss: 0.0519 - accuracy: 0.9853\n",
      "Epoch 273/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0463 - accuracy: 0.9853\n",
      "Epoch 274/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0458 - accuracy: 0.9855\n",
      "Epoch 275/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0475 - accuracy: 0.9839\n",
      "Epoch 276/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0428 - accuracy: 0.9875\n",
      "Epoch 277/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0433 - accuracy: 0.9861\n",
      "Epoch 278/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0447 - accuracy: 0.9863\n",
      "Epoch 279/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0475 - accuracy: 0.9842\n",
      "Epoch 280/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0460 - accuracy: 0.9842\n",
      "Epoch 281/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0475 - accuracy: 0.9861\n",
      "Epoch 282/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0423 - accuracy: 0.9872\n",
      "Epoch 283/500\n",
      "7/7 [==============================] - 2s 257ms/step - loss: 0.0449 - accuracy: 0.9866\n",
      "Epoch 284/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0436 - accuracy: 0.9853\n",
      "Epoch 285/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0504 - accuracy: 0.9838\n",
      "Epoch 286/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0461 - accuracy: 0.9859\n",
      "Epoch 287/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0428 - accuracy: 0.9876\n",
      "Epoch 288/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0398 - accuracy: 0.9878\n",
      "Epoch 289/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0422 - accuracy: 0.9891\n",
      "Epoch 290/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0394 - accuracy: 0.9885\n",
      "Epoch 291/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0446 - accuracy: 0.9859\n",
      "Epoch 292/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0446 - accuracy: 0.9868\n",
      "Epoch 293/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0468 - accuracy: 0.9858\n",
      "Epoch 294/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0453 - accuracy: 0.9869\n",
      "Epoch 295/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0451 - accuracy: 0.9853\n",
      "Epoch 296/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0418 - accuracy: 0.9872\n",
      "Epoch 297/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0435 - accuracy: 0.9853\n",
      "Epoch 298/500\n",
      "7/7 [==============================] - 2s 266ms/step - loss: 0.0432 - accuracy: 0.9872\n",
      "Epoch 299/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0463 - accuracy: 0.9851\n",
      "Epoch 300/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0402 - accuracy: 0.9881\n",
      "Epoch 301/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0434 - accuracy: 0.9876\n",
      "Epoch 302/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0441 - accuracy: 0.9868\n",
      "Epoch 303/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0421 - accuracy: 0.9869\n",
      "Epoch 304/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0405 - accuracy: 0.9875\n",
      "Epoch 305/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0387 - accuracy: 0.9884\n",
      "Epoch 306/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0397 - accuracy: 0.9889\n",
      "Epoch 307/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0504 - accuracy: 0.9849\n",
      "Epoch 308/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0449 - accuracy: 0.9866\n",
      "Epoch 309/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0458 - accuracy: 0.9869\n",
      "Epoch 310/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0403 - accuracy: 0.9881\n",
      "Epoch 311/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0421 - accuracy: 0.9872\n",
      "Epoch 312/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0402 - accuracy: 0.9872\n",
      "Epoch 313/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0424 - accuracy: 0.9871\n",
      "Epoch 314/500\n",
      "7/7 [==============================] - 2s 250ms/step - loss: 0.0408 - accuracy: 0.9875\n",
      "Epoch 315/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0412 - accuracy: 0.9866\n",
      "Epoch 316/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0432 - accuracy: 0.9865\n",
      "Epoch 317/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0418 - accuracy: 0.9878\n",
      "Epoch 318/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0433 - accuracy: 0.9869\n",
      "Epoch 319/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0414 - accuracy: 0.9878\n",
      "Epoch 320/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0420 - accuracy: 0.9871\n",
      "Epoch 321/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0432 - accuracy: 0.9859\n",
      "Epoch 322/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0501 - accuracy: 0.9840\n",
      "Epoch 323/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0378 - accuracy: 0.9879\n",
      "Epoch 324/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0413 - accuracy: 0.9866\n",
      "Epoch 325/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0404 - accuracy: 0.9875\n",
      "Epoch 326/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0454 - accuracy: 0.9855\n",
      "Epoch 327/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0449 - accuracy: 0.9855\n",
      "Epoch 328/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0447 - accuracy: 0.9874\n",
      "Epoch 329/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0394 - accuracy: 0.9869\n",
      "Epoch 330/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0408 - accuracy: 0.9879\n",
      "Epoch 331/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0435 - accuracy: 0.9872\n",
      "Epoch 332/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0417 - accuracy: 0.9866\n",
      "Epoch 333/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0387 - accuracy: 0.9891\n",
      "Epoch 334/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0421 - accuracy: 0.9869\n",
      "Epoch 335/500\n",
      "7/7 [==============================] - 2s 255ms/step - loss: 0.0410 - accuracy: 0.9863\n",
      "Epoch 336/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0419 - accuracy: 0.9875\n",
      "Epoch 337/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0415 - accuracy: 0.9872\n",
      "Epoch 338/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0417 - accuracy: 0.9875\n",
      "Epoch 339/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0394 - accuracy: 0.9879\n",
      "Epoch 340/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0380 - accuracy: 0.9886\n",
      "Epoch 341/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0432 - accuracy: 0.9866\n",
      "Epoch 342/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0419 - accuracy: 0.9868\n",
      "Epoch 343/500\n",
      "7/7 [==============================] - 2s 269ms/step - loss: 0.0426 - accuracy: 0.9865\n",
      "Epoch 344/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0380 - accuracy: 0.9888\n",
      "Epoch 345/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0403 - accuracy: 0.9871\n",
      "Epoch 346/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0380 - accuracy: 0.9898\n",
      "Epoch 347/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0380 - accuracy: 0.9889\n",
      "Epoch 348/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0382 - accuracy: 0.9885\n",
      "Epoch 349/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0409 - accuracy: 0.9869\n",
      "Epoch 350/500\n",
      "7/7 [==============================] - 2s 250ms/step - loss: 0.0400 - accuracy: 0.9869\n",
      "Epoch 351/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0382 - accuracy: 0.9885\n",
      "Epoch 352/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0392 - accuracy: 0.9888\n",
      "Epoch 353/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0379 - accuracy: 0.9884\n",
      "Epoch 354/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0406 - accuracy: 0.9875\n",
      "Epoch 355/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0398 - accuracy: 0.9879\n",
      "Epoch 356/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0367 - accuracy: 0.9894\n",
      "Epoch 357/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0389 - accuracy: 0.9875\n",
      "Epoch 358/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0392 - accuracy: 0.9878\n",
      "Epoch 359/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0391 - accuracy: 0.9882\n",
      "Epoch 360/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0367 - accuracy: 0.9889\n",
      "Epoch 361/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0410 - accuracy: 0.9875\n",
      "Epoch 362/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0447 - accuracy: 0.9856\n",
      "Epoch 363/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0473 - accuracy: 0.9851\n",
      "Epoch 364/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0460 - accuracy: 0.9862\n",
      "Epoch 365/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0443 - accuracy: 0.9848\n",
      "Epoch 366/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0389 - accuracy: 0.9876\n",
      "Epoch 367/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0379 - accuracy: 0.9891\n",
      "Epoch 368/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0363 - accuracy: 0.9895\n",
      "Epoch 369/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0408 - accuracy: 0.9871\n",
      "Epoch 370/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0413 - accuracy: 0.9876\n",
      "Epoch 371/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0410 - accuracy: 0.9866\n",
      "Epoch 372/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0399 - accuracy: 0.9879\n",
      "Epoch 373/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0435 - accuracy: 0.9875\n",
      "Epoch 374/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0411 - accuracy: 0.9872\n",
      "Epoch 375/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0414 - accuracy: 0.9871\n",
      "Epoch 376/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0433 - accuracy: 0.9866\n",
      "Epoch 377/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0378 - accuracy: 0.9891\n",
      "Epoch 378/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0379 - accuracy: 0.9886\n",
      "Epoch 379/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0393 - accuracy: 0.9878\n",
      "Epoch 380/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0412 - accuracy: 0.9875\n",
      "Epoch 381/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0355 - accuracy: 0.9886\n",
      "Epoch 382/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0370 - accuracy: 0.9889\n",
      "Epoch 383/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0365 - accuracy: 0.9886\n",
      "Epoch 384/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0371 - accuracy: 0.9889\n",
      "Epoch 385/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0380 - accuracy: 0.9882\n",
      "Epoch 386/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0369 - accuracy: 0.9898\n",
      "Epoch 387/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0373 - accuracy: 0.9884\n",
      "Epoch 388/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0390 - accuracy: 0.9885\n",
      "Epoch 389/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0376 - accuracy: 0.9884\n",
      "Epoch 390/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0383 - accuracy: 0.9885\n",
      "Epoch 391/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0415 - accuracy: 0.9869\n",
      "Epoch 392/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0382 - accuracy: 0.9881\n",
      "Epoch 393/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0351 - accuracy: 0.9898\n",
      "Epoch 394/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0373 - accuracy: 0.9889\n",
      "Epoch 395/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0377 - accuracy: 0.9892\n",
      "Epoch 396/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0410 - accuracy: 0.9863\n",
      "Epoch 397/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0370 - accuracy: 0.9886\n",
      "Epoch 398/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0380 - accuracy: 0.9889\n",
      "Epoch 399/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0381 - accuracy: 0.9898\n",
      "Epoch 400/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0374 - accuracy: 0.9889\n",
      "Epoch 401/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0384 - accuracy: 0.9889\n",
      "Epoch 402/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0366 - accuracy: 0.9882\n",
      "Epoch 403/500\n",
      "7/7 [==============================] - 2s 255ms/step - loss: 0.0392 - accuracy: 0.9882\n",
      "Epoch 404/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0371 - accuracy: 0.9881\n",
      "Epoch 405/500\n",
      "7/7 [==============================] - 2s 275ms/step - loss: 0.0387 - accuracy: 0.9875\n",
      "Epoch 406/500\n",
      "7/7 [==============================] - 2s 256ms/step - loss: 0.0384 - accuracy: 0.9891\n",
      "Epoch 407/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0388 - accuracy: 0.9898\n",
      "Epoch 408/500\n",
      "7/7 [==============================] - 2s 255ms/step - loss: 0.0374 - accuracy: 0.9888\n",
      "Epoch 409/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0351 - accuracy: 0.9895\n",
      "Epoch 410/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0357 - accuracy: 0.9888\n",
      "Epoch 411/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0372 - accuracy: 0.9882\n",
      "Epoch 412/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0376 - accuracy: 0.9884\n",
      "Epoch 413/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0324 - accuracy: 0.9901\n",
      "Epoch 414/500\n",
      "7/7 [==============================] - 2s 259ms/step - loss: 0.0318 - accuracy: 0.9907\n",
      "Epoch 415/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0359 - accuracy: 0.9894\n",
      "Epoch 416/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0407 - accuracy: 0.9882\n",
      "Epoch 417/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0349 - accuracy: 0.9899\n",
      "Epoch 418/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0347 - accuracy: 0.9898\n",
      "Epoch 419/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0426 - accuracy: 0.9872\n",
      "Epoch 420/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0367 - accuracy: 0.9886\n",
      "Epoch 421/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0354 - accuracy: 0.9899\n",
      "Epoch 422/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0373 - accuracy: 0.9899\n",
      "Epoch 423/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0350 - accuracy: 0.9898\n",
      "Epoch 424/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0326 - accuracy: 0.9904\n",
      "Epoch 425/500\n",
      "7/7 [==============================] - 2s 271ms/step - loss: 0.0352 - accuracy: 0.9898\n",
      "Epoch 426/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0359 - accuracy: 0.9902\n",
      "Epoch 427/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0362 - accuracy: 0.9888\n",
      "Epoch 428/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0361 - accuracy: 0.9899\n",
      "Epoch 429/500\n",
      "7/7 [==============================] - 2s 257ms/step - loss: 0.0377 - accuracy: 0.9886\n",
      "Epoch 430/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0351 - accuracy: 0.9895\n",
      "Epoch 431/500\n",
      "7/7 [==============================] - 2s 255ms/step - loss: 0.0333 - accuracy: 0.9908\n",
      "Epoch 432/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0346 - accuracy: 0.9898\n",
      "Epoch 433/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0349 - accuracy: 0.9901\n",
      "Epoch 434/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0356 - accuracy: 0.9888\n",
      "Epoch 435/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0338 - accuracy: 0.9898\n",
      "Epoch 436/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0382 - accuracy: 0.9885\n",
      "Epoch 437/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0370 - accuracy: 0.9882\n",
      "Epoch 438/500\n",
      "7/7 [==============================] - 2s 250ms/step - loss: 0.0395 - accuracy: 0.9871\n",
      "Epoch 439/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0362 - accuracy: 0.9886\n",
      "Epoch 440/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0321 - accuracy: 0.9902\n",
      "Epoch 441/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0314 - accuracy: 0.9909\n",
      "Epoch 442/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0400 - accuracy: 0.9875\n",
      "Epoch 443/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0378 - accuracy: 0.9882\n",
      "Epoch 444/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0331 - accuracy: 0.9898\n",
      "Epoch 445/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0363 - accuracy: 0.9884\n",
      "Epoch 446/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0343 - accuracy: 0.9899\n",
      "Epoch 447/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0370 - accuracy: 0.9886\n",
      "Epoch 448/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0397 - accuracy: 0.9862\n",
      "Epoch 449/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0337 - accuracy: 0.9898\n",
      "Epoch 450/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0356 - accuracy: 0.9894\n",
      "Epoch 451/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0328 - accuracy: 0.9889\n",
      "Epoch 452/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0355 - accuracy: 0.9897\n",
      "Epoch 453/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0310 - accuracy: 0.9905\n",
      "Epoch 454/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0360 - accuracy: 0.9888\n",
      "Epoch 455/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0361 - accuracy: 0.9886\n",
      "Epoch 456/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0404 - accuracy: 0.9874\n",
      "Epoch 457/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0339 - accuracy: 0.9898\n",
      "Epoch 458/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0383 - accuracy: 0.9884\n",
      "Epoch 459/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0343 - accuracy: 0.9895\n",
      "Epoch 460/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0343 - accuracy: 0.9884\n",
      "Epoch 461/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0355 - accuracy: 0.9881\n",
      "Epoch 462/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0378 - accuracy: 0.9886\n",
      "Epoch 463/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0339 - accuracy: 0.9901\n",
      "Epoch 464/500\n",
      "7/7 [==============================] - 2s 275ms/step - loss: 0.0312 - accuracy: 0.9909\n",
      "Epoch 465/500\n",
      "7/7 [==============================] - 2s 250ms/step - loss: 0.0330 - accuracy: 0.9901\n",
      "Epoch 466/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0326 - accuracy: 0.9907\n",
      "Epoch 467/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0328 - accuracy: 0.9895\n",
      "Epoch 468/500\n",
      "7/7 [==============================] - 2s 265ms/step - loss: 0.0354 - accuracy: 0.9894\n",
      "Epoch 469/500\n",
      "7/7 [==============================] - 2s 262ms/step - loss: 0.0374 - accuracy: 0.9876\n",
      "Epoch 470/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0378 - accuracy: 0.9889\n",
      "Epoch 471/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0353 - accuracy: 0.9874\n",
      "Epoch 472/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0369 - accuracy: 0.9884\n",
      "Epoch 473/500\n",
      "7/7 [==============================] - 2s 257ms/step - loss: 0.0347 - accuracy: 0.9888\n",
      "Epoch 474/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0380 - accuracy: 0.9874\n",
      "Epoch 475/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0327 - accuracy: 0.9904\n",
      "Epoch 476/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0329 - accuracy: 0.9908\n",
      "Epoch 477/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0311 - accuracy: 0.9897\n",
      "Epoch 478/500\n",
      "7/7 [==============================] - 2s 250ms/step - loss: 0.0353 - accuracy: 0.9889\n",
      "Epoch 479/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0312 - accuracy: 0.9897\n",
      "Epoch 480/500\n",
      "7/7 [==============================] - 2s 254ms/step - loss: 0.0353 - accuracy: 0.9885\n",
      "Epoch 481/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0381 - accuracy: 0.9876\n",
      "Epoch 482/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0344 - accuracy: 0.9884\n",
      "Epoch 483/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0349 - accuracy: 0.9889\n",
      "Epoch 484/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0360 - accuracy: 0.9895\n",
      "Epoch 485/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0387 - accuracy: 0.9866\n",
      "Epoch 486/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0329 - accuracy: 0.9894\n",
      "Epoch 487/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0364 - accuracy: 0.9886\n",
      "Epoch 488/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0379 - accuracy: 0.9881\n",
      "Epoch 489/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0381 - accuracy: 0.9872\n",
      "Epoch 490/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0368 - accuracy: 0.9899\n",
      "Epoch 491/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0345 - accuracy: 0.9882\n",
      "Epoch 492/500\n",
      "7/7 [==============================] - 2s 253ms/step - loss: 0.0348 - accuracy: 0.9897\n",
      "Epoch 493/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0309 - accuracy: 0.9911\n",
      "Epoch 494/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0326 - accuracy: 0.9901\n",
      "Epoch 495/500\n",
      "7/7 [==============================] - 2s 249ms/step - loss: 0.0326 - accuracy: 0.9907\n",
      "Epoch 496/500\n",
      "7/7 [==============================] - 2s 251ms/step - loss: 0.0296 - accuracy: 0.9920\n",
      "Epoch 497/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0296 - accuracy: 0.9908\n",
      "Epoch 498/500\n",
      "7/7 [==============================] - 2s 256ms/step - loss: 0.0322 - accuracy: 0.9909\n",
      "Epoch 499/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0304 - accuracy: 0.9920\n",
      "Epoch 500/500\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.0316 - accuracy: 0.9907\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig(vocab_size=15, hidden_size=16, num_hidden_layer=8, num_attention_heads=4,\n",
    "                    intermediate_size=64, max_position_embeddings=14)\n",
    "bert = TFBertModel(config)\n",
    "\n",
    "input_seq = tf.keras.layers.Input(shape=(14,), name='input_seq', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(14,), name='attention_mask', dtype='int32')\n",
    "\n",
    "encoder = bert(input_seq, attention_mask=mask)[0][:,0] \n",
    "\n",
    "x = tf.keras.layers.Dense(32, activation='relu')(encoder)\n",
    "y = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_seq, mask], outputs=y)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, decay=1e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
    "\n",
    "X_mask = np.ones((len(X_train),14))\n",
    "history = model.fit([X_train, X_mask], y_train_onehot, batch_size=1024, epochs=500)\n",
    "model.save_weights('./checkpoint_Bert/Transformer_Bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training - Loss Function')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.subplot()\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Training - Loss Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 3s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.6448775e-02, 3.0260968e-03, 9.8049098e-01, 3.4161436e-05,\n",
       "        1.0143701e-08],\n",
       "       [6.6307746e-09, 3.0830529e-07, 2.4225022e-07, 1.8034127e-03,\n",
       "        9.9819607e-01],\n",
       "       [9.9998420e-01, 1.4281015e-05, 9.7421076e-09, 1.4540957e-06,\n",
       "        2.2030476e-12],\n",
       "       ...,\n",
       "       [9.9997878e-01, 1.9820520e-05, 8.2963298e-09, 1.4654826e-06,\n",
       "        2.5373625e-12],\n",
       "       [2.0826361e-04, 2.2738825e-06, 9.9977356e-01, 1.5801674e-05,\n",
       "        7.8163553e-10],\n",
       "       [9.9997580e-01, 2.2477867e-05, 7.8304465e-09, 1.6271752e-06,\n",
       "        2.6890742e-12]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_mask = np.ones((len(X_test),14))\n",
    "y_prob = model.predict([X_test, X_test_mask])\n",
    "y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mac_hash</th>\n",
       "      <th>C0</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b882f1d44602a25349a08f4a0af32977</td>\n",
       "      <td>1.644878e-02</td>\n",
       "      <td>3.026097e-03</td>\n",
       "      <td>9.804910e-01</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>1.014370e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86e644b498922f3a2fd0e6e1a1525de3</td>\n",
       "      <td>6.630775e-09</td>\n",
       "      <td>3.083053e-07</td>\n",
       "      <td>2.422502e-07</td>\n",
       "      <td>0.001803</td>\n",
       "      <td>9.981961e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>691662b04ee08015062d901a4c5628b1</td>\n",
       "      <td>9.999842e-01</td>\n",
       "      <td>1.428101e-05</td>\n",
       "      <td>9.742108e-09</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.203048e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52b5c510a28774237f4f118764c2ed6f</td>\n",
       "      <td>1.278075e-05</td>\n",
       "      <td>9.999846e-01</td>\n",
       "      <td>1.420635e-08</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.989767e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9f3c995e53d109f532056b6eae29a0b5</td>\n",
       "      <td>5.603024e-05</td>\n",
       "      <td>1.705543e-02</td>\n",
       "      <td>9.828635e-01</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>3.136811e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>5001c0dfe522886ed884c0dc4e5848df</td>\n",
       "      <td>9.999769e-01</td>\n",
       "      <td>2.151068e-05</td>\n",
       "      <td>7.950217e-09</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>2.632553e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425</th>\n",
       "      <td>f1c8781e701e89068fa0b8a62ecbc564</td>\n",
       "      <td>1.104203e-08</td>\n",
       "      <td>7.250752e-07</td>\n",
       "      <td>2.572630e-07</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>9.984059e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426</th>\n",
       "      <td>646136b402e136422466a2acd8636630</td>\n",
       "      <td>9.999788e-01</td>\n",
       "      <td>1.982052e-05</td>\n",
       "      <td>8.296330e-09</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>2.537362e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427</th>\n",
       "      <td>c7bea6491029f86ab3d1f0f9b599ca2c</td>\n",
       "      <td>2.082636e-04</td>\n",
       "      <td>2.273883e-06</td>\n",
       "      <td>9.997736e-01</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>7.816355e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428</th>\n",
       "      <td>822fd3104f38cc6374f2996d3d6920a2</td>\n",
       "      <td>9.999758e-01</td>\n",
       "      <td>2.247787e-05</td>\n",
       "      <td>7.830447e-09</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>2.689074e-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3429 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              mac_hash            C0            C1  \\\n",
       "0     b882f1d44602a25349a08f4a0af32977  1.644878e-02  3.026097e-03   \n",
       "1     86e644b498922f3a2fd0e6e1a1525de3  6.630775e-09  3.083053e-07   \n",
       "2     691662b04ee08015062d901a4c5628b1  9.999842e-01  1.428101e-05   \n",
       "3     52b5c510a28774237f4f118764c2ed6f  1.278075e-05  9.999846e-01   \n",
       "4     9f3c995e53d109f532056b6eae29a0b5  5.603024e-05  1.705543e-02   \n",
       "...                                ...           ...           ...   \n",
       "3424  5001c0dfe522886ed884c0dc4e5848df  9.999769e-01  2.151068e-05   \n",
       "3425  f1c8781e701e89068fa0b8a62ecbc564  1.104203e-08  7.250752e-07   \n",
       "3426  646136b402e136422466a2acd8636630  9.999788e-01  1.982052e-05   \n",
       "3427  c7bea6491029f86ab3d1f0f9b599ca2c  2.082636e-04  2.273883e-06   \n",
       "3428  822fd3104f38cc6374f2996d3d6920a2  9.999758e-01  2.247787e-05   \n",
       "\n",
       "                C2        C3            C4  \n",
       "0     9.804910e-01  0.000034  1.014370e-08  \n",
       "1     2.422502e-07  0.001803  9.981961e-01  \n",
       "2     9.742108e-09  0.000001  2.203048e-12  \n",
       "3     1.420635e-08  0.000003  2.989767e-08  \n",
       "4     9.828635e-01  0.000022  3.136811e-06  \n",
       "...            ...       ...           ...  \n",
       "3424  7.950217e-09  0.000002  2.632553e-12  \n",
       "3425  2.572630e-07  0.001593  9.984059e-01  \n",
       "3426  8.296330e-09  0.000001  2.537362e-12  \n",
       "3427  9.997736e-01  0.000016  7.816355e-10  \n",
       "3428  7.830447e-09  0.000002  2.689074e-12  \n",
       "\n",
       "[3429 rows x 6 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame({'mac_hash':sample['mac_hash'], 'C0':y_prob[:,0], 'C1':y_prob[:,1], 'C2':y_prob[:,2], 'C3':y_prob[:,3], 'C4':y_prob[:,4]})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"output_transformer_bert.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#FF0000> Transformer XLNet (w/o group)</font> \t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (with validation set spiltted from training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_seq (InputLayer)         [(None, 14)]         0           []                               \n",
      "                                                                                                  \n",
      " tfxl_net_model (TFXLNetModel)  TFXLNetModelOutput(  28672       ['input_seq[0][0]']              \n",
      "                                last_hidden_state=(                                               \n",
      "                                None, 14, 16),                                                    \n",
      "                                 mems=((14, None, 1                                               \n",
      "                                6),                                                               \n",
      "                                 (14, None, 16),                                                  \n",
      "                                 (14, None, 16),                                                  \n",
      "                                 (14, None, 16),                                                  \n",
      "                                 (14, None, 16),                                                  \n",
      "                                 (14, None, 16),                                                  \n",
      "                                 (14, None, 16),                                                  \n",
      "                                 (14, None, 16)),                                                 \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2 (Sl  (None, 16)          0           ['tfxl_net_model[0][0]']         \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 32)           544         ['tf.__operators__.getitem_2[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 14)]         0           []                               \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 5)            165         ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 29,381\n",
      "Trainable params: 29,381\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetConfig, TFXLNetModel\n",
    "data = np.array(list(id_dict.values()))\n",
    "X_train = data[:,:-1]\n",
    "y_train = data[:,-1]\n",
    "y_train = np.eye(5)[y_train]\n",
    "\n",
    "\n",
    "config = XLNetConfig(vocab_size=15, d_model=16, n_layer=8, n_head=4, d_inner=64)\n",
    "\n",
    "xlnet = TFXLNetModel(config)\n",
    "\n",
    "input_seq = tf.keras.layers.Input(shape=(14,), name='input_seq', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(14,), name='attention_mask', dtype='int32')\n",
    "\n",
    "embeddings = xlnet(input_seq)[0]\n",
    "embeddings = embeddings[:, 0]\n",
    "\n",
    "x = tf.keras.layers.Dense(32, activation='relu')(embeddings)\n",
    "y = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(x)\n",
    "model = tf.keras.Model(inputs=[input_seq, mask], outputs=y)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, decay=1e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model/transformer/mask_emb:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model/transformer/layer_._7/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "7/7 [==============================] - 9s 486ms/step - loss: 1.6217 - accuracy: 0.2611 - val_loss: 1.4257 - val_accuracy: 0.4138\n",
      "Epoch 2/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 1.4688 - accuracy: 0.4116 - val_loss: 1.3783 - val_accuracy: 0.4181\n",
      "Epoch 3/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 1.3938 - accuracy: 0.4662 - val_loss: 1.2456 - val_accuracy: 0.6509\n",
      "Epoch 4/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 1.2607 - accuracy: 0.6115 - val_loss: 1.1002 - val_accuracy: 0.6537\n",
      "Epoch 5/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 1.1113 - accuracy: 0.6344 - val_loss: 0.9643 - val_accuracy: 0.6552\n",
      "Epoch 6/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.9873 - accuracy: 0.6433 - val_loss: 0.8801 - val_accuracy: 0.6739\n",
      "Epoch 7/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.9093 - accuracy: 0.6588 - val_loss: 0.8129 - val_accuracy: 0.6968\n",
      "Epoch 8/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.8393 - accuracy: 0.6834 - val_loss: 0.7421 - val_accuracy: 0.7098\n",
      "Epoch 9/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.7738 - accuracy: 0.7115 - val_loss: 0.6795 - val_accuracy: 0.7414\n",
      "Epoch 10/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.7067 - accuracy: 0.7337 - val_loss: 0.6294 - val_accuracy: 0.7572\n",
      "Epoch 11/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.6542 - accuracy: 0.7479 - val_loss: 0.6044 - val_accuracy: 0.7644\n",
      "Epoch 12/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.6137 - accuracy: 0.7485 - val_loss: 0.5570 - val_accuracy: 0.7701\n",
      "Epoch 13/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.5723 - accuracy: 0.7608 - val_loss: 0.5268 - val_accuracy: 0.7730\n",
      "Epoch 14/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.5509 - accuracy: 0.7581 - val_loss: 0.5100 - val_accuracy: 0.7874\n",
      "Epoch 15/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.5306 - accuracy: 0.7603 - val_loss: 0.4984 - val_accuracy: 0.7816\n",
      "Epoch 16/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.5204 - accuracy: 0.7603 - val_loss: 0.4764 - val_accuracy: 0.7874\n",
      "Epoch 17/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.5047 - accuracy: 0.7586 - val_loss: 0.4709 - val_accuracy: 0.7830\n",
      "Epoch 18/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.4943 - accuracy: 0.7602 - val_loss: 0.4614 - val_accuracy: 0.7802\n",
      "Epoch 19/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.4909 - accuracy: 0.7565 - val_loss: 0.4594 - val_accuracy: 0.7859\n",
      "Epoch 20/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.4880 - accuracy: 0.7651 - val_loss: 0.4552 - val_accuracy: 0.7802\n",
      "Epoch 21/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.4876 - accuracy: 0.7715 - val_loss: 0.4737 - val_accuracy: 0.7672\n",
      "Epoch 22/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.4814 - accuracy: 0.7707 - val_loss: 0.4521 - val_accuracy: 0.8089\n",
      "Epoch 23/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.4712 - accuracy: 0.7741 - val_loss: 0.4393 - val_accuracy: 0.7759\n",
      "Epoch 24/500\n",
      "7/7 [==============================] - 2s 325ms/step - loss: 0.4624 - accuracy: 0.7765 - val_loss: 0.4296 - val_accuracy: 0.8075\n",
      "Epoch 25/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.4536 - accuracy: 0.7859 - val_loss: 0.4449 - val_accuracy: 0.8060\n",
      "Epoch 26/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.4402 - accuracy: 0.8036 - val_loss: 0.4028 - val_accuracy: 0.8391\n",
      "Epoch 27/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.4104 - accuracy: 0.8330 - val_loss: 0.3425 - val_accuracy: 0.9037\n",
      "Epoch 28/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.3650 - accuracy: 0.8791 - val_loss: 0.2942 - val_accuracy: 0.9210\n",
      "Epoch 29/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.3322 - accuracy: 0.8917 - val_loss: 0.2635 - val_accuracy: 0.9210\n",
      "Epoch 30/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.2960 - accuracy: 0.9029 - val_loss: 0.2421 - val_accuracy: 0.9267\n",
      "Epoch 31/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.2732 - accuracy: 0.9114 - val_loss: 0.2182 - val_accuracy: 0.9382\n",
      "Epoch 32/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.2496 - accuracy: 0.9152 - val_loss: 0.1847 - val_accuracy: 0.9483\n",
      "Epoch 33/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.2232 - accuracy: 0.9293 - val_loss: 0.1902 - val_accuracy: 0.9425\n",
      "Epoch 34/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.2194 - accuracy: 0.9288 - val_loss: 0.2071 - val_accuracy: 0.9353\n",
      "Epoch 35/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.2023 - accuracy: 0.9369 - val_loss: 0.1544 - val_accuracy: 0.9483\n",
      "Epoch 36/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.1894 - accuracy: 0.9414 - val_loss: 0.1461 - val_accuracy: 0.9497\n",
      "Epoch 37/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.1815 - accuracy: 0.9452 - val_loss: 0.1393 - val_accuracy: 0.9526\n",
      "Epoch 38/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.1902 - accuracy: 0.9382 - val_loss: 0.1328 - val_accuracy: 0.9569\n",
      "Epoch 39/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.1776 - accuracy: 0.9416 - val_loss: 0.1241 - val_accuracy: 0.9626\n",
      "Epoch 40/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.1771 - accuracy: 0.9444 - val_loss: 0.1574 - val_accuracy: 0.9440\n",
      "Epoch 41/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.1636 - accuracy: 0.9494 - val_loss: 0.1127 - val_accuracy: 0.9626\n",
      "Epoch 42/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.1607 - accuracy: 0.9500 - val_loss: 0.1059 - val_accuracy: 0.9655\n",
      "Epoch 43/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.1526 - accuracy: 0.9510 - val_loss: 0.1310 - val_accuracy: 0.9497\n",
      "Epoch 44/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.1546 - accuracy: 0.9511 - val_loss: 0.1117 - val_accuracy: 0.9598\n",
      "Epoch 45/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.1484 - accuracy: 0.9529 - val_loss: 0.1026 - val_accuracy: 0.9655\n",
      "Epoch 46/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.1464 - accuracy: 0.9542 - val_loss: 0.1068 - val_accuracy: 0.9626\n",
      "Epoch 47/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.1539 - accuracy: 0.9513 - val_loss: 0.1156 - val_accuracy: 0.9612\n",
      "Epoch 48/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.1431 - accuracy: 0.9559 - val_loss: 0.0917 - val_accuracy: 0.9727\n",
      "Epoch 49/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.1429 - accuracy: 0.9550 - val_loss: 0.1042 - val_accuracy: 0.9626\n",
      "Epoch 50/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.1422 - accuracy: 0.9553 - val_loss: 0.0975 - val_accuracy: 0.9713\n",
      "Epoch 51/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.1409 - accuracy: 0.9550 - val_loss: 0.1034 - val_accuracy: 0.9684\n",
      "Epoch 52/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.1441 - accuracy: 0.9543 - val_loss: 0.0933 - val_accuracy: 0.9655\n",
      "Epoch 53/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.1324 - accuracy: 0.9594 - val_loss: 0.1044 - val_accuracy: 0.9612\n",
      "Epoch 54/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.1274 - accuracy: 0.9609 - val_loss: 0.0843 - val_accuracy: 0.9713\n",
      "Epoch 55/500\n",
      "7/7 [==============================] - 2s 327ms/step - loss: 0.1303 - accuracy: 0.9583 - val_loss: 0.0893 - val_accuracy: 0.9698\n",
      "Epoch 56/500\n",
      "7/7 [==============================] - 2s 325ms/step - loss: 0.1270 - accuracy: 0.9609 - val_loss: 0.0901 - val_accuracy: 0.9698\n",
      "Epoch 57/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.1217 - accuracy: 0.9604 - val_loss: 0.0715 - val_accuracy: 0.9727\n",
      "Epoch 58/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.1241 - accuracy: 0.9606 - val_loss: 0.0737 - val_accuracy: 0.9741\n",
      "Epoch 59/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.1188 - accuracy: 0.9631 - val_loss: 0.0916 - val_accuracy: 0.9670\n",
      "Epoch 60/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.1196 - accuracy: 0.9633 - val_loss: 0.1019 - val_accuracy: 0.9626\n",
      "Epoch 61/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.1257 - accuracy: 0.9614 - val_loss: 0.0903 - val_accuracy: 0.9713\n",
      "Epoch 62/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.1233 - accuracy: 0.9614 - val_loss: 0.0727 - val_accuracy: 0.9756\n",
      "Epoch 63/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.1201 - accuracy: 0.9618 - val_loss: 0.0716 - val_accuracy: 0.9727\n",
      "Epoch 64/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.1206 - accuracy: 0.9606 - val_loss: 0.1014 - val_accuracy: 0.9684\n",
      "Epoch 65/500\n",
      "7/7 [==============================] - 2s 326ms/step - loss: 0.1261 - accuracy: 0.9606 - val_loss: 0.1385 - val_accuracy: 0.9397\n",
      "Epoch 66/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.1285 - accuracy: 0.9580 - val_loss: 0.0724 - val_accuracy: 0.9756\n",
      "Epoch 67/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.1255 - accuracy: 0.9583 - val_loss: 0.0671 - val_accuracy: 0.9756\n",
      "Epoch 68/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.1137 - accuracy: 0.9636 - val_loss: 0.1267 - val_accuracy: 0.9670\n",
      "Epoch 69/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.1297 - accuracy: 0.9594 - val_loss: 0.0822 - val_accuracy: 0.9713\n",
      "Epoch 70/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.1308 - accuracy: 0.9570 - val_loss: 0.0780 - val_accuracy: 0.9684\n",
      "Epoch 71/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.1216 - accuracy: 0.9590 - val_loss: 0.1165 - val_accuracy: 0.9583\n",
      "Epoch 72/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.1199 - accuracy: 0.9615 - val_loss: 0.0805 - val_accuracy: 0.9727\n",
      "Epoch 73/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.1119 - accuracy: 0.9642 - val_loss: 0.0684 - val_accuracy: 0.9756\n",
      "Epoch 74/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.1043 - accuracy: 0.9679 - val_loss: 0.0802 - val_accuracy: 0.9713\n",
      "Epoch 75/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.1096 - accuracy: 0.9655 - val_loss: 0.0659 - val_accuracy: 0.9770\n",
      "Epoch 76/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.1071 - accuracy: 0.9684 - val_loss: 0.0652 - val_accuracy: 0.9756\n",
      "Epoch 77/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.1044 - accuracy: 0.9674 - val_loss: 0.0794 - val_accuracy: 0.9741\n",
      "Epoch 78/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.1051 - accuracy: 0.9662 - val_loss: 0.0603 - val_accuracy: 0.9799\n",
      "Epoch 79/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.1060 - accuracy: 0.9677 - val_loss: 0.0705 - val_accuracy: 0.9770\n",
      "Epoch 80/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.1073 - accuracy: 0.9682 - val_loss: 0.0973 - val_accuracy: 0.9670\n",
      "Epoch 81/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.1065 - accuracy: 0.9658 - val_loss: 0.0584 - val_accuracy: 0.9813\n",
      "Epoch 82/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.1034 - accuracy: 0.9687 - val_loss: 0.0759 - val_accuracy: 0.9741\n",
      "Epoch 83/500\n",
      "7/7 [==============================] - 2s 318ms/step - loss: 0.1037 - accuracy: 0.9676 - val_loss: 0.0631 - val_accuracy: 0.9784\n",
      "Epoch 84/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.1028 - accuracy: 0.9657 - val_loss: 0.0620 - val_accuracy: 0.9784\n",
      "Epoch 85/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.1107 - accuracy: 0.9654 - val_loss: 0.0604 - val_accuracy: 0.9799\n",
      "Epoch 86/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.1075 - accuracy: 0.9673 - val_loss: 0.0900 - val_accuracy: 0.9698\n",
      "Epoch 87/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.1037 - accuracy: 0.9684 - val_loss: 0.0683 - val_accuracy: 0.9756\n",
      "Epoch 88/500\n",
      "7/7 [==============================] - 2s 325ms/step - loss: 0.1078 - accuracy: 0.9665 - val_loss: 0.0672 - val_accuracy: 0.9784\n",
      "Epoch 89/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.1067 - accuracy: 0.9655 - val_loss: 0.0630 - val_accuracy: 0.9784\n",
      "Epoch 90/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0976 - accuracy: 0.9705 - val_loss: 0.0702 - val_accuracy: 0.9756\n",
      "Epoch 91/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0996 - accuracy: 0.9685 - val_loss: 0.0594 - val_accuracy: 0.9813\n",
      "Epoch 92/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0991 - accuracy: 0.9693 - val_loss: 0.0626 - val_accuracy: 0.9813\n",
      "Epoch 93/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0977 - accuracy: 0.9719 - val_loss: 0.0610 - val_accuracy: 0.9770\n",
      "Epoch 94/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0926 - accuracy: 0.9719 - val_loss: 0.0577 - val_accuracy: 0.9799\n",
      "Epoch 95/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0960 - accuracy: 0.9701 - val_loss: 0.0619 - val_accuracy: 0.9799\n",
      "Epoch 96/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0929 - accuracy: 0.9713 - val_loss: 0.0806 - val_accuracy: 0.9770\n",
      "Epoch 97/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0952 - accuracy: 0.9724 - val_loss: 0.0528 - val_accuracy: 0.9813\n",
      "Epoch 98/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0978 - accuracy: 0.9689 - val_loss: 0.0613 - val_accuracy: 0.9799\n",
      "Epoch 99/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.1016 - accuracy: 0.9695 - val_loss: 0.0814 - val_accuracy: 0.9756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0999 - accuracy: 0.9689 - val_loss: 0.0536 - val_accuracy: 0.9842\n",
      "Epoch 101/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0916 - accuracy: 0.9708 - val_loss: 0.0559 - val_accuracy: 0.9828\n",
      "Epoch 102/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0938 - accuracy: 0.9705 - val_loss: 0.0606 - val_accuracy: 0.9856\n",
      "Epoch 103/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.0939 - accuracy: 0.9719 - val_loss: 0.0547 - val_accuracy: 0.9756\n",
      "Epoch 104/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0939 - accuracy: 0.9709 - val_loss: 0.0612 - val_accuracy: 0.9784\n",
      "Epoch 105/500\n",
      "7/7 [==============================] - 2s 325ms/step - loss: 0.0915 - accuracy: 0.9724 - val_loss: 0.0850 - val_accuracy: 0.9713\n",
      "Epoch 106/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.1003 - accuracy: 0.9681 - val_loss: 0.0460 - val_accuracy: 0.9885\n",
      "Epoch 107/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0932 - accuracy: 0.9730 - val_loss: 0.0514 - val_accuracy: 0.9813\n",
      "Epoch 108/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0896 - accuracy: 0.9733 - val_loss: 0.0730 - val_accuracy: 0.9727\n",
      "Epoch 109/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0909 - accuracy: 0.9725 - val_loss: 0.0471 - val_accuracy: 0.9871\n",
      "Epoch 110/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0920 - accuracy: 0.9709 - val_loss: 0.0622 - val_accuracy: 0.9784\n",
      "Epoch 111/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0951 - accuracy: 0.9705 - val_loss: 0.0507 - val_accuracy: 0.9828\n",
      "Epoch 112/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0939 - accuracy: 0.9709 - val_loss: 0.0618 - val_accuracy: 0.9813\n",
      "Epoch 113/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0869 - accuracy: 0.9733 - val_loss: 0.0469 - val_accuracy: 0.9856\n",
      "Epoch 114/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0854 - accuracy: 0.9754 - val_loss: 0.0680 - val_accuracy: 0.9756\n",
      "Epoch 115/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0890 - accuracy: 0.9711 - val_loss: 0.0690 - val_accuracy: 0.9741\n",
      "Epoch 116/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0923 - accuracy: 0.9703 - val_loss: 0.0493 - val_accuracy: 0.9842\n",
      "Epoch 117/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0889 - accuracy: 0.9725 - val_loss: 0.0889 - val_accuracy: 0.9583\n",
      "Epoch 118/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0952 - accuracy: 0.9697 - val_loss: 0.0521 - val_accuracy: 0.9828\n",
      "Epoch 119/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0899 - accuracy: 0.9713 - val_loss: 0.0469 - val_accuracy: 0.9871\n",
      "Epoch 120/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0932 - accuracy: 0.9684 - val_loss: 0.0555 - val_accuracy: 0.9770\n",
      "Epoch 121/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0977 - accuracy: 0.9698 - val_loss: 0.0878 - val_accuracy: 0.9569\n",
      "Epoch 122/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0927 - accuracy: 0.9698 - val_loss: 0.0499 - val_accuracy: 0.9828\n",
      "Epoch 123/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0871 - accuracy: 0.9738 - val_loss: 0.0565 - val_accuracy: 0.9799\n",
      "Epoch 124/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0883 - accuracy: 0.9732 - val_loss: 0.0509 - val_accuracy: 0.9856\n",
      "Epoch 125/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0859 - accuracy: 0.9716 - val_loss: 0.0587 - val_accuracy: 0.9842\n",
      "Epoch 126/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0946 - accuracy: 0.9709 - val_loss: 0.0539 - val_accuracy: 0.9842\n",
      "Epoch 127/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0976 - accuracy: 0.9685 - val_loss: 0.0701 - val_accuracy: 0.9741\n",
      "Epoch 128/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0949 - accuracy: 0.9695 - val_loss: 0.0705 - val_accuracy: 0.9741\n",
      "Epoch 129/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.0944 - accuracy: 0.9685 - val_loss: 0.0619 - val_accuracy: 0.9770\n",
      "Epoch 130/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0896 - accuracy: 0.9714 - val_loss: 0.0505 - val_accuracy: 0.9813\n",
      "Epoch 131/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.0902 - accuracy: 0.9717 - val_loss: 0.0668 - val_accuracy: 0.9756\n",
      "Epoch 132/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0881 - accuracy: 0.9714 - val_loss: 0.0624 - val_accuracy: 0.9770\n",
      "Epoch 133/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0878 - accuracy: 0.9727 - val_loss: 0.0515 - val_accuracy: 0.9828\n",
      "Epoch 134/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0921 - accuracy: 0.9697 - val_loss: 0.0776 - val_accuracy: 0.9670\n",
      "Epoch 135/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0934 - accuracy: 0.9705 - val_loss: 0.0748 - val_accuracy: 0.9727\n",
      "Epoch 136/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0964 - accuracy: 0.9684 - val_loss: 0.0520 - val_accuracy: 0.9871\n",
      "Epoch 137/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0859 - accuracy: 0.9717 - val_loss: 0.0521 - val_accuracy: 0.9842\n",
      "Epoch 138/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0814 - accuracy: 0.9727 - val_loss: 0.0552 - val_accuracy: 0.9828\n",
      "Epoch 139/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0832 - accuracy: 0.9760 - val_loss: 0.0453 - val_accuracy: 0.9856\n",
      "Epoch 140/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0855 - accuracy: 0.9762 - val_loss: 0.0540 - val_accuracy: 0.9813\n",
      "Epoch 141/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0835 - accuracy: 0.9748 - val_loss: 0.0523 - val_accuracy: 0.9828\n",
      "Epoch 142/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.0850 - accuracy: 0.9745 - val_loss: 0.0474 - val_accuracy: 0.9799\n",
      "Epoch 143/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0829 - accuracy: 0.9743 - val_loss: 0.0551 - val_accuracy: 0.9784\n",
      "Epoch 144/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0786 - accuracy: 0.9778 - val_loss: 0.0623 - val_accuracy: 0.9799\n",
      "Epoch 145/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0839 - accuracy: 0.9743 - val_loss: 0.0651 - val_accuracy: 0.9756\n",
      "Epoch 146/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0985 - accuracy: 0.9697 - val_loss: 0.0799 - val_accuracy: 0.9698\n",
      "Epoch 147/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0862 - accuracy: 0.9732 - val_loss: 0.0574 - val_accuracy: 0.9770\n",
      "Epoch 148/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0876 - accuracy: 0.9725 - val_loss: 0.0527 - val_accuracy: 0.9828\n",
      "Epoch 149/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0884 - accuracy: 0.9703 - val_loss: 0.0669 - val_accuracy: 0.9799\n",
      "Epoch 150/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0837 - accuracy: 0.9754 - val_loss: 0.0404 - val_accuracy: 0.9899\n",
      "Epoch 151/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0859 - accuracy: 0.9719 - val_loss: 0.0461 - val_accuracy: 0.9856\n",
      "Epoch 152/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0794 - accuracy: 0.9770 - val_loss: 0.0604 - val_accuracy: 0.9770\n",
      "Epoch 153/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0871 - accuracy: 0.9730 - val_loss: 0.0560 - val_accuracy: 0.9799\n",
      "Epoch 154/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0838 - accuracy: 0.9735 - val_loss: 0.1184 - val_accuracy: 0.9497\n",
      "Epoch 155/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0871 - accuracy: 0.9721 - val_loss: 0.0537 - val_accuracy: 0.9799\n",
      "Epoch 156/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0869 - accuracy: 0.9729 - val_loss: 0.0591 - val_accuracy: 0.9784\n",
      "Epoch 157/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0974 - accuracy: 0.9687 - val_loss: 0.0722 - val_accuracy: 0.9713\n",
      "Epoch 158/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0876 - accuracy: 0.9690 - val_loss: 0.0493 - val_accuracy: 0.9842\n",
      "Epoch 159/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0839 - accuracy: 0.9737 - val_loss: 0.0544 - val_accuracy: 0.9813\n",
      "Epoch 160/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0751 - accuracy: 0.9768 - val_loss: 0.0603 - val_accuracy: 0.9799\n",
      "Epoch 161/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0825 - accuracy: 0.9729 - val_loss: 0.0496 - val_accuracy: 0.9813\n",
      "Epoch 162/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0772 - accuracy: 0.9768 - val_loss: 0.0410 - val_accuracy: 0.9885\n",
      "Epoch 163/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0753 - accuracy: 0.9762 - val_loss: 0.0491 - val_accuracy: 0.9871\n",
      "Epoch 164/500\n",
      "7/7 [==============================] - 2s 345ms/step - loss: 0.0714 - accuracy: 0.9778 - val_loss: 0.0499 - val_accuracy: 0.9856\n",
      "Epoch 165/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0704 - accuracy: 0.9783 - val_loss: 0.0446 - val_accuracy: 0.9828\n",
      "Epoch 166/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0745 - accuracy: 0.9770 - val_loss: 0.0495 - val_accuracy: 0.9813\n",
      "Epoch 167/500\n",
      "7/7 [==============================] - 3s 350ms/step - loss: 0.0770 - accuracy: 0.9767 - val_loss: 0.0508 - val_accuracy: 0.9842\n",
      "Epoch 168/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0801 - accuracy: 0.9756 - val_loss: 0.0387 - val_accuracy: 0.9871\n",
      "Epoch 169/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0819 - accuracy: 0.9749 - val_loss: 0.0640 - val_accuracy: 0.9799\n",
      "Epoch 170/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0742 - accuracy: 0.9764 - val_loss: 0.0487 - val_accuracy: 0.9842\n",
      "Epoch 171/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0812 - accuracy: 0.9732 - val_loss: 0.0415 - val_accuracy: 0.9856\n",
      "Epoch 172/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.0845 - accuracy: 0.9727 - val_loss: 0.0372 - val_accuracy: 0.9899\n",
      "Epoch 173/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.1167 - accuracy: 0.9647 - val_loss: 0.1370 - val_accuracy: 0.9555\n",
      "Epoch 174/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0953 - accuracy: 0.9697 - val_loss: 0.0511 - val_accuracy: 0.9813\n",
      "Epoch 175/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0929 - accuracy: 0.9711 - val_loss: 0.0599 - val_accuracy: 0.9770\n",
      "Epoch 176/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0874 - accuracy: 0.9751 - val_loss: 0.0507 - val_accuracy: 0.9828\n",
      "Epoch 177/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0731 - accuracy: 0.9786 - val_loss: 0.0482 - val_accuracy: 0.9899\n",
      "Epoch 178/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0775 - accuracy: 0.9772 - val_loss: 0.0461 - val_accuracy: 0.9842\n",
      "Epoch 179/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0712 - accuracy: 0.9791 - val_loss: 0.0531 - val_accuracy: 0.9842\n",
      "Epoch 180/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0727 - accuracy: 0.9765 - val_loss: 0.0417 - val_accuracy: 0.9871\n",
      "Epoch 181/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0770 - accuracy: 0.9773 - val_loss: 0.0640 - val_accuracy: 0.9828\n",
      "Epoch 182/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0765 - accuracy: 0.9753 - val_loss: 0.0480 - val_accuracy: 0.9856\n",
      "Epoch 183/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0731 - accuracy: 0.9770 - val_loss: 0.0584 - val_accuracy: 0.9813\n",
      "Epoch 184/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0721 - accuracy: 0.9772 - val_loss: 0.0444 - val_accuracy: 0.9856\n",
      "Epoch 185/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0720 - accuracy: 0.9768 - val_loss: 0.0391 - val_accuracy: 0.9899\n",
      "Epoch 186/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0705 - accuracy: 0.9765 - val_loss: 0.0435 - val_accuracy: 0.9871\n",
      "Epoch 187/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0737 - accuracy: 0.9765 - val_loss: 0.0547 - val_accuracy: 0.9784\n",
      "Epoch 188/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0751 - accuracy: 0.9776 - val_loss: 0.0498 - val_accuracy: 0.9828\n",
      "Epoch 189/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0701 - accuracy: 0.9776 - val_loss: 0.0412 - val_accuracy: 0.9885\n",
      "Epoch 190/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0709 - accuracy: 0.9770 - val_loss: 0.0516 - val_accuracy: 0.9813\n",
      "Epoch 191/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0722 - accuracy: 0.9780 - val_loss: 0.0494 - val_accuracy: 0.9871\n",
      "Epoch 192/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0733 - accuracy: 0.9767 - val_loss: 0.0431 - val_accuracy: 0.9871\n",
      "Epoch 193/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0694 - accuracy: 0.9786 - val_loss: 0.0440 - val_accuracy: 0.9899\n",
      "Epoch 194/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0705 - accuracy: 0.9789 - val_loss: 0.0388 - val_accuracy: 0.9899\n",
      "Epoch 195/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0697 - accuracy: 0.9783 - val_loss: 0.0412 - val_accuracy: 0.9899\n",
      "Epoch 196/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0649 - accuracy: 0.9796 - val_loss: 0.0427 - val_accuracy: 0.9899\n",
      "Epoch 197/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0659 - accuracy: 0.9791 - val_loss: 0.0451 - val_accuracy: 0.9856\n",
      "Epoch 198/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0699 - accuracy: 0.9788 - val_loss: 0.0467 - val_accuracy: 0.9799\n",
      "Epoch 199/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0782 - accuracy: 0.9745 - val_loss: 0.0544 - val_accuracy: 0.9813\n",
      "Epoch 200/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0743 - accuracy: 0.9773 - val_loss: 0.0409 - val_accuracy: 0.9885\n",
      "Epoch 201/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0740 - accuracy: 0.9765 - val_loss: 0.0505 - val_accuracy: 0.9813\n",
      "Epoch 202/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0708 - accuracy: 0.9776 - val_loss: 0.0454 - val_accuracy: 0.9813\n",
      "Epoch 203/500\n",
      "7/7 [==============================] - 2s 325ms/step - loss: 0.0687 - accuracy: 0.9776 - val_loss: 0.0517 - val_accuracy: 0.9856\n",
      "Epoch 204/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0699 - accuracy: 0.9781 - val_loss: 0.0454 - val_accuracy: 0.9899\n",
      "Epoch 205/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0707 - accuracy: 0.9788 - val_loss: 0.0572 - val_accuracy: 0.9741\n",
      "Epoch 206/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0751 - accuracy: 0.9759 - val_loss: 0.0397 - val_accuracy: 0.9871\n",
      "Epoch 207/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0704 - accuracy: 0.9773 - val_loss: 0.0448 - val_accuracy: 0.9856\n",
      "Epoch 208/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0675 - accuracy: 0.9802 - val_loss: 0.0423 - val_accuracy: 0.9885\n",
      "Epoch 209/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0673 - accuracy: 0.9800 - val_loss: 0.0400 - val_accuracy: 0.9885\n",
      "Epoch 210/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0666 - accuracy: 0.9794 - val_loss: 0.0399 - val_accuracy: 0.9885\n",
      "Epoch 211/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0649 - accuracy: 0.9805 - val_loss: 0.0375 - val_accuracy: 0.9885\n",
      "Epoch 212/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0639 - accuracy: 0.9810 - val_loss: 0.0346 - val_accuracy: 0.9914\n",
      "Epoch 213/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0669 - accuracy: 0.9805 - val_loss: 0.0502 - val_accuracy: 0.9813\n",
      "Epoch 214/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 318ms/step - loss: 0.0667 - accuracy: 0.9792 - val_loss: 0.0320 - val_accuracy: 0.9928\n",
      "Epoch 215/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0639 - accuracy: 0.9802 - val_loss: 0.0417 - val_accuracy: 0.9885\n",
      "Epoch 216/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0629 - accuracy: 0.9812 - val_loss: 0.0395 - val_accuracy: 0.9871\n",
      "Epoch 217/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0649 - accuracy: 0.9796 - val_loss: 0.0409 - val_accuracy: 0.9899\n",
      "Epoch 218/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0611 - accuracy: 0.9802 - val_loss: 0.0531 - val_accuracy: 0.9871\n",
      "Epoch 219/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0697 - accuracy: 0.9765 - val_loss: 0.0424 - val_accuracy: 0.9871\n",
      "Epoch 220/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0605 - accuracy: 0.9826 - val_loss: 0.0460 - val_accuracy: 0.9885\n",
      "Epoch 221/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0665 - accuracy: 0.9812 - val_loss: 0.0492 - val_accuracy: 0.9885\n",
      "Epoch 222/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0692 - accuracy: 0.9772 - val_loss: 0.0604 - val_accuracy: 0.9813\n",
      "Epoch 223/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0671 - accuracy: 0.9796 - val_loss: 0.0403 - val_accuracy: 0.9885\n",
      "Epoch 224/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0634 - accuracy: 0.9799 - val_loss: 0.0596 - val_accuracy: 0.9828\n",
      "Epoch 225/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0663 - accuracy: 0.9792 - val_loss: 0.0375 - val_accuracy: 0.9885\n",
      "Epoch 226/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0629 - accuracy: 0.9810 - val_loss: 0.0432 - val_accuracy: 0.9885\n",
      "Epoch 227/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0620 - accuracy: 0.9802 - val_loss: 0.0369 - val_accuracy: 0.9856\n",
      "Epoch 228/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0671 - accuracy: 0.9792 - val_loss: 0.0475 - val_accuracy: 0.9842\n",
      "Epoch 229/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0712 - accuracy: 0.9783 - val_loss: 0.0430 - val_accuracy: 0.9856\n",
      "Epoch 230/500\n",
      "7/7 [==============================] - 2s 330ms/step - loss: 0.0721 - accuracy: 0.9780 - val_loss: 0.0437 - val_accuracy: 0.9885\n",
      "Epoch 231/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0626 - accuracy: 0.9802 - val_loss: 0.0400 - val_accuracy: 0.9842\n",
      "Epoch 232/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0713 - accuracy: 0.9775 - val_loss: 0.0410 - val_accuracy: 0.9885\n",
      "Epoch 233/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0657 - accuracy: 0.9794 - val_loss: 0.0414 - val_accuracy: 0.9856\n",
      "Epoch 234/500\n",
      "7/7 [==============================] - 2s 337ms/step - loss: 0.0703 - accuracy: 0.9767 - val_loss: 0.0390 - val_accuracy: 0.9899\n",
      "Epoch 235/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0804 - accuracy: 0.9735 - val_loss: 0.0668 - val_accuracy: 0.9756\n",
      "Epoch 236/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0701 - accuracy: 0.9781 - val_loss: 0.0493 - val_accuracy: 0.9784\n",
      "Epoch 237/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0673 - accuracy: 0.9773 - val_loss: 0.0478 - val_accuracy: 0.9828\n",
      "Epoch 238/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0692 - accuracy: 0.9781 - val_loss: 0.0368 - val_accuracy: 0.9856\n",
      "Epoch 239/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0642 - accuracy: 0.9797 - val_loss: 0.0453 - val_accuracy: 0.9871\n",
      "Epoch 240/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0627 - accuracy: 0.9812 - val_loss: 0.0628 - val_accuracy: 0.9842\n",
      "Epoch 241/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0744 - accuracy: 0.9768 - val_loss: 0.0435 - val_accuracy: 0.9842\n",
      "Epoch 242/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0829 - accuracy: 0.9735 - val_loss: 0.0428 - val_accuracy: 0.9813\n",
      "Epoch 243/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0723 - accuracy: 0.9770 - val_loss: 0.0576 - val_accuracy: 0.9799\n",
      "Epoch 244/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0708 - accuracy: 0.9759 - val_loss: 0.0428 - val_accuracy: 0.9871\n",
      "Epoch 245/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0759 - accuracy: 0.9764 - val_loss: 0.0761 - val_accuracy: 0.9698\n",
      "Epoch 246/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0761 - accuracy: 0.9756 - val_loss: 0.0567 - val_accuracy: 0.9813\n",
      "Epoch 247/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0668 - accuracy: 0.9799 - val_loss: 0.0494 - val_accuracy: 0.9856\n",
      "Epoch 248/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0691 - accuracy: 0.9788 - val_loss: 0.0475 - val_accuracy: 0.9842\n",
      "Epoch 249/500\n",
      "7/7 [==============================] - 2s 318ms/step - loss: 0.0691 - accuracy: 0.9776 - val_loss: 0.0406 - val_accuracy: 0.9885\n",
      "Epoch 250/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0662 - accuracy: 0.9780 - val_loss: 0.0536 - val_accuracy: 0.9828\n",
      "Epoch 251/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0686 - accuracy: 0.9800 - val_loss: 0.0573 - val_accuracy: 0.9842\n",
      "Epoch 252/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0665 - accuracy: 0.9791 - val_loss: 0.0350 - val_accuracy: 0.9914\n",
      "Epoch 253/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0692 - accuracy: 0.9791 - val_loss: 0.0525 - val_accuracy: 0.9799\n",
      "Epoch 254/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0634 - accuracy: 0.9794 - val_loss: 0.0390 - val_accuracy: 0.9856\n",
      "Epoch 255/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.0614 - accuracy: 0.9820 - val_loss: 0.0491 - val_accuracy: 0.9828\n",
      "Epoch 256/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0598 - accuracy: 0.9804 - val_loss: 0.0425 - val_accuracy: 0.9871\n",
      "Epoch 257/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0618 - accuracy: 0.9813 - val_loss: 0.0392 - val_accuracy: 0.9899\n",
      "Epoch 258/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0665 - accuracy: 0.9778 - val_loss: 0.0455 - val_accuracy: 0.9856\n",
      "Epoch 259/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0683 - accuracy: 0.9788 - val_loss: 0.0429 - val_accuracy: 0.9856\n",
      "Epoch 260/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0766 - accuracy: 0.9757 - val_loss: 0.0480 - val_accuracy: 0.9871\n",
      "Epoch 261/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0591 - accuracy: 0.9831 - val_loss: 0.0381 - val_accuracy: 0.9871\n",
      "Epoch 262/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0622 - accuracy: 0.9816 - val_loss: 0.0431 - val_accuracy: 0.9885\n",
      "Epoch 263/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0653 - accuracy: 0.9813 - val_loss: 0.0443 - val_accuracy: 0.9856\n",
      "Epoch 264/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0636 - accuracy: 0.9802 - val_loss: 0.0584 - val_accuracy: 0.9813\n",
      "Epoch 265/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0662 - accuracy: 0.9791 - val_loss: 0.0456 - val_accuracy: 0.9885\n",
      "Epoch 266/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0637 - accuracy: 0.9796 - val_loss: 0.0379 - val_accuracy: 0.9899\n",
      "Epoch 267/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0625 - accuracy: 0.9807 - val_loss: 0.0458 - val_accuracy: 0.9885\n",
      "Epoch 268/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0570 - accuracy: 0.9837 - val_loss: 0.0411 - val_accuracy: 0.9871\n",
      "Epoch 269/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0639 - accuracy: 0.9800 - val_loss: 0.0511 - val_accuracy: 0.9828\n",
      "Epoch 270/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0678 - accuracy: 0.9781 - val_loss: 0.0339 - val_accuracy: 0.9871\n",
      "Epoch 271/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0597 - accuracy: 0.9832 - val_loss: 0.0403 - val_accuracy: 0.9899\n",
      "Epoch 272/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0576 - accuracy: 0.9834 - val_loss: 0.0453 - val_accuracy: 0.9899\n",
      "Epoch 273/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0587 - accuracy: 0.9810 - val_loss: 0.0352 - val_accuracy: 0.9914\n",
      "Epoch 274/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0579 - accuracy: 0.9840 - val_loss: 0.0408 - val_accuracy: 0.9899\n",
      "Epoch 275/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0575 - accuracy: 0.9823 - val_loss: 0.0454 - val_accuracy: 0.9871\n",
      "Epoch 276/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0681 - accuracy: 0.9797 - val_loss: 0.0364 - val_accuracy: 0.9914\n",
      "Epoch 277/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0642 - accuracy: 0.9808 - val_loss: 0.0455 - val_accuracy: 0.9856\n",
      "Epoch 278/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0558 - accuracy: 0.9836 - val_loss: 0.0406 - val_accuracy: 0.9871\n",
      "Epoch 279/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0592 - accuracy: 0.9813 - val_loss: 0.0375 - val_accuracy: 0.9914\n",
      "Epoch 280/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0555 - accuracy: 0.9834 - val_loss: 0.0656 - val_accuracy: 0.9784\n",
      "Epoch 281/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0616 - accuracy: 0.9816 - val_loss: 0.0420 - val_accuracy: 0.9871\n",
      "Epoch 282/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0700 - accuracy: 0.9781 - val_loss: 0.0465 - val_accuracy: 0.9842\n",
      "Epoch 283/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0647 - accuracy: 0.9807 - val_loss: 0.0379 - val_accuracy: 0.9899\n",
      "Epoch 284/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0599 - accuracy: 0.9807 - val_loss: 0.0389 - val_accuracy: 0.9885\n",
      "Epoch 285/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0601 - accuracy: 0.9810 - val_loss: 0.0403 - val_accuracy: 0.9856\n",
      "Epoch 286/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0622 - accuracy: 0.9807 - val_loss: 0.0387 - val_accuracy: 0.9871\n",
      "Epoch 287/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0655 - accuracy: 0.9797 - val_loss: 0.0716 - val_accuracy: 0.9828\n",
      "Epoch 288/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0734 - accuracy: 0.9765 - val_loss: 0.0396 - val_accuracy: 0.9856\n",
      "Epoch 289/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0620 - accuracy: 0.9804 - val_loss: 0.0545 - val_accuracy: 0.9813\n",
      "Epoch 290/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0630 - accuracy: 0.9815 - val_loss: 0.0397 - val_accuracy: 0.9842\n",
      "Epoch 291/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0596 - accuracy: 0.9820 - val_loss: 0.0569 - val_accuracy: 0.9756\n",
      "Epoch 292/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0671 - accuracy: 0.9794 - val_loss: 0.0537 - val_accuracy: 0.9828\n",
      "Epoch 293/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0607 - accuracy: 0.9804 - val_loss: 0.0413 - val_accuracy: 0.9842\n",
      "Epoch 294/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0664 - accuracy: 0.9783 - val_loss: 0.0518 - val_accuracy: 0.9756\n",
      "Epoch 295/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0665 - accuracy: 0.9773 - val_loss: 0.0554 - val_accuracy: 0.9813\n",
      "Epoch 296/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0606 - accuracy: 0.9828 - val_loss: 0.0518 - val_accuracy: 0.9784\n",
      "Epoch 297/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.0557 - accuracy: 0.9837 - val_loss: 0.0391 - val_accuracy: 0.9885\n",
      "Epoch 298/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0636 - accuracy: 0.9804 - val_loss: 0.0369 - val_accuracy: 0.9914\n",
      "Epoch 299/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0593 - accuracy: 0.9818 - val_loss: 0.0773 - val_accuracy: 0.9713\n",
      "Epoch 300/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0682 - accuracy: 0.9807 - val_loss: 0.0493 - val_accuracy: 0.9828\n",
      "Epoch 301/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0631 - accuracy: 0.9788 - val_loss: 0.0383 - val_accuracy: 0.9885\n",
      "Epoch 302/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0627 - accuracy: 0.9812 - val_loss: 0.0350 - val_accuracy: 0.9914\n",
      "Epoch 303/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0588 - accuracy: 0.9820 - val_loss: 0.0535 - val_accuracy: 0.9856\n",
      "Epoch 304/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0594 - accuracy: 0.9820 - val_loss: 0.0398 - val_accuracy: 0.9899\n",
      "Epoch 305/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0581 - accuracy: 0.9829 - val_loss: 0.0496 - val_accuracy: 0.9856\n",
      "Epoch 306/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0584 - accuracy: 0.9826 - val_loss: 0.0341 - val_accuracy: 0.9899\n",
      "Epoch 307/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0640 - accuracy: 0.9799 - val_loss: 0.0566 - val_accuracy: 0.9799\n",
      "Epoch 308/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0627 - accuracy: 0.9820 - val_loss: 0.0423 - val_accuracy: 0.9842\n",
      "Epoch 309/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0549 - accuracy: 0.9837 - val_loss: 0.0504 - val_accuracy: 0.9799\n",
      "Epoch 310/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.0612 - accuracy: 0.9807 - val_loss: 0.0352 - val_accuracy: 0.9885\n",
      "Epoch 311/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0595 - accuracy: 0.9826 - val_loss: 0.0592 - val_accuracy: 0.9828\n",
      "Epoch 312/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0629 - accuracy: 0.9799 - val_loss: 0.0363 - val_accuracy: 0.9914\n",
      "Epoch 313/500\n",
      "7/7 [==============================] - 2s 336ms/step - loss: 0.0583 - accuracy: 0.9834 - val_loss: 0.0449 - val_accuracy: 0.9856\n",
      "Epoch 314/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0662 - accuracy: 0.9792 - val_loss: 0.0424 - val_accuracy: 0.9856\n",
      "Epoch 315/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0608 - accuracy: 0.9821 - val_loss: 0.0444 - val_accuracy: 0.9842\n",
      "Epoch 316/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0545 - accuracy: 0.9836 - val_loss: 0.0449 - val_accuracy: 0.9856\n",
      "Epoch 317/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0683 - accuracy: 0.9797 - val_loss: 0.0486 - val_accuracy: 0.9871\n",
      "Epoch 318/500\n",
      "7/7 [==============================] - 2s 325ms/step - loss: 0.0583 - accuracy: 0.9824 - val_loss: 0.0381 - val_accuracy: 0.9899\n",
      "Epoch 319/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0526 - accuracy: 0.9856 - val_loss: 0.0406 - val_accuracy: 0.9899\n",
      "Epoch 320/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0606 - accuracy: 0.9834 - val_loss: 0.0417 - val_accuracy: 0.9899\n",
      "Epoch 321/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0605 - accuracy: 0.9812 - val_loss: 0.0382 - val_accuracy: 0.9871\n",
      "Epoch 322/500\n",
      "7/7 [==============================] - 2s 326ms/step - loss: 0.0584 - accuracy: 0.9812 - val_loss: 0.0453 - val_accuracy: 0.9899\n",
      "Epoch 323/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0580 - accuracy: 0.9829 - val_loss: 0.0433 - val_accuracy: 0.9899\n",
      "Epoch 324/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0641 - accuracy: 0.9802 - val_loss: 0.0351 - val_accuracy: 0.9899\n",
      "Epoch 325/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0659 - accuracy: 0.9796 - val_loss: 0.0534 - val_accuracy: 0.9784\n",
      "Epoch 326/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0612 - accuracy: 0.9812 - val_loss: 0.0343 - val_accuracy: 0.9914\n",
      "Epoch 327/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0569 - accuracy: 0.9845 - val_loss: 0.0475 - val_accuracy: 0.9856\n",
      "Epoch 328/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0652 - accuracy: 0.9797 - val_loss: 0.0348 - val_accuracy: 0.9899\n",
      "Epoch 329/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0668 - accuracy: 0.9796 - val_loss: 0.0349 - val_accuracy: 0.9914\n",
      "Epoch 330/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0569 - accuracy: 0.9807 - val_loss: 0.0366 - val_accuracy: 0.9928\n",
      "Epoch 331/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0624 - accuracy: 0.9791 - val_loss: 0.0394 - val_accuracy: 0.9899\n",
      "Epoch 332/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0613 - accuracy: 0.9823 - val_loss: 0.0342 - val_accuracy: 0.9914\n",
      "Epoch 333/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0602 - accuracy: 0.9804 - val_loss: 0.0398 - val_accuracy: 0.9899\n",
      "Epoch 334/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0557 - accuracy: 0.9829 - val_loss: 0.0444 - val_accuracy: 0.9899\n",
      "Epoch 335/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0588 - accuracy: 0.9831 - val_loss: 0.0376 - val_accuracy: 0.9899\n",
      "Epoch 336/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0615 - accuracy: 0.9789 - val_loss: 0.0478 - val_accuracy: 0.9842\n",
      "Epoch 337/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0644 - accuracy: 0.9791 - val_loss: 0.0505 - val_accuracy: 0.9799\n",
      "Epoch 338/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0542 - accuracy: 0.9840 - val_loss: 0.0396 - val_accuracy: 0.9885\n",
      "Epoch 339/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0557 - accuracy: 0.9834 - val_loss: 0.0478 - val_accuracy: 0.9842\n",
      "Epoch 340/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0576 - accuracy: 0.9831 - val_loss: 0.0467 - val_accuracy: 0.9842\n",
      "Epoch 341/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0633 - accuracy: 0.9800 - val_loss: 0.0327 - val_accuracy: 0.9914\n",
      "Epoch 342/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0575 - accuracy: 0.9820 - val_loss: 0.0653 - val_accuracy: 0.9813\n",
      "Epoch 343/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0654 - accuracy: 0.9794 - val_loss: 0.0357 - val_accuracy: 0.9928\n",
      "Epoch 344/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0576 - accuracy: 0.9815 - val_loss: 0.0553 - val_accuracy: 0.9799\n",
      "Epoch 345/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0573 - accuracy: 0.9812 - val_loss: 0.0374 - val_accuracy: 0.9899\n",
      "Epoch 346/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0592 - accuracy: 0.9820 - val_loss: 0.0569 - val_accuracy: 0.9770\n",
      "Epoch 347/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0594 - accuracy: 0.9812 - val_loss: 0.0374 - val_accuracy: 0.9899\n",
      "Epoch 348/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.0599 - accuracy: 0.9812 - val_loss: 0.0525 - val_accuracy: 0.9813\n",
      "Epoch 349/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0555 - accuracy: 0.9832 - val_loss: 0.0359 - val_accuracy: 0.9899\n",
      "Epoch 350/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0602 - accuracy: 0.9821 - val_loss: 0.0554 - val_accuracy: 0.9813\n",
      "Epoch 351/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0595 - accuracy: 0.9813 - val_loss: 0.0365 - val_accuracy: 0.9899\n",
      "Epoch 352/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0575 - accuracy: 0.9829 - val_loss: 0.0387 - val_accuracy: 0.9885\n",
      "Epoch 353/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0516 - accuracy: 0.9848 - val_loss: 0.0417 - val_accuracy: 0.9899\n",
      "Epoch 354/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0533 - accuracy: 0.9850 - val_loss: 0.0434 - val_accuracy: 0.9871\n",
      "Epoch 355/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0553 - accuracy: 0.9829 - val_loss: 0.0400 - val_accuracy: 0.9899\n",
      "Epoch 356/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0613 - accuracy: 0.9813 - val_loss: 0.0477 - val_accuracy: 0.9856\n",
      "Epoch 357/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0577 - accuracy: 0.9828 - val_loss: 0.0372 - val_accuracy: 0.9871\n",
      "Epoch 358/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0522 - accuracy: 0.9837 - val_loss: 0.0483 - val_accuracy: 0.9856\n",
      "Epoch 359/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0571 - accuracy: 0.9836 - val_loss: 0.0363 - val_accuracy: 0.9914\n",
      "Epoch 360/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0556 - accuracy: 0.9820 - val_loss: 0.0406 - val_accuracy: 0.9885\n",
      "Epoch 361/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.0508 - accuracy: 0.9847 - val_loss: 0.0412 - val_accuracy: 0.9871\n",
      "Epoch 362/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0575 - accuracy: 0.9839 - val_loss: 0.0395 - val_accuracy: 0.9885\n",
      "Epoch 363/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0584 - accuracy: 0.9821 - val_loss: 0.0427 - val_accuracy: 0.9885\n",
      "Epoch 364/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0560 - accuracy: 0.9839 - val_loss: 0.0353 - val_accuracy: 0.9928\n",
      "Epoch 365/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0524 - accuracy: 0.9842 - val_loss: 0.0507 - val_accuracy: 0.9856\n",
      "Epoch 366/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.0509 - accuracy: 0.9855 - val_loss: 0.0362 - val_accuracy: 0.9914\n",
      "Epoch 367/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0518 - accuracy: 0.9848 - val_loss: 0.0514 - val_accuracy: 0.9828\n",
      "Epoch 368/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0538 - accuracy: 0.9842 - val_loss: 0.0326 - val_accuracy: 0.9914\n",
      "Epoch 369/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0529 - accuracy: 0.9836 - val_loss: 0.0458 - val_accuracy: 0.9899\n",
      "Epoch 370/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0547 - accuracy: 0.9828 - val_loss: 0.0403 - val_accuracy: 0.9885\n",
      "Epoch 371/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0573 - accuracy: 0.9826 - val_loss: 0.0469 - val_accuracy: 0.9871\n",
      "Epoch 372/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0545 - accuracy: 0.9850 - val_loss: 0.0392 - val_accuracy: 0.9885\n",
      "Epoch 373/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0516 - accuracy: 0.9856 - val_loss: 0.0558 - val_accuracy: 0.9813\n",
      "Epoch 374/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0505 - accuracy: 0.9845 - val_loss: 0.0324 - val_accuracy: 0.9899\n",
      "Epoch 375/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0547 - accuracy: 0.9844 - val_loss: 0.0376 - val_accuracy: 0.9899\n",
      "Epoch 376/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0556 - accuracy: 0.9826 - val_loss: 0.0403 - val_accuracy: 0.9885\n",
      "Epoch 377/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0511 - accuracy: 0.9847 - val_loss: 0.0397 - val_accuracy: 0.9899\n",
      "Epoch 378/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0519 - accuracy: 0.9847 - val_loss: 0.0350 - val_accuracy: 0.9885\n",
      "Epoch 379/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0506 - accuracy: 0.9844 - val_loss: 0.0412 - val_accuracy: 0.9885\n",
      "Epoch 380/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0597 - accuracy: 0.9820 - val_loss: 0.0395 - val_accuracy: 0.9871\n",
      "Epoch 381/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0572 - accuracy: 0.9826 - val_loss: 0.0470 - val_accuracy: 0.9871\n",
      "Epoch 382/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0527 - accuracy: 0.9837 - val_loss: 0.0454 - val_accuracy: 0.9885\n",
      "Epoch 383/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0515 - accuracy: 0.9844 - val_loss: 0.0363 - val_accuracy: 0.9885\n",
      "Epoch 384/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0551 - accuracy: 0.9824 - val_loss: 0.0461 - val_accuracy: 0.9813\n",
      "Epoch 385/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0485 - accuracy: 0.9855 - val_loss: 0.0385 - val_accuracy: 0.9871\n",
      "Epoch 386/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0501 - accuracy: 0.9844 - val_loss: 0.0411 - val_accuracy: 0.9871\n",
      "Epoch 387/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0545 - accuracy: 0.9826 - val_loss: 0.0417 - val_accuracy: 0.9871\n",
      "Epoch 388/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0582 - accuracy: 0.9840 - val_loss: 0.0423 - val_accuracy: 0.9885\n",
      "Epoch 389/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0536 - accuracy: 0.9845 - val_loss: 0.0515 - val_accuracy: 0.9813\n",
      "Epoch 390/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0612 - accuracy: 0.9813 - val_loss: 0.0389 - val_accuracy: 0.9885\n",
      "Epoch 391/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0549 - accuracy: 0.9842 - val_loss: 0.0567 - val_accuracy: 0.9813\n",
      "Epoch 392/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0523 - accuracy: 0.9848 - val_loss: 0.0424 - val_accuracy: 0.9899\n",
      "Epoch 393/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0551 - accuracy: 0.9829 - val_loss: 0.0570 - val_accuracy: 0.9741\n",
      "Epoch 394/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0571 - accuracy: 0.9815 - val_loss: 0.0415 - val_accuracy: 0.9885\n",
      "Epoch 395/500\n",
      "7/7 [==============================] - 2s 340ms/step - loss: 0.0535 - accuracy: 0.9848 - val_loss: 0.0358 - val_accuracy: 0.9914\n",
      "Epoch 396/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.0470 - accuracy: 0.9866 - val_loss: 0.0341 - val_accuracy: 0.9899\n",
      "Epoch 397/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0515 - accuracy: 0.9840 - val_loss: 0.0405 - val_accuracy: 0.9899\n",
      "Epoch 398/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0467 - accuracy: 0.9859 - val_loss: 0.0330 - val_accuracy: 0.9928\n",
      "Epoch 399/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0481 - accuracy: 0.9855 - val_loss: 0.0458 - val_accuracy: 0.9885\n",
      "Epoch 400/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0528 - accuracy: 0.9837 - val_loss: 0.0395 - val_accuracy: 0.9899\n",
      "Epoch 401/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0549 - accuracy: 0.9828 - val_loss: 0.0357 - val_accuracy: 0.9914\n",
      "Epoch 402/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0540 - accuracy: 0.9834 - val_loss: 0.0509 - val_accuracy: 0.9799\n",
      "Epoch 403/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0579 - accuracy: 0.9826 - val_loss: 0.0402 - val_accuracy: 0.9914\n",
      "Epoch 404/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0582 - accuracy: 0.9826 - val_loss: 0.0377 - val_accuracy: 0.9899\n",
      "Epoch 405/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0521 - accuracy: 0.9842 - val_loss: 0.0449 - val_accuracy: 0.9842\n",
      "Epoch 406/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0525 - accuracy: 0.9832 - val_loss: 0.0389 - val_accuracy: 0.9871\n",
      "Epoch 407/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0494 - accuracy: 0.9863 - val_loss: 0.0459 - val_accuracy: 0.9842\n",
      "Epoch 408/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0509 - accuracy: 0.9842 - val_loss: 0.0425 - val_accuracy: 0.9885\n",
      "Epoch 409/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0492 - accuracy: 0.9850 - val_loss: 0.0332 - val_accuracy: 0.9928\n",
      "Epoch 410/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0488 - accuracy: 0.9842 - val_loss: 0.0578 - val_accuracy: 0.9784\n",
      "Epoch 411/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0585 - accuracy: 0.9818 - val_loss: 0.0296 - val_accuracy: 0.9899\n",
      "Epoch 412/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0592 - accuracy: 0.9818 - val_loss: 0.0596 - val_accuracy: 0.9799\n",
      "Epoch 413/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0537 - accuracy: 0.9832 - val_loss: 0.0320 - val_accuracy: 0.9928\n",
      "Epoch 414/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0497 - accuracy: 0.9853 - val_loss: 0.0310 - val_accuracy: 0.9914\n",
      "Epoch 415/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0555 - accuracy: 0.9834 - val_loss: 0.0444 - val_accuracy: 0.9842\n",
      "Epoch 416/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0521 - accuracy: 0.9836 - val_loss: 0.0367 - val_accuracy: 0.9871\n",
      "Epoch 417/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0535 - accuracy: 0.9852 - val_loss: 0.0526 - val_accuracy: 0.9842\n",
      "Epoch 418/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0550 - accuracy: 0.9828 - val_loss: 0.0433 - val_accuracy: 0.9856\n",
      "Epoch 419/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0495 - accuracy: 0.9844 - val_loss: 0.0418 - val_accuracy: 0.9856\n",
      "Epoch 420/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0533 - accuracy: 0.9845 - val_loss: 0.0455 - val_accuracy: 0.9842\n",
      "Epoch 421/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0506 - accuracy: 0.9850 - val_loss: 0.0340 - val_accuracy: 0.9899\n",
      "Epoch 422/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0462 - accuracy: 0.9867 - val_loss: 0.0371 - val_accuracy: 0.9871\n",
      "Epoch 423/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0539 - accuracy: 0.9840 - val_loss: 0.0440 - val_accuracy: 0.9871\n",
      "Epoch 424/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0504 - accuracy: 0.9842 - val_loss: 0.0388 - val_accuracy: 0.9871\n",
      "Epoch 425/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0563 - accuracy: 0.9829 - val_loss: 0.0431 - val_accuracy: 0.9856\n",
      "Epoch 426/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.0499 - accuracy: 0.9855 - val_loss: 0.0374 - val_accuracy: 0.9885\n",
      "Epoch 427/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0545 - accuracy: 0.9832 - val_loss: 0.0688 - val_accuracy: 0.9756\n",
      "Epoch 428/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0581 - accuracy: 0.9823 - val_loss: 0.0332 - val_accuracy: 0.9899\n",
      "Epoch 429/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0498 - accuracy: 0.9855 - val_loss: 0.0610 - val_accuracy: 0.9770\n",
      "Epoch 430/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0531 - accuracy: 0.9842 - val_loss: 0.0321 - val_accuracy: 0.9914\n",
      "Epoch 431/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0501 - accuracy: 0.9848 - val_loss: 0.0466 - val_accuracy: 0.9856\n",
      "Epoch 432/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0514 - accuracy: 0.9834 - val_loss: 0.0467 - val_accuracy: 0.9885\n",
      "Epoch 433/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0535 - accuracy: 0.9842 - val_loss: 0.0409 - val_accuracy: 0.9871\n",
      "Epoch 434/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0504 - accuracy: 0.9853 - val_loss: 0.0400 - val_accuracy: 0.9899\n",
      "Epoch 435/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0501 - accuracy: 0.9861 - val_loss: 0.0420 - val_accuracy: 0.9885\n",
      "Epoch 436/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0527 - accuracy: 0.9845 - val_loss: 0.0446 - val_accuracy: 0.9871\n",
      "Epoch 437/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0509 - accuracy: 0.9853 - val_loss: 0.0351 - val_accuracy: 0.9914\n",
      "Epoch 438/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0485 - accuracy: 0.9852 - val_loss: 0.0539 - val_accuracy: 0.9856\n",
      "Epoch 439/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0541 - accuracy: 0.9839 - val_loss: 0.0480 - val_accuracy: 0.9856\n",
      "Epoch 440/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0545 - accuracy: 0.9845 - val_loss: 0.0419 - val_accuracy: 0.9856\n",
      "Epoch 441/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0658 - accuracy: 0.9789 - val_loss: 0.0382 - val_accuracy: 0.9871\n",
      "Epoch 442/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0614 - accuracy: 0.9805 - val_loss: 0.0602 - val_accuracy: 0.9784\n",
      "Epoch 443/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0610 - accuracy: 0.9812 - val_loss: 0.0516 - val_accuracy: 0.9813\n",
      "Epoch 444/500\n",
      "7/7 [==============================] - 2s 325ms/step - loss: 0.0671 - accuracy: 0.9796 - val_loss: 0.0789 - val_accuracy: 0.9713\n",
      "Epoch 445/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0784 - accuracy: 0.9767 - val_loss: 0.0400 - val_accuracy: 0.9842\n",
      "Epoch 446/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0558 - accuracy: 0.9840 - val_loss: 0.0431 - val_accuracy: 0.9871\n",
      "Epoch 447/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0483 - accuracy: 0.9850 - val_loss: 0.0412 - val_accuracy: 0.9828\n",
      "Epoch 448/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0494 - accuracy: 0.9850 - val_loss: 0.0481 - val_accuracy: 0.9842\n",
      "Epoch 449/500\n",
      "7/7 [==============================] - 2s 324ms/step - loss: 0.0520 - accuracy: 0.9845 - val_loss: 0.0404 - val_accuracy: 0.9871\n",
      "Epoch 450/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0478 - accuracy: 0.9856 - val_loss: 0.0418 - val_accuracy: 0.9899\n",
      "Epoch 451/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0512 - accuracy: 0.9852 - val_loss: 0.0535 - val_accuracy: 0.9842\n",
      "Epoch 452/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0572 - accuracy: 0.9824 - val_loss: 0.0317 - val_accuracy: 0.9914\n",
      "Epoch 453/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0546 - accuracy: 0.9844 - val_loss: 0.0459 - val_accuracy: 0.9871\n",
      "Epoch 454/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0469 - accuracy: 0.9856 - val_loss: 0.0410 - val_accuracy: 0.9899\n",
      "Epoch 455/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0546 - accuracy: 0.9837 - val_loss: 0.0513 - val_accuracy: 0.9828\n",
      "Epoch 456/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0560 - accuracy: 0.9828 - val_loss: 0.0368 - val_accuracy: 0.9899\n",
      "Epoch 457/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0452 - accuracy: 0.9867 - val_loss: 0.0365 - val_accuracy: 0.9914\n",
      "Epoch 458/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0514 - accuracy: 0.9850 - val_loss: 0.0447 - val_accuracy: 0.9885\n",
      "Epoch 459/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0500 - accuracy: 0.9844 - val_loss: 0.0372 - val_accuracy: 0.9871\n",
      "Epoch 460/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0507 - accuracy: 0.9852 - val_loss: 0.0498 - val_accuracy: 0.9842\n",
      "Epoch 461/500\n",
      "7/7 [==============================] - 2s 318ms/step - loss: 0.0563 - accuracy: 0.9834 - val_loss: 0.0446 - val_accuracy: 0.9871\n",
      "Epoch 462/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0463 - accuracy: 0.9863 - val_loss: 0.0506 - val_accuracy: 0.9813\n",
      "Epoch 463/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0506 - accuracy: 0.9844 - val_loss: 0.0349 - val_accuracy: 0.9871\n",
      "Epoch 464/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0585 - accuracy: 0.9818 - val_loss: 0.0599 - val_accuracy: 0.9799\n",
      "Epoch 465/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0510 - accuracy: 0.9829 - val_loss: 0.0327 - val_accuracy: 0.9899\n",
      "Epoch 466/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0457 - accuracy: 0.9852 - val_loss: 0.0396 - val_accuracy: 0.9871\n",
      "Epoch 467/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0475 - accuracy: 0.9867 - val_loss: 0.0376 - val_accuracy: 0.9871\n",
      "Epoch 468/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0489 - accuracy: 0.9858 - val_loss: 0.0435 - val_accuracy: 0.9885\n",
      "Epoch 469/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0520 - accuracy: 0.9844 - val_loss: 0.0378 - val_accuracy: 0.9885\n",
      "Epoch 470/500\n",
      "7/7 [==============================] - 2s 339ms/step - loss: 0.0493 - accuracy: 0.9858 - val_loss: 0.0406 - val_accuracy: 0.9885\n",
      "Epoch 471/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0536 - accuracy: 0.9823 - val_loss: 0.0376 - val_accuracy: 0.9914\n",
      "Epoch 472/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0543 - accuracy: 0.9823 - val_loss: 0.0461 - val_accuracy: 0.9828\n",
      "Epoch 473/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0472 - accuracy: 0.9852 - val_loss: 0.0436 - val_accuracy: 0.9899\n",
      "Epoch 474/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0491 - accuracy: 0.9852 - val_loss: 0.0588 - val_accuracy: 0.9784\n",
      "Epoch 475/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0553 - accuracy: 0.9824 - val_loss: 0.0377 - val_accuracy: 0.9871\n",
      "Epoch 476/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0552 - accuracy: 0.9836 - val_loss: 0.0405 - val_accuracy: 0.9871\n",
      "Epoch 477/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0568 - accuracy: 0.9837 - val_loss: 0.0824 - val_accuracy: 0.9756\n",
      "Epoch 478/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0537 - accuracy: 0.9824 - val_loss: 0.0328 - val_accuracy: 0.9914\n",
      "Epoch 479/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0532 - accuracy: 0.9840 - val_loss: 0.0566 - val_accuracy: 0.9813\n",
      "Epoch 480/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0497 - accuracy: 0.9856 - val_loss: 0.0362 - val_accuracy: 0.9899\n",
      "Epoch 481/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0535 - accuracy: 0.9836 - val_loss: 0.0486 - val_accuracy: 0.9813\n",
      "Epoch 482/500\n",
      "7/7 [==============================] - 2s 318ms/step - loss: 0.0516 - accuracy: 0.9837 - val_loss: 0.0369 - val_accuracy: 0.9899\n",
      "Epoch 483/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0498 - accuracy: 0.9845 - val_loss: 0.0408 - val_accuracy: 0.9842\n",
      "Epoch 484/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0480 - accuracy: 0.9861 - val_loss: 0.0396 - val_accuracy: 0.9871\n",
      "Epoch 485/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0457 - accuracy: 0.9863 - val_loss: 0.0366 - val_accuracy: 0.9885\n",
      "Epoch 486/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0476 - accuracy: 0.9852 - val_loss: 0.0568 - val_accuracy: 0.9770\n",
      "Epoch 487/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0506 - accuracy: 0.9847 - val_loss: 0.0364 - val_accuracy: 0.9885\n",
      "Epoch 488/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0518 - accuracy: 0.9842 - val_loss: 0.0583 - val_accuracy: 0.9741\n",
      "Epoch 489/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0561 - accuracy: 0.9816 - val_loss: 0.0343 - val_accuracy: 0.9899\n",
      "Epoch 490/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0524 - accuracy: 0.9828 - val_loss: 0.0604 - val_accuracy: 0.9784\n",
      "Epoch 491/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0508 - accuracy: 0.9850 - val_loss: 0.0338 - val_accuracy: 0.9871\n",
      "Epoch 492/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0480 - accuracy: 0.9858 - val_loss: 0.0407 - val_accuracy: 0.9856\n",
      "Epoch 493/500\n",
      "7/7 [==============================] - 2s 330ms/step - loss: 0.0476 - accuracy: 0.9856 - val_loss: 0.0374 - val_accuracy: 0.9856\n",
      "Epoch 494/500\n",
      "7/7 [==============================] - 2s 322ms/step - loss: 0.0481 - accuracy: 0.9855 - val_loss: 0.0393 - val_accuracy: 0.9828\n",
      "Epoch 495/500\n",
      "7/7 [==============================] - 2s 321ms/step - loss: 0.0443 - accuracy: 0.9855 - val_loss: 0.0432 - val_accuracy: 0.9871\n",
      "Epoch 496/500\n",
      "7/7 [==============================] - 2s 320ms/step - loss: 0.0565 - accuracy: 0.9832 - val_loss: 0.0439 - val_accuracy: 0.9828\n",
      "Epoch 497/500\n",
      "7/7 [==============================] - 2s 326ms/step - loss: 0.0493 - accuracy: 0.9831 - val_loss: 0.0468 - val_accuracy: 0.9871\n",
      "Epoch 498/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0498 - accuracy: 0.9850 - val_loss: 0.0386 - val_accuracy: 0.9885\n",
      "Epoch 499/500\n",
      "7/7 [==============================] - 2s 323ms/step - loss: 0.0453 - accuracy: 0.9863 - val_loss: 0.0380 - val_accuracy: 0.9899\n",
      "Epoch 500/500\n",
      "7/7 [==============================] - 2s 319ms/step - loss: 0.0466 - accuracy: 0.9859 - val_loss: 0.0377 - val_accuracy: 0.9871\n"
     ]
    }
   ],
   "source": [
    "X_mask = np.ones((len(X_train),14))\n",
    "history = model.fit([X_train, X_mask], y_train, batch_size=1024, epochs=500, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training - Loss Function')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.subplot()\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Training - Loss Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (train with whole training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model_1/transformer/mask_emb:0', 'tfxl_net_model_1/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model_1/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model_1/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model_1/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model_1/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model_1/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model_1/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model_1/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model_1/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model_1/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model_1/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model_1/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model_1/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model_1/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model_1/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model_1/transformer/layer_._7/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model_1/transformer/mask_emb:0', 'tfxl_net_model_1/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model_1/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model_1/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model_1/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model_1/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model_1/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model_1/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model_1/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model_1/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model_1/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model_1/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model_1/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model_1/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model_1/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model_1/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model_1/transformer/layer_._7/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "7/7 [==============================] - 9s 349ms/step - loss: 1.5853 - accuracy: 0.3071\n",
      "Epoch 2/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 1.4554 - accuracy: 0.4065\n",
      "Epoch 3/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 1.3830 - accuracy: 0.4567\n",
      "Epoch 4/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 1.1940 - accuracy: 0.5925\n",
      "Epoch 5/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.9711 - accuracy: 0.6416\n",
      "Epoch 6/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.8476 - accuracy: 0.6725\n",
      "Epoch 7/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.7625 - accuracy: 0.7056\n",
      "Epoch 8/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.6878 - accuracy: 0.7195\n",
      "Epoch 9/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.6437 - accuracy: 0.7300\n",
      "Epoch 10/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.5998 - accuracy: 0.7400\n",
      "Epoch 11/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.5644 - accuracy: 0.7461\n",
      "Epoch 12/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.5395 - accuracy: 0.7544\n",
      "Epoch 13/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.5179 - accuracy: 0.7612\n",
      "Epoch 14/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.5093 - accuracy: 0.7554\n",
      "Epoch 15/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.4980 - accuracy: 0.7599\n",
      "Epoch 16/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.4888 - accuracy: 0.7570\n",
      "Epoch 17/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.4829 - accuracy: 0.7707\n",
      "Epoch 18/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.4759 - accuracy: 0.7708\n",
      "Epoch 19/500\n",
      "7/7 [==============================] - 3s 349ms/step - loss: 0.4607 - accuracy: 0.7823\n",
      "Epoch 20/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.4526 - accuracy: 0.7911\n",
      "Epoch 21/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.4284 - accuracy: 0.8033\n",
      "Epoch 22/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.4007 - accuracy: 0.8230\n",
      "Epoch 23/500\n",
      "7/7 [==============================] - 2s 355ms/step - loss: 0.3785 - accuracy: 0.8319\n",
      "Epoch 24/500\n",
      "7/7 [==============================] - 3s 368ms/step - loss: 0.3512 - accuracy: 0.8484\n",
      "Epoch 25/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.3383 - accuracy: 0.8593\n",
      "Epoch 26/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.3225 - accuracy: 0.8689\n",
      "Epoch 27/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.3093 - accuracy: 0.8776\n",
      "Epoch 28/500\n",
      "7/7 [==============================] - 2s 356ms/step - loss: 0.2878 - accuracy: 0.8942\n",
      "Epoch 29/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.2621 - accuracy: 0.9090\n",
      "Epoch 30/500\n",
      "7/7 [==============================] - 2s 352ms/step - loss: 0.2468 - accuracy: 0.9156\n",
      "Epoch 31/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.2246 - accuracy: 0.9230\n",
      "Epoch 32/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.2138 - accuracy: 0.9335\n",
      "Epoch 33/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.1959 - accuracy: 0.9349\n",
      "Epoch 34/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.1874 - accuracy: 0.9365\n",
      "Epoch 35/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.1746 - accuracy: 0.9417\n",
      "Epoch 36/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.1808 - accuracy: 0.9365\n",
      "Epoch 37/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.1661 - accuracy: 0.9454\n",
      "Epoch 38/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.1559 - accuracy: 0.9471\n",
      "Epoch 39/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.1540 - accuracy: 0.9504\n",
      "Epoch 40/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.1497 - accuracy: 0.9507\n",
      "Epoch 41/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.1443 - accuracy: 0.9532\n",
      "Epoch 42/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.1416 - accuracy: 0.9543\n",
      "Epoch 43/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.1418 - accuracy: 0.9543\n",
      "Epoch 44/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.1401 - accuracy: 0.9526\n",
      "Epoch 45/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.1368 - accuracy: 0.9530\n",
      "Epoch 46/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.1312 - accuracy: 0.9589\n",
      "Epoch 47/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.1298 - accuracy: 0.9559\n",
      "Epoch 48/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.1263 - accuracy: 0.9570\n",
      "Epoch 49/500\n",
      "7/7 [==============================] - 3s 373ms/step - loss: 0.1247 - accuracy: 0.9603\n",
      "Epoch 50/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.1279 - accuracy: 0.9560\n",
      "Epoch 51/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.1269 - accuracy: 0.9582\n",
      "Epoch 52/500\n",
      "7/7 [==============================] - 3s 374ms/step - loss: 0.1251 - accuracy: 0.9553\n",
      "Epoch 53/500\n",
      "7/7 [==============================] - 3s 369ms/step - loss: 0.1194 - accuracy: 0.9606\n",
      "Epoch 54/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.1158 - accuracy: 0.9616\n",
      "Epoch 55/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.1167 - accuracy: 0.9611\n",
      "Epoch 56/500\n",
      "7/7 [==============================] - 2s 345ms/step - loss: 0.1174 - accuracy: 0.9616\n",
      "Epoch 57/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.1206 - accuracy: 0.9598\n",
      "Epoch 58/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.1147 - accuracy: 0.9613\n",
      "Epoch 59/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.1178 - accuracy: 0.9609\n",
      "Epoch 60/500\n",
      "7/7 [==============================] - 3s 359ms/step - loss: 0.1103 - accuracy: 0.9649\n",
      "Epoch 61/500\n",
      "7/7 [==============================] - 3s 368ms/step - loss: 0.1147 - accuracy: 0.9618\n",
      "Epoch 62/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.1144 - accuracy: 0.9622\n",
      "Epoch 63/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.1088 - accuracy: 0.9628\n",
      "Epoch 64/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.1054 - accuracy: 0.9648\n",
      "Epoch 65/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.1057 - accuracy: 0.9645\n",
      "Epoch 66/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.1035 - accuracy: 0.9648\n",
      "Epoch 67/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.1037 - accuracy: 0.9651\n",
      "Epoch 68/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.1068 - accuracy: 0.9648\n",
      "Epoch 69/500\n",
      "7/7 [==============================] - 3s 371ms/step - loss: 0.1058 - accuracy: 0.9654\n",
      "Epoch 70/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.1089 - accuracy: 0.9626\n",
      "Epoch 71/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.1031 - accuracy: 0.9661\n",
      "Epoch 72/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.1052 - accuracy: 0.9657\n",
      "Epoch 73/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.1052 - accuracy: 0.9658\n",
      "Epoch 74/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.1024 - accuracy: 0.9672\n",
      "Epoch 75/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0995 - accuracy: 0.9672\n",
      "Epoch 76/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.1032 - accuracy: 0.9658\n",
      "Epoch 77/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.1015 - accuracy: 0.9671\n",
      "Epoch 78/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0962 - accuracy: 0.9672\n",
      "Epoch 79/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0978 - accuracy: 0.9681\n",
      "Epoch 80/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0947 - accuracy: 0.9698\n",
      "Epoch 81/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0955 - accuracy: 0.9687\n",
      "Epoch 82/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0939 - accuracy: 0.9672\n",
      "Epoch 83/500\n",
      "7/7 [==============================] - 3s 351ms/step - loss: 0.0998 - accuracy: 0.9667\n",
      "Epoch 84/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0991 - accuracy: 0.9678\n",
      "Epoch 85/500\n",
      "7/7 [==============================] - 2s 353ms/step - loss: 0.0958 - accuracy: 0.9677\n",
      "Epoch 86/500\n",
      "7/7 [==============================] - 2s 353ms/step - loss: 0.1023 - accuracy: 0.9661\n",
      "Epoch 87/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0994 - accuracy: 0.9667\n",
      "Epoch 88/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0939 - accuracy: 0.9684\n",
      "Epoch 89/500\n",
      "7/7 [==============================] - 3s 380ms/step - loss: 0.0921 - accuracy: 0.9692\n",
      "Epoch 90/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0911 - accuracy: 0.9688\n",
      "Epoch 91/500\n",
      "7/7 [==============================] - 3s 356ms/step - loss: 0.0954 - accuracy: 0.9688\n",
      "Epoch 92/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0950 - accuracy: 0.9698\n",
      "Epoch 93/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0879 - accuracy: 0.9715\n",
      "Epoch 94/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0911 - accuracy: 0.9698\n",
      "Epoch 95/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0871 - accuracy: 0.9695\n",
      "Epoch 96/500\n",
      "7/7 [==============================] - 2s 355ms/step - loss: 0.0828 - accuracy: 0.9737\n",
      "Epoch 97/500\n",
      "7/7 [==============================] - 2s 354ms/step - loss: 0.0902 - accuracy: 0.9694\n",
      "Epoch 98/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0865 - accuracy: 0.9720\n",
      "Epoch 99/500\n",
      "7/7 [==============================] - 3s 366ms/step - loss: 0.0856 - accuracy: 0.9727\n",
      "Epoch 100/500\n",
      "7/7 [==============================] - 2s 355ms/step - loss: 0.0882 - accuracy: 0.9711\n",
      "Epoch 101/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0850 - accuracy: 0.9718\n",
      "Epoch 102/500\n",
      "7/7 [==============================] - 3s 377ms/step - loss: 0.0885 - accuracy: 0.9703\n",
      "Epoch 103/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0855 - accuracy: 0.9741\n",
      "Epoch 104/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0813 - accuracy: 0.9756\n",
      "Epoch 105/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0868 - accuracy: 0.9694\n",
      "Epoch 106/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0882 - accuracy: 0.9687\n",
      "Epoch 107/500\n",
      "7/7 [==============================] - 2s 352ms/step - loss: 0.0826 - accuracy: 0.9726\n",
      "Epoch 108/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0907 - accuracy: 0.9698\n",
      "Epoch 109/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0926 - accuracy: 0.9681\n",
      "Epoch 110/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0889 - accuracy: 0.9698\n",
      "Epoch 111/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0835 - accuracy: 0.9711\n",
      "Epoch 112/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0837 - accuracy: 0.9738\n",
      "Epoch 113/500\n",
      "7/7 [==============================] - 3s 365ms/step - loss: 0.0820 - accuracy: 0.9744\n",
      "Epoch 114/500\n",
      "7/7 [==============================] - 3s 358ms/step - loss: 0.0780 - accuracy: 0.9749\n",
      "Epoch 115/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0823 - accuracy: 0.9738\n",
      "Epoch 116/500\n",
      "7/7 [==============================] - 3s 353ms/step - loss: 0.0804 - accuracy: 0.9728\n",
      "Epoch 117/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0819 - accuracy: 0.9723\n",
      "Epoch 118/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0759 - accuracy: 0.9749\n",
      "Epoch 119/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0754 - accuracy: 0.9750\n",
      "Epoch 120/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0789 - accuracy: 0.9753\n",
      "Epoch 121/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0811 - accuracy: 0.9726\n",
      "Epoch 122/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0852 - accuracy: 0.9734\n",
      "Epoch 123/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0796 - accuracy: 0.9731\n",
      "Epoch 124/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0886 - accuracy: 0.9713\n",
      "Epoch 125/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0826 - accuracy: 0.9700\n",
      "Epoch 126/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0832 - accuracy: 0.9741\n",
      "Epoch 127/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0745 - accuracy: 0.9769\n",
      "Epoch 128/500\n",
      "7/7 [==============================] - 2s 352ms/step - loss: 0.0793 - accuracy: 0.9726\n",
      "Epoch 129/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0757 - accuracy: 0.9750\n",
      "Epoch 130/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0745 - accuracy: 0.9754\n",
      "Epoch 131/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0711 - accuracy: 0.9769\n",
      "Epoch 132/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0742 - accuracy: 0.9761\n",
      "Epoch 133/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0764 - accuracy: 0.9743\n",
      "Epoch 134/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0701 - accuracy: 0.9766\n",
      "Epoch 135/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0745 - accuracy: 0.9759\n",
      "Epoch 136/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0772 - accuracy: 0.9738\n",
      "Epoch 137/500\n",
      "7/7 [==============================] - 3s 375ms/step - loss: 0.0761 - accuracy: 0.9754\n",
      "Epoch 138/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0757 - accuracy: 0.9740\n",
      "Epoch 139/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0728 - accuracy: 0.9773\n",
      "Epoch 140/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0729 - accuracy: 0.9783\n",
      "Epoch 141/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0690 - accuracy: 0.9783\n",
      "Epoch 142/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0693 - accuracy: 0.9792\n",
      "Epoch 143/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0725 - accuracy: 0.9769\n",
      "Epoch 144/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0724 - accuracy: 0.9769\n",
      "Epoch 145/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0710 - accuracy: 0.9754\n",
      "Epoch 146/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0716 - accuracy: 0.9777\n",
      "Epoch 147/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0689 - accuracy: 0.9770\n",
      "Epoch 148/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0671 - accuracy: 0.9796\n",
      "Epoch 149/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0674 - accuracy: 0.9787\n",
      "Epoch 150/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0665 - accuracy: 0.9802\n",
      "Epoch 151/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0684 - accuracy: 0.9780\n",
      "Epoch 152/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0691 - accuracy: 0.9772\n",
      "Epoch 153/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0682 - accuracy: 0.9787\n",
      "Epoch 154/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0728 - accuracy: 0.9774\n",
      "Epoch 155/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0734 - accuracy: 0.9764\n",
      "Epoch 156/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0698 - accuracy: 0.9782\n",
      "Epoch 157/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0703 - accuracy: 0.9767\n",
      "Epoch 158/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0721 - accuracy: 0.9773\n",
      "Epoch 159/500\n",
      "7/7 [==============================] - 2s 353ms/step - loss: 0.0738 - accuracy: 0.9769\n",
      "Epoch 160/500\n",
      "7/7 [==============================] - 3s 350ms/step - loss: 0.0753 - accuracy: 0.9759\n",
      "Epoch 161/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0701 - accuracy: 0.9770\n",
      "Epoch 162/500\n",
      "7/7 [==============================] - 2s 353ms/step - loss: 0.0688 - accuracy: 0.9795\n",
      "Epoch 163/500\n",
      "7/7 [==============================] - 2s 352ms/step - loss: 0.0706 - accuracy: 0.9786\n",
      "Epoch 164/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0636 - accuracy: 0.9810\n",
      "Epoch 165/500\n",
      "7/7 [==============================] - 2s 352ms/step - loss: 0.0687 - accuracy: 0.9776\n",
      "Epoch 166/500\n",
      "7/7 [==============================] - 3s 368ms/step - loss: 0.0669 - accuracy: 0.9797\n",
      "Epoch 167/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0642 - accuracy: 0.9795\n",
      "Epoch 168/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0683 - accuracy: 0.9783\n",
      "Epoch 169/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0607 - accuracy: 0.9799\n",
      "Epoch 170/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0627 - accuracy: 0.9812\n",
      "Epoch 171/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0676 - accuracy: 0.9793\n",
      "Epoch 172/500\n",
      "7/7 [==============================] - 3s 371ms/step - loss: 0.0695 - accuracy: 0.9789\n",
      "Epoch 173/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0692 - accuracy: 0.9792\n",
      "Epoch 174/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0651 - accuracy: 0.9795\n",
      "Epoch 175/500\n",
      "7/7 [==============================] - 3s 375ms/step - loss: 0.0665 - accuracy: 0.9782\n",
      "Epoch 176/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0608 - accuracy: 0.9818\n",
      "Epoch 177/500\n",
      "7/7 [==============================] - 2s 356ms/step - loss: 0.0625 - accuracy: 0.9825\n",
      "Epoch 178/500\n",
      "7/7 [==============================] - 3s 368ms/step - loss: 0.0646 - accuracy: 0.9790\n",
      "Epoch 179/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0656 - accuracy: 0.9786\n",
      "Epoch 180/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0658 - accuracy: 0.9809\n",
      "Epoch 181/500\n",
      "7/7 [==============================] - 3s 376ms/step - loss: 0.0768 - accuracy: 0.9753\n",
      "Epoch 182/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0741 - accuracy: 0.9759\n",
      "Epoch 183/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0636 - accuracy: 0.9802\n",
      "Epoch 184/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0673 - accuracy: 0.9790\n",
      "Epoch 185/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0649 - accuracy: 0.9803\n",
      "Epoch 186/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0637 - accuracy: 0.9800\n",
      "Epoch 187/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0646 - accuracy: 0.9779\n",
      "Epoch 188/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0639 - accuracy: 0.9807\n",
      "Epoch 189/500\n",
      "7/7 [==============================] - 2s 356ms/step - loss: 0.0611 - accuracy: 0.9815\n",
      "Epoch 190/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0623 - accuracy: 0.9803\n",
      "Epoch 191/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0688 - accuracy: 0.9784\n",
      "Epoch 192/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0636 - accuracy: 0.9805\n",
      "Epoch 193/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0609 - accuracy: 0.9816\n",
      "Epoch 194/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0630 - accuracy: 0.9797\n",
      "Epoch 195/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0607 - accuracy: 0.9797\n",
      "Epoch 196/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0650 - accuracy: 0.9787\n",
      "Epoch 197/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0628 - accuracy: 0.9812\n",
      "Epoch 198/500\n",
      "7/7 [==============================] - 3s 369ms/step - loss: 0.0648 - accuracy: 0.9800\n",
      "Epoch 199/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0615 - accuracy: 0.9795\n",
      "Epoch 200/500\n",
      "7/7 [==============================] - 2s 352ms/step - loss: 0.0584 - accuracy: 0.9830\n",
      "Epoch 201/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0554 - accuracy: 0.9823\n",
      "Epoch 202/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0596 - accuracy: 0.9829\n",
      "Epoch 203/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0635 - accuracy: 0.9796\n",
      "Epoch 204/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0596 - accuracy: 0.9809\n",
      "Epoch 205/500\n",
      "7/7 [==============================] - 2s 352ms/step - loss: 0.0584 - accuracy: 0.9816\n",
      "Epoch 206/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0595 - accuracy: 0.9809\n",
      "Epoch 207/500\n",
      "7/7 [==============================] - 2s 353ms/step - loss: 0.0619 - accuracy: 0.9802\n",
      "Epoch 208/500\n",
      "7/7 [==============================] - 2s 354ms/step - loss: 0.0661 - accuracy: 0.9790\n",
      "Epoch 209/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0599 - accuracy: 0.9807\n",
      "Epoch 210/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0601 - accuracy: 0.9807\n",
      "Epoch 211/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0619 - accuracy: 0.9805\n",
      "Epoch 212/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0643 - accuracy: 0.9800\n",
      "Epoch 213/500\n",
      "7/7 [==============================] - 2s 353ms/step - loss: 0.0582 - accuracy: 0.9802\n",
      "Epoch 214/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0569 - accuracy: 0.9826\n",
      "Epoch 215/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0577 - accuracy: 0.9816\n",
      "Epoch 216/500\n",
      "7/7 [==============================] - 2s 353ms/step - loss: 0.0596 - accuracy: 0.9820\n",
      "Epoch 217/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0619 - accuracy: 0.9806\n",
      "Epoch 218/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0628 - accuracy: 0.9816\n",
      "Epoch 219/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0583 - accuracy: 0.9816\n",
      "Epoch 220/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0609 - accuracy: 0.9816\n",
      "Epoch 221/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0592 - accuracy: 0.9806\n",
      "Epoch 222/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0534 - accuracy: 0.9835\n",
      "Epoch 223/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0535 - accuracy: 0.9838\n",
      "Epoch 224/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0591 - accuracy: 0.9813\n",
      "Epoch 225/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0557 - accuracy: 0.9819\n",
      "Epoch 226/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0549 - accuracy: 0.9825\n",
      "Epoch 227/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0557 - accuracy: 0.9836\n",
      "Epoch 228/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0590 - accuracy: 0.9823\n",
      "Epoch 229/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0561 - accuracy: 0.9829\n",
      "Epoch 230/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0578 - accuracy: 0.9816\n",
      "Epoch 231/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0529 - accuracy: 0.9842\n",
      "Epoch 232/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0532 - accuracy: 0.9845\n",
      "Epoch 233/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0559 - accuracy: 0.9825\n",
      "Epoch 234/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0575 - accuracy: 0.9812\n",
      "Epoch 235/500\n",
      "7/7 [==============================] - 2s 345ms/step - loss: 0.0559 - accuracy: 0.9819\n",
      "Epoch 236/500\n",
      "7/7 [==============================] - 3s 359ms/step - loss: 0.0553 - accuracy: 0.9828\n",
      "Epoch 237/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0566 - accuracy: 0.9825\n",
      "Epoch 238/500\n",
      "7/7 [==============================] - 3s 367ms/step - loss: 0.0553 - accuracy: 0.9830\n",
      "Epoch 239/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0600 - accuracy: 0.9816\n",
      "Epoch 240/500\n",
      "7/7 [==============================] - 2s 352ms/step - loss: 0.0587 - accuracy: 0.9806\n",
      "Epoch 241/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0565 - accuracy: 0.9820\n",
      "Epoch 242/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0591 - accuracy: 0.9818\n",
      "Epoch 243/500\n",
      "7/7 [==============================] - 2s 355ms/step - loss: 0.0536 - accuracy: 0.9816\n",
      "Epoch 244/500\n",
      "7/7 [==============================] - 3s 351ms/step - loss: 0.0576 - accuracy: 0.9822\n",
      "Epoch 245/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0571 - accuracy: 0.9816\n",
      "Epoch 246/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0580 - accuracy: 0.9829\n",
      "Epoch 247/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0615 - accuracy: 0.9807\n",
      "Epoch 248/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0587 - accuracy: 0.9818\n",
      "Epoch 249/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0558 - accuracy: 0.9823\n",
      "Epoch 250/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0574 - accuracy: 0.9820\n",
      "Epoch 251/500\n",
      "7/7 [==============================] - 3s 358ms/step - loss: 0.0525 - accuracy: 0.9840\n",
      "Epoch 252/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0537 - accuracy: 0.9836\n",
      "Epoch 253/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0555 - accuracy: 0.9835\n",
      "Epoch 254/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0521 - accuracy: 0.9836\n",
      "Epoch 255/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0528 - accuracy: 0.9836\n",
      "Epoch 256/500\n",
      "7/7 [==============================] - 3s 372ms/step - loss: 0.0540 - accuracy: 0.9835\n",
      "Epoch 257/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0553 - accuracy: 0.9835\n",
      "Epoch 258/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0514 - accuracy: 0.9830\n",
      "Epoch 259/500\n",
      "7/7 [==============================] - 2s 353ms/step - loss: 0.0591 - accuracy: 0.9813\n",
      "Epoch 260/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0579 - accuracy: 0.9816\n",
      "Epoch 261/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0540 - accuracy: 0.9832\n",
      "Epoch 262/500\n",
      "7/7 [==============================] - 2s 352ms/step - loss: 0.0539 - accuracy: 0.9828\n",
      "Epoch 263/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0500 - accuracy: 0.9852\n",
      "Epoch 264/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0511 - accuracy: 0.9852\n",
      "Epoch 265/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0543 - accuracy: 0.9819\n",
      "Epoch 266/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0615 - accuracy: 0.9803\n",
      "Epoch 267/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0591 - accuracy: 0.9819\n",
      "Epoch 268/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0546 - accuracy: 0.9832\n",
      "Epoch 269/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0565 - accuracy: 0.9819\n",
      "Epoch 270/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0519 - accuracy: 0.9848\n",
      "Epoch 271/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0537 - accuracy: 0.9829\n",
      "Epoch 272/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0525 - accuracy: 0.9845\n",
      "Epoch 273/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0554 - accuracy: 0.9845\n",
      "Epoch 274/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0531 - accuracy: 0.9835\n",
      "Epoch 275/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0562 - accuracy: 0.9826\n",
      "Epoch 276/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0514 - accuracy: 0.9852\n",
      "Epoch 277/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0525 - accuracy: 0.9836\n",
      "Epoch 278/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0526 - accuracy: 0.9843\n",
      "Epoch 279/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0545 - accuracy: 0.9830\n",
      "Epoch 280/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0544 - accuracy: 0.9829\n",
      "Epoch 281/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0511 - accuracy: 0.9853\n",
      "Epoch 282/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0524 - accuracy: 0.9845\n",
      "Epoch 283/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0528 - accuracy: 0.9835\n",
      "Epoch 284/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0590 - accuracy: 0.9818\n",
      "Epoch 285/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0522 - accuracy: 0.9835\n",
      "Epoch 286/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0524 - accuracy: 0.9851\n",
      "Epoch 287/500\n",
      "7/7 [==============================] - 2s 352ms/step - loss: 0.0553 - accuracy: 0.9826\n",
      "Epoch 288/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0592 - accuracy: 0.9810\n",
      "Epoch 289/500\n",
      "7/7 [==============================] - 3s 362ms/step - loss: 0.0546 - accuracy: 0.9823\n",
      "Epoch 290/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0525 - accuracy: 0.9846\n",
      "Epoch 291/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0484 - accuracy: 0.9851\n",
      "Epoch 292/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0491 - accuracy: 0.9849\n",
      "Epoch 293/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0507 - accuracy: 0.9848\n",
      "Epoch 294/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0501 - accuracy: 0.9840\n",
      "Epoch 295/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0466 - accuracy: 0.9856\n",
      "Epoch 296/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0552 - accuracy: 0.9835\n",
      "Epoch 297/500\n",
      "7/7 [==============================] - 3s 359ms/step - loss: 0.0495 - accuracy: 0.9855\n",
      "Epoch 298/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0510 - accuracy: 0.9839\n",
      "Epoch 299/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0502 - accuracy: 0.9828\n",
      "Epoch 300/500\n",
      "7/7 [==============================] - 2s 345ms/step - loss: 0.0497 - accuracy: 0.9843\n",
      "Epoch 301/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0509 - accuracy: 0.9845\n",
      "Epoch 302/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0502 - accuracy: 0.9849\n",
      "Epoch 303/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0524 - accuracy: 0.9843\n",
      "Epoch 304/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0516 - accuracy: 0.9832\n",
      "Epoch 305/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0504 - accuracy: 0.9848\n",
      "Epoch 306/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0480 - accuracy: 0.9851\n",
      "Epoch 307/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0493 - accuracy: 0.9835\n",
      "Epoch 308/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0468 - accuracy: 0.9840\n",
      "Epoch 309/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0528 - accuracy: 0.9838\n",
      "Epoch 310/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0574 - accuracy: 0.9829\n",
      "Epoch 311/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0524 - accuracy: 0.9828\n",
      "Epoch 312/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0482 - accuracy: 0.9843\n",
      "Epoch 313/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0502 - accuracy: 0.9825\n",
      "Epoch 314/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0480 - accuracy: 0.9852\n",
      "Epoch 315/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0454 - accuracy: 0.9853\n",
      "Epoch 316/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0493 - accuracy: 0.9856\n",
      "Epoch 317/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0488 - accuracy: 0.9848\n",
      "Epoch 318/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0472 - accuracy: 0.9856\n",
      "Epoch 319/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0476 - accuracy: 0.9851\n",
      "Epoch 320/500\n",
      "7/7 [==============================] - 2s 345ms/step - loss: 0.0504 - accuracy: 0.9849\n",
      "Epoch 321/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0538 - accuracy: 0.9822\n",
      "Epoch 322/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0505 - accuracy: 0.9855\n",
      "Epoch 323/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0488 - accuracy: 0.9845\n",
      "Epoch 324/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0482 - accuracy: 0.9852\n",
      "Epoch 325/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0487 - accuracy: 0.9842\n",
      "Epoch 326/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0473 - accuracy: 0.9861\n",
      "Epoch 327/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0480 - accuracy: 0.9845\n",
      "Epoch 328/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0547 - accuracy: 0.9823\n",
      "Epoch 329/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0501 - accuracy: 0.9851\n",
      "Epoch 330/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0523 - accuracy: 0.9832\n",
      "Epoch 331/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0492 - accuracy: 0.9858\n",
      "Epoch 332/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0497 - accuracy: 0.9846\n",
      "Epoch 333/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0537 - accuracy: 0.9838\n",
      "Epoch 334/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0500 - accuracy: 0.9848\n",
      "Epoch 335/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0495 - accuracy: 0.9846\n",
      "Epoch 336/500\n",
      "7/7 [==============================] - 2s 353ms/step - loss: 0.0460 - accuracy: 0.9855\n",
      "Epoch 337/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0458 - accuracy: 0.9866\n",
      "Epoch 338/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0451 - accuracy: 0.9865\n",
      "Epoch 339/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0451 - accuracy: 0.9856\n",
      "Epoch 340/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0436 - accuracy: 0.9862\n",
      "Epoch 341/500\n",
      "7/7 [==============================] - 2s 352ms/step - loss: 0.0487 - accuracy: 0.9843\n",
      "Epoch 342/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0470 - accuracy: 0.9855\n",
      "Epoch 343/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0460 - accuracy: 0.9852\n",
      "Epoch 344/500\n",
      "7/7 [==============================] - 3s 358ms/step - loss: 0.0486 - accuracy: 0.9845\n",
      "Epoch 345/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0480 - accuracy: 0.9842\n",
      "Epoch 346/500\n",
      "7/7 [==============================] - 2s 352ms/step - loss: 0.0440 - accuracy: 0.9874\n",
      "Epoch 347/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0440 - accuracy: 0.9856\n",
      "Epoch 348/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0460 - accuracy: 0.9855\n",
      "Epoch 349/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0472 - accuracy: 0.9859\n",
      "Epoch 350/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0443 - accuracy: 0.9869\n",
      "Epoch 351/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0438 - accuracy: 0.9861\n",
      "Epoch 352/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0455 - accuracy: 0.9848\n",
      "Epoch 353/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0483 - accuracy: 0.9838\n",
      "Epoch 354/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0475 - accuracy: 0.9871\n",
      "Epoch 355/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0426 - accuracy: 0.9871\n",
      "Epoch 356/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0503 - accuracy: 0.9851\n",
      "Epoch 357/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0487 - accuracy: 0.9849\n",
      "Epoch 358/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0478 - accuracy: 0.9845\n",
      "Epoch 359/500\n",
      "7/7 [==============================] - 3s 374ms/step - loss: 0.0435 - accuracy: 0.9869\n",
      "Epoch 360/500\n",
      "7/7 [==============================] - 2s 354ms/step - loss: 0.0490 - accuracy: 0.9851\n",
      "Epoch 361/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0477 - accuracy: 0.9855\n",
      "Epoch 362/500\n",
      "7/7 [==============================] - 3s 380ms/step - loss: 0.0460 - accuracy: 0.9858\n",
      "Epoch 363/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0457 - accuracy: 0.9856\n",
      "Epoch 364/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0442 - accuracy: 0.9869\n",
      "Epoch 365/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0475 - accuracy: 0.9855\n",
      "Epoch 366/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0468 - accuracy: 0.9849\n",
      "Epoch 367/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0485 - accuracy: 0.9858\n",
      "Epoch 368/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0470 - accuracy: 0.9852\n",
      "Epoch 369/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0461 - accuracy: 0.9855\n",
      "Epoch 370/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0472 - accuracy: 0.9853\n",
      "Epoch 371/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0483 - accuracy: 0.9856\n",
      "Epoch 372/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0400 - accuracy: 0.9875\n",
      "Epoch 373/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0458 - accuracy: 0.9853\n",
      "Epoch 374/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0447 - accuracy: 0.9874\n",
      "Epoch 375/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0487 - accuracy: 0.9852\n",
      "Epoch 376/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0428 - accuracy: 0.9863\n",
      "Epoch 377/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0470 - accuracy: 0.9861\n",
      "Epoch 378/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0466 - accuracy: 0.9852\n",
      "Epoch 379/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0481 - accuracy: 0.9851\n",
      "Epoch 380/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0412 - accuracy: 0.9879\n",
      "Epoch 381/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0440 - accuracy: 0.9856\n",
      "Epoch 382/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0424 - accuracy: 0.9879\n",
      "Epoch 383/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0456 - accuracy: 0.9859\n",
      "Epoch 384/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0465 - accuracy: 0.9851\n",
      "Epoch 385/500\n",
      "7/7 [==============================] - 2s 352ms/step - loss: 0.0442 - accuracy: 0.9863\n",
      "Epoch 386/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0448 - accuracy: 0.9866\n",
      "Epoch 387/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0450 - accuracy: 0.9852\n",
      "Epoch 388/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0483 - accuracy: 0.9848\n",
      "Epoch 389/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0474 - accuracy: 0.9861\n",
      "Epoch 390/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0469 - accuracy: 0.9852\n",
      "Epoch 391/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0441 - accuracy: 0.9866\n",
      "Epoch 392/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0456 - accuracy: 0.9852\n",
      "Epoch 393/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0451 - accuracy: 0.9863\n",
      "Epoch 394/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0441 - accuracy: 0.9858\n",
      "Epoch 395/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0435 - accuracy: 0.9876\n",
      "Epoch 396/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0432 - accuracy: 0.9861\n",
      "Epoch 397/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0434 - accuracy: 0.9858\n",
      "Epoch 398/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0463 - accuracy: 0.9863\n",
      "Epoch 399/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0424 - accuracy: 0.9871\n",
      "Epoch 400/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0491 - accuracy: 0.9839\n",
      "Epoch 401/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0431 - accuracy: 0.9865\n",
      "Epoch 402/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0462 - accuracy: 0.9858\n",
      "Epoch 403/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0470 - accuracy: 0.9855\n",
      "Epoch 404/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0465 - accuracy: 0.9853\n",
      "Epoch 405/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0434 - accuracy: 0.9876\n",
      "Epoch 406/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0493 - accuracy: 0.9833\n",
      "Epoch 407/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0482 - accuracy: 0.9838\n",
      "Epoch 408/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0490 - accuracy: 0.9863\n",
      "Epoch 409/500\n",
      "7/7 [==============================] - 2s 353ms/step - loss: 0.0457 - accuracy: 0.9853\n",
      "Epoch 410/500\n",
      "7/7 [==============================] - 3s 385ms/step - loss: 0.0463 - accuracy: 0.9861\n",
      "Epoch 411/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0414 - accuracy: 0.9875\n",
      "Epoch 412/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0453 - accuracy: 0.9869\n",
      "Epoch 413/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0423 - accuracy: 0.9865\n",
      "Epoch 414/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0445 - accuracy: 0.9869\n",
      "Epoch 415/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0448 - accuracy: 0.9872\n",
      "Epoch 416/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0451 - accuracy: 0.9859\n",
      "Epoch 417/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0459 - accuracy: 0.9866\n",
      "Epoch 418/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0460 - accuracy: 0.9862\n",
      "Epoch 419/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0416 - accuracy: 0.9879\n",
      "Epoch 420/500\n",
      "7/7 [==============================] - 3s 374ms/step - loss: 0.0446 - accuracy: 0.9863\n",
      "Epoch 421/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0488 - accuracy: 0.9849\n",
      "Epoch 422/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0436 - accuracy: 0.9852\n",
      "Epoch 423/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0424 - accuracy: 0.9863\n",
      "Epoch 424/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0442 - accuracy: 0.9855\n",
      "Epoch 425/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0368 - accuracy: 0.9891\n",
      "Epoch 426/500\n",
      "7/7 [==============================] - 2s 345ms/step - loss: 0.0447 - accuracy: 0.9862\n",
      "Epoch 427/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0424 - accuracy: 0.9879\n",
      "Epoch 428/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0425 - accuracy: 0.9881\n",
      "Epoch 429/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0429 - accuracy: 0.9876\n",
      "Epoch 430/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0437 - accuracy: 0.9863\n",
      "Epoch 431/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0421 - accuracy: 0.9862\n",
      "Epoch 432/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0403 - accuracy: 0.9891\n",
      "Epoch 433/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0411 - accuracy: 0.9872\n",
      "Epoch 434/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0415 - accuracy: 0.9871\n",
      "Epoch 435/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0424 - accuracy: 0.9871\n",
      "Epoch 436/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0455 - accuracy: 0.9862\n",
      "Epoch 437/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0415 - accuracy: 0.9869\n",
      "Epoch 438/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0444 - accuracy: 0.9871\n",
      "Epoch 439/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0412 - accuracy: 0.9868\n",
      "Epoch 440/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0433 - accuracy: 0.9865\n",
      "Epoch 441/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0451 - accuracy: 0.9859\n",
      "Epoch 442/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0457 - accuracy: 0.9855\n",
      "Epoch 443/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0433 - accuracy: 0.9858\n",
      "Epoch 444/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0434 - accuracy: 0.9871\n",
      "Epoch 445/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0422 - accuracy: 0.9876\n",
      "Epoch 446/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0397 - accuracy: 0.9872\n",
      "Epoch 447/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0411 - accuracy: 0.9872\n",
      "Epoch 448/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0435 - accuracy: 0.9868\n",
      "Epoch 449/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0394 - accuracy: 0.9879\n",
      "Epoch 450/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0409 - accuracy: 0.9875\n",
      "Epoch 451/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0397 - accuracy: 0.9888\n",
      "Epoch 452/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0410 - accuracy: 0.9879\n",
      "Epoch 453/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0398 - accuracy: 0.9879\n",
      "Epoch 454/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0434 - accuracy: 0.9865\n",
      "Epoch 455/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0389 - accuracy: 0.9878\n",
      "Epoch 456/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0404 - accuracy: 0.9869\n",
      "Epoch 457/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0438 - accuracy: 0.9872\n",
      "Epoch 458/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0427 - accuracy: 0.9871\n",
      "Epoch 459/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0436 - accuracy: 0.9855\n",
      "Epoch 460/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0407 - accuracy: 0.9876\n",
      "Epoch 461/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0461 - accuracy: 0.9859\n",
      "Epoch 462/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0445 - accuracy: 0.9861\n",
      "Epoch 463/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0394 - accuracy: 0.9882\n",
      "Epoch 464/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0450 - accuracy: 0.9858\n",
      "Epoch 465/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0435 - accuracy: 0.9871\n",
      "Epoch 466/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0403 - accuracy: 0.9868\n",
      "Epoch 467/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0384 - accuracy: 0.9871\n",
      "Epoch 468/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0418 - accuracy: 0.9875\n",
      "Epoch 469/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0412 - accuracy: 0.9876\n",
      "Epoch 470/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0414 - accuracy: 0.9875\n",
      "Epoch 471/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0431 - accuracy: 0.9856\n",
      "Epoch 472/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0438 - accuracy: 0.9876\n",
      "Epoch 473/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0466 - accuracy: 0.9861\n",
      "Epoch 474/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0430 - accuracy: 0.9865\n",
      "Epoch 475/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0403 - accuracy: 0.9881\n",
      "Epoch 476/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0423 - accuracy: 0.9865\n",
      "Epoch 477/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0441 - accuracy: 0.9863\n",
      "Epoch 478/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0432 - accuracy: 0.9863\n",
      "Epoch 479/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0424 - accuracy: 0.9871\n",
      "Epoch 480/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0391 - accuracy: 0.9869\n",
      "Epoch 481/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0430 - accuracy: 0.9866\n",
      "Epoch 482/500\n",
      "7/7 [==============================] - 3s 357ms/step - loss: 0.0436 - accuracy: 0.9862\n",
      "Epoch 483/500\n",
      "7/7 [==============================] - 3s 366ms/step - loss: 0.0435 - accuracy: 0.9855\n",
      "Epoch 484/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0493 - accuracy: 0.9845\n",
      "Epoch 485/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0407 - accuracy: 0.9868\n",
      "Epoch 486/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0420 - accuracy: 0.9875\n",
      "Epoch 487/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0400 - accuracy: 0.9885\n",
      "Epoch 488/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0394 - accuracy: 0.9879\n",
      "Epoch 489/500\n",
      "7/7 [==============================] - 2s 346ms/step - loss: 0.0467 - accuracy: 0.9848\n",
      "Epoch 490/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0393 - accuracy: 0.9874\n",
      "Epoch 491/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0423 - accuracy: 0.9868\n",
      "Epoch 492/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0416 - accuracy: 0.9871\n",
      "Epoch 493/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0404 - accuracy: 0.9871\n",
      "Epoch 494/500\n",
      "7/7 [==============================] - 2s 347ms/step - loss: 0.0440 - accuracy: 0.9869\n",
      "Epoch 495/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0431 - accuracy: 0.9872\n",
      "Epoch 496/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0443 - accuracy: 0.9862\n",
      "Epoch 497/500\n",
      "7/7 [==============================] - 2s 350ms/step - loss: 0.0403 - accuracy: 0.9875\n",
      "Epoch 498/500\n",
      "7/7 [==============================] - 2s 348ms/step - loss: 0.0369 - accuracy: 0.9899\n",
      "Epoch 499/500\n",
      "7/7 [==============================] - 2s 349ms/step - loss: 0.0430 - accuracy: 0.9846\n",
      "Epoch 500/500\n",
      "7/7 [==============================] - 2s 351ms/step - loss: 0.0426 - accuracy: 0.9861\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetConfig, TFXLNetModel\n",
    "data = np.array(list(id_dict.values()))\n",
    "X_train = data[:,:-1]\n",
    "y_train = data[:,-1]\n",
    "y_train = np.eye(5)[y_train]\n",
    "\n",
    "\n",
    "config = XLNetConfig(vocab_size=15, d_model=16, n_layer=8, n_head=4, d_inner=64\n",
    "                    )\n",
    "\n",
    "xlnet = TFXLNetModel(config)\n",
    "\n",
    "input_seq = tf.keras.layers.Input(shape=(14,), name='input_seq', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(14,), name='attention_mask', dtype='int32')\n",
    "\n",
    "embeddings = xlnet(input_seq)[0]\n",
    "embeddings = embeddings[:, 0]\n",
    "\n",
    "x = tf.keras.layers.Dense(32, activation='relu')(embeddings)\n",
    "y = tf.keras.layers.Dense(5, activation='softmax', name='outputs')(x)\n",
    "model = tf.keras.Model(inputs=[input_seq, mask], outputs=y)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, decay=1e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
    "X_mask = np.ones((len(X_train),14))\n",
    "history = model.fit([X_train, X_mask], y_train, batch_size=1024, epochs=500)\n",
    "model.save_weights('./checkpoint_XLNet/Transformer_XLNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training - Loss Function')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.subplot()\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Training - Loss Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 2s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[7.0250077e-05, 4.9036869e-04, 9.9942005e-01, 5.8276610e-06,\n",
       "        1.3468104e-05],\n",
       "       [5.2224317e-07, 9.7536883e-07, 9.1128601e-07, 3.3847494e-03,\n",
       "        9.9661273e-01],\n",
       "       [9.9999422e-01, 2.6535886e-06, 2.2957142e-06, 8.9325613e-07,\n",
       "        4.6479940e-08],\n",
       "       ...,\n",
       "       [9.9999428e-01, 3.2391313e-06, 1.6806187e-06, 8.6096998e-07,\n",
       "        4.4235225e-08],\n",
       "       [4.0106032e-05, 8.9091707e-05, 9.9986315e-01, 5.6180406e-06,\n",
       "        2.0288348e-06],\n",
       "       [9.9999440e-01, 3.2198886e-06, 1.3595537e-06, 1.1042979e-06,\n",
       "        4.5760743e-08]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_mask = np.ones((len(X_test),14))\n",
    "y_prob = model.predict([X_test, X_test_mask])\n",
    "y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mac_hash</th>\n",
       "      <th>C0</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b882f1d44602a25349a08f4a0af32977</td>\n",
       "      <td>7.025008e-05</td>\n",
       "      <td>4.903687e-04</td>\n",
       "      <td>9.994200e-01</td>\n",
       "      <td>5.827661e-06</td>\n",
       "      <td>1.346810e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86e644b498922f3a2fd0e6e1a1525de3</td>\n",
       "      <td>5.222432e-07</td>\n",
       "      <td>9.753688e-07</td>\n",
       "      <td>9.112860e-07</td>\n",
       "      <td>3.384749e-03</td>\n",
       "      <td>9.966127e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>691662b04ee08015062d901a4c5628b1</td>\n",
       "      <td>9.999942e-01</td>\n",
       "      <td>2.653589e-06</td>\n",
       "      <td>2.295714e-06</td>\n",
       "      <td>8.932561e-07</td>\n",
       "      <td>4.647994e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52b5c510a28774237f4f118764c2ed6f</td>\n",
       "      <td>1.680964e-04</td>\n",
       "      <td>9.996778e-01</td>\n",
       "      <td>2.268377e-05</td>\n",
       "      <td>1.306366e-04</td>\n",
       "      <td>8.692384e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9f3c995e53d109f532056b6eae29a0b5</td>\n",
       "      <td>4.433210e-05</td>\n",
       "      <td>1.598631e-02</td>\n",
       "      <td>9.838040e-01</td>\n",
       "      <td>8.713668e-05</td>\n",
       "      <td>7.806838e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>5001c0dfe522886ed884c0dc4e5848df</td>\n",
       "      <td>9.999944e-01</td>\n",
       "      <td>3.110032e-06</td>\n",
       "      <td>1.399284e-06</td>\n",
       "      <td>1.078556e-06</td>\n",
       "      <td>4.705225e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425</th>\n",
       "      <td>f1c8781e701e89068fa0b8a62ecbc564</td>\n",
       "      <td>8.473531e-07</td>\n",
       "      <td>2.341644e-06</td>\n",
       "      <td>1.235653e-06</td>\n",
       "      <td>1.770269e-03</td>\n",
       "      <td>9.982253e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426</th>\n",
       "      <td>646136b402e136422466a2acd8636630</td>\n",
       "      <td>9.999943e-01</td>\n",
       "      <td>3.239131e-06</td>\n",
       "      <td>1.680619e-06</td>\n",
       "      <td>8.609700e-07</td>\n",
       "      <td>4.423523e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427</th>\n",
       "      <td>c7bea6491029f86ab3d1f0f9b599ca2c</td>\n",
       "      <td>4.010603e-05</td>\n",
       "      <td>8.909171e-05</td>\n",
       "      <td>9.998631e-01</td>\n",
       "      <td>5.618041e-06</td>\n",
       "      <td>2.028835e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428</th>\n",
       "      <td>822fd3104f38cc6374f2996d3d6920a2</td>\n",
       "      <td>9.999944e-01</td>\n",
       "      <td>3.219889e-06</td>\n",
       "      <td>1.359554e-06</td>\n",
       "      <td>1.104298e-06</td>\n",
       "      <td>4.576074e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3429 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              mac_hash            C0            C1  \\\n",
       "0     b882f1d44602a25349a08f4a0af32977  7.025008e-05  4.903687e-04   \n",
       "1     86e644b498922f3a2fd0e6e1a1525de3  5.222432e-07  9.753688e-07   \n",
       "2     691662b04ee08015062d901a4c5628b1  9.999942e-01  2.653589e-06   \n",
       "3     52b5c510a28774237f4f118764c2ed6f  1.680964e-04  9.996778e-01   \n",
       "4     9f3c995e53d109f532056b6eae29a0b5  4.433210e-05  1.598631e-02   \n",
       "...                                ...           ...           ...   \n",
       "3424  5001c0dfe522886ed884c0dc4e5848df  9.999944e-01  3.110032e-06   \n",
       "3425  f1c8781e701e89068fa0b8a62ecbc564  8.473531e-07  2.341644e-06   \n",
       "3426  646136b402e136422466a2acd8636630  9.999943e-01  3.239131e-06   \n",
       "3427  c7bea6491029f86ab3d1f0f9b599ca2c  4.010603e-05  8.909171e-05   \n",
       "3428  822fd3104f38cc6374f2996d3d6920a2  9.999944e-01  3.219889e-06   \n",
       "\n",
       "                C2            C3            C4  \n",
       "0     9.994200e-01  5.827661e-06  1.346810e-05  \n",
       "1     9.112860e-07  3.384749e-03  9.966127e-01  \n",
       "2     2.295714e-06  8.932561e-07  4.647994e-08  \n",
       "3     2.268377e-05  1.306366e-04  8.692384e-07  \n",
       "4     9.838040e-01  8.713668e-05  7.806838e-05  \n",
       "...            ...           ...           ...  \n",
       "3424  1.399284e-06  1.078556e-06  4.705225e-08  \n",
       "3425  1.235653e-06  1.770269e-03  9.982253e-01  \n",
       "3426  1.680619e-06  8.609700e-07  4.423523e-08  \n",
       "3427  9.998631e-01  5.618041e-06  2.028835e-06  \n",
       "3428  1.359554e-06  1.104298e-06  4.576074e-08  \n",
       "\n",
       "[3429 rows x 6 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame({'mac_hash':sample['mac_hash'], 'C0':y_prob[:,0], 'C1':y_prob[:,1], 'C2':y_prob[:,2], 'C3':y_prob[:,3], 'C4':y_prob[:,4]})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"output_transformer_XLNet.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#FF0000> Data preprocess (w/ group)</font> \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_id_dict = {m: [] for m in id_list}\n",
    "\n",
    "group_1 = dict()\n",
    "group_2 = dict()\n",
    "group_3 = dict()\n",
    "group_4 = dict()\n",
    "\n",
    "for data in list(train_data.values):\n",
    "    loc = data[1]\n",
    "    group_id_dict[data[0]].append(loc)\n",
    "\n",
    "## 1~2\n",
    "## 3~7\n",
    "## 8~14\n",
    "for k, v in group_id_dict.items():\n",
    "    if 1 <= len(v) <= 2:\n",
    "        group_1[k] = [0] * (2 - len(v)) + v\n",
    "        group_1[k].append(label_dict[k])\n",
    "    elif 3 <= len(v) <= 7:\n",
    "        # padding\n",
    "        group_2[k] = [0] * (7 - len(v)) + v\n",
    "        group_2[k].append(label_dict[k])\n",
    "    elif 8 <= len(v) <= 14:\n",
    "        # padding\n",
    "        group_3[k] = [0] * (14 - len(v)) + v\n",
    "        group_3[k].append(label_dict[k])\n",
    "        \n",
    "data = np.array(list(group_1.values()))\n",
    "X_train_1 = data[:,:-1]\n",
    "y_train_1 = data[:,-1]\n",
    "\n",
    "data = np.array(list(group_2.values()))\n",
    "X_train_2 = data[:,:-1]\n",
    "y_train_2 = data[:,-1]\n",
    "\n",
    "data = np.array(list(group_3.values()))\n",
    "X_train_3 = data[:,:-1]\n",
    "y_train_3 = data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#FF0000> Catboost (w/ group)</font> \t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (with cross validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/home/leo/anaconda3/envs/MLtest/lib/python3.9/site-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......................................., score=0.944 total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......................................., score=0.939 total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......................................., score=0.941 total time=   0.5s\n",
      "[CV] END ......................................., score=0.936 total time=   0.5s\n",
      "[CV] END ......................................., score=0.947 total time=   0.5s\n",
      "Group 2 cross validation mean accuracy: 94.12064171122994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......................................., score=0.922 total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......................................., score=0.937 total time=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ......................................., score=0.953 total time=   0.8s\n",
      "[CV] END ......................................., score=0.946 total time=   0.8s\n",
      "[CV] END ......................................., score=0.942 total time=   0.8s\n",
      "Group 3 cross validation mean accuracy: 93.99037522667038\n",
      "All group mean cross validation accuracy: 96.10575310733743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    3.8s finished\n"
     ]
    }
   ],
   "source": [
    "acc_1 = 100\n",
    "model_2 = CatBoostClassifier(loss_function='MultiClass', verbose=0)\n",
    "result = cross_validate(model_2, X_train_2, y_train_2, cv=5, verbose=3)\n",
    "acc_2 = result['test_score'].mean()*100\n",
    "print('Group 2 cross validation mean accuracy:', acc_2)\n",
    "model_3 = CatBoostClassifier(loss_function='MultiClass', verbose=0)\n",
    "result = cross_validate(model_3, X_train_3, y_train_3, cv=5, verbose=3)\n",
    "acc_3 = result['test_score'].mean()*100\n",
    "print('Group 3 cross validation mean accuracy:', acc_3)\n",
    "\n",
    "#model = CatBoostClassifier(iterations=1000,learning_rate=0.1,max_depth=7,verbose=100,\n",
    "#                           early_stopping_rounds=500,task_type='GPU',eval_metric='AUC')\n",
    "\n",
    "print('All group mean cross validation accuracy:', \n",
    "      (acc_1*len(group_1)+acc_2*len(group_2)+acc_3*len(group_3))/\n",
    "      (len(group_1)+len(group_2)+len(group_3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (train with  whole training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy: 0.9998563011927001\n"
     ]
    }
   ],
   "source": [
    "model_2 = CatBoostClassifier(loss_function='MultiClass')\n",
    "model_2.fit(X_train_2, y_train_2, verbose=0)\n",
    "model_3 = CatBoostClassifier(loss_function='MultiClass')\n",
    "model_3.fit(X_train_3, y_train_3, verbose=0)\n",
    "\n",
    "y_pred_1 = np.zeros((len(group_1), 1), dtype=int)\n",
    "y_pred_2 = model_2.predict(X_train_2)\n",
    "y_pred_3 = model_3.predict(X_train_3)\n",
    "\n",
    "y_pred_group = np.concatenate((y_pred_1, y_pred_2, y_pred_3), axis=0)\n",
    "\n",
    "y_train_group = np.concatenate((y_train_1, y_train_2, y_train_3), axis=0)\n",
    "print('Training set accuracy:', accuracy_score(y_train_group, y_pred_group))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_id_dict = {m: [] for m in id_list_test}\n",
    "\n",
    "for data in list(test_data.values):\n",
    "    loc = data[1]\n",
    "    group_id_dict[data[0]].append(loc)\n",
    "\n",
    "i = 0\n",
    "y_prob_group = np.zeros((3429,5))\n",
    "for k, v in group_id_dict.items():\n",
    "    if 1 <= len(v) <= 2:\n",
    "        X = [0] * (2 - len(v)) + v\n",
    "        y_prob_group[i] = [1,0,0,0,0]\n",
    "    elif 3 <= len(v) <= 7:\n",
    "        # padding\n",
    "        X = [0] * (7 - len(v)) + v\n",
    "        y_prob_group[i][0:4] = model_2.predict_proba(X)\n",
    "    elif 8 <= len(v) <= 14:\n",
    "        # padding\n",
    "        X = [0] * (14 - len(v)) + v\n",
    "        y_prob_group[i][1:] = model_3.predict_proba(X)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mac_hash</th>\n",
       "      <th>C0</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b882f1d44602a25349a08f4a0af32977</td>\n",
       "      <td>0.102879</td>\n",
       "      <td>0.027298</td>\n",
       "      <td>0.869613</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86e644b498922f3a2fd0e6e1a1525de3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.007759</td>\n",
       "      <td>0.992156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>691662b04ee08015062d901a4c5628b1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52b5c510a28774237f4f118764c2ed6f</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.999068</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9f3c995e53d109f532056b6eae29a0b5</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.040785</td>\n",
       "      <td>0.955921</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>5001c0dfe522886ed884c0dc4e5848df</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425</th>\n",
       "      <td>f1c8781e701e89068fa0b8a62ecbc564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.013290</td>\n",
       "      <td>0.986214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426</th>\n",
       "      <td>646136b402e136422466a2acd8636630</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427</th>\n",
       "      <td>c7bea6491029f86ab3d1f0f9b599ca2c</td>\n",
       "      <td>0.007019</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.992889</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428</th>\n",
       "      <td>822fd3104f38cc6374f2996d3d6920a2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3429 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              mac_hash        C0        C1        C2  \\\n",
       "0     b882f1d44602a25349a08f4a0af32977  0.102879  0.027298  0.869613   \n",
       "1     86e644b498922f3a2fd0e6e1a1525de3  0.000000  0.000006  0.000078   \n",
       "2     691662b04ee08015062d901a4c5628b1  1.000000  0.000000  0.000000   \n",
       "3     52b5c510a28774237f4f118764c2ed6f  0.000926  0.999068  0.000001   \n",
       "4     9f3c995e53d109f532056b6eae29a0b5  0.000705  0.040785  0.955921   \n",
       "...                                ...       ...       ...       ...   \n",
       "3424  5001c0dfe522886ed884c0dc4e5848df  1.000000  0.000000  0.000000   \n",
       "3425  f1c8781e701e89068fa0b8a62ecbc564  0.000000  0.000017  0.000479   \n",
       "3426  646136b402e136422466a2acd8636630  1.000000  0.000000  0.000000   \n",
       "3427  c7bea6491029f86ab3d1f0f9b599ca2c  0.007019  0.000069  0.992889   \n",
       "3428  822fd3104f38cc6374f2996d3d6920a2  1.000000  0.000000  0.000000   \n",
       "\n",
       "            C3        C4  \n",
       "0     0.000210  0.000000  \n",
       "1     0.007759  0.992156  \n",
       "2     0.000000  0.000000  \n",
       "3     0.000005  0.000000  \n",
       "4     0.002588  0.000000  \n",
       "...        ...       ...  \n",
       "3424  0.000000  0.000000  \n",
       "3425  0.013290  0.986214  \n",
       "3426  0.000000  0.000000  \n",
       "3427  0.000023  0.000000  \n",
       "3428  0.000000  0.000000  \n",
       "\n",
       "[3429 rows x 6 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame({'mac_hash':sample['mac_hash'], 'C0':y_prob_group[:,0], 'C1':y_prob_group[:,1], 'C2':y_prob_group[:,2], 'C3':y_prob_group[:,3], 'C4':y_prob_group[:,4]})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"output_Catboost_group.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=#FF0000> Transformer BERT (w/ group)</font> \t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (with validation set spiltted from training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_2_onehot = np.eye(4)[y_train_2]\n",
    "y_train_3_onehot = np.eye(4)[y_train_3 - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(vocab_size=15, hidden_size=16, num_hidden_layer=8, num_attention_heads=4,\n",
    "                    intermediate_size=64, max_position_embeddings=14)\n",
    "bert = TFBertModel(config)\n",
    "\n",
    "input_seq = tf.keras.layers.Input(shape=(7,), name='input_seq', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(7,), name='attention_mask', dtype='int32')\n",
    "\n",
    "encoder = bert(input_seq, attention_mask=mask)[0][:,0] \n",
    "\n",
    "x = tf.keras.layers.Dense(32, activation='relu')(encoder)\n",
    "y = tf.keras.layers.Dense(4, activation='softmax', name='outputs')(x)\n",
    "\n",
    "model_2 = tf.keras.Model(inputs=[input_seq, mask], outputs=y)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, decay=1e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model_2.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "4/4 [==============================] - 12s 673ms/step - loss: 1.0616 - accuracy: 0.5003 - val_loss: 1.0138 - val_accuracy: 0.5106\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 70ms/step - loss: 1.0018 - accuracy: 0.5270 - val_loss: 0.9915 - val_accuracy: 0.5319\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.9863 - accuracy: 0.5342 - val_loss: 0.9804 - val_accuracy: 0.5266\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.9605 - accuracy: 0.5383 - val_loss: 0.9572 - val_accuracy: 0.5266\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.9386 - accuracy: 0.5431 - val_loss: 0.9125 - val_accuracy: 0.5266\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.8911 - accuracy: 0.5472 - val_loss: 0.8258 - val_accuracy: 0.5798\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.7893 - accuracy: 0.6583 - val_loss: 0.6723 - val_accuracy: 0.7766\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.6351 - accuracy: 0.7873 - val_loss: 0.5133 - val_accuracy: 0.8245\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.4862 - accuracy: 0.8253 - val_loss: 0.3975 - val_accuracy: 0.8298\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.3996 - accuracy: 0.8449 - val_loss: 0.3300 - val_accuracy: 0.8617\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.3399 - accuracy: 0.8681 - val_loss: 0.2586 - val_accuracy: 0.9149\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.2705 - accuracy: 0.9002 - val_loss: 0.2082 - val_accuracy: 0.9415\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.2326 - accuracy: 0.9281 - val_loss: 0.1928 - val_accuracy: 0.9574\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.2063 - accuracy: 0.9453 - val_loss: 0.1822 - val_accuracy: 0.9681\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.1841 - accuracy: 0.9560 - val_loss: 0.1857 - val_accuracy: 0.9309\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.1768 - accuracy: 0.9513 - val_loss: 0.1637 - val_accuracy: 0.9787\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.1552 - accuracy: 0.9632 - val_loss: 0.1525 - val_accuracy: 0.9521\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.1489 - accuracy: 0.9572 - val_loss: 0.1263 - val_accuracy: 0.9734\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.1439 - accuracy: 0.9566 - val_loss: 0.1156 - val_accuracy: 0.9734\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.1373 - accuracy: 0.9560 - val_loss: 0.1325 - val_accuracy: 0.9468\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.1318 - accuracy: 0.9596 - val_loss: 0.1069 - val_accuracy: 0.9787\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.1200 - accuracy: 0.9679 - val_loss: 0.1099 - val_accuracy: 0.9787\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.1168 - accuracy: 0.9643 - val_loss: 0.1007 - val_accuracy: 0.9840\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.1162 - accuracy: 0.9661 - val_loss: 0.0915 - val_accuracy: 0.9840\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.1077 - accuracy: 0.9655 - val_loss: 0.0906 - val_accuracy: 0.9734\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.1024 - accuracy: 0.9691 - val_loss: 0.0817 - val_accuracy: 0.9894\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.1014 - accuracy: 0.9667 - val_loss: 0.1012 - val_accuracy: 0.9734\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.1077 - accuracy: 0.9691 - val_loss: 0.0779 - val_accuracy: 0.9734\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0911 - accuracy: 0.9739 - val_loss: 0.0751 - val_accuracy: 0.9840\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 70ms/step - loss: 0.1088 - accuracy: 0.9608 - val_loss: 0.0796 - val_accuracy: 0.9734\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.1023 - accuracy: 0.9649 - val_loss: 0.0789 - val_accuracy: 0.9840\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0947 - accuracy: 0.9673 - val_loss: 0.0888 - val_accuracy: 0.9628\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0846 - accuracy: 0.9745 - val_loss: 0.0764 - val_accuracy: 0.9734\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0949 - accuracy: 0.9649 - val_loss: 0.0904 - val_accuracy: 0.9628\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.1023 - accuracy: 0.9626 - val_loss: 0.0695 - val_accuracy: 0.9787\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0901 - accuracy: 0.9673 - val_loss: 0.0645 - val_accuracy: 0.9840\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0972 - accuracy: 0.9655 - val_loss: 0.0683 - val_accuracy: 0.9734\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0866 - accuracy: 0.9691 - val_loss: 0.0635 - val_accuracy: 0.9787\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0961 - accuracy: 0.9685 - val_loss: 0.0716 - val_accuracy: 0.9681\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0863 - accuracy: 0.9703 - val_loss: 0.0616 - val_accuracy: 0.9894\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0780 - accuracy: 0.9721 - val_loss: 0.0690 - val_accuracy: 0.9787\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0803 - accuracy: 0.9739 - val_loss: 0.0640 - val_accuracy: 0.9787\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0718 - accuracy: 0.9774 - val_loss: 0.0594 - val_accuracy: 0.9894\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0722 - accuracy: 0.9750 - val_loss: 0.0523 - val_accuracy: 0.9734\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0753 - accuracy: 0.9768 - val_loss: 0.0589 - val_accuracy: 0.9787\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 70ms/step - loss: 0.0815 - accuracy: 0.9709 - val_loss: 0.0496 - val_accuracy: 0.9787\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0923 - accuracy: 0.9626 - val_loss: 0.0494 - val_accuracy: 0.9840\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0818 - accuracy: 0.9667 - val_loss: 0.0495 - val_accuracy: 0.9947\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0746 - accuracy: 0.9721 - val_loss: 0.0486 - val_accuracy: 0.9840\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0652 - accuracy: 0.9792 - val_loss: 0.0388 - val_accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0672 - accuracy: 0.9768 - val_loss: 0.0355 - val_accuracy: 0.9947\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 71ms/step - loss: 0.0626 - accuracy: 0.9816 - val_loss: 0.0425 - val_accuracy: 0.9787\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0724 - accuracy: 0.9768 - val_loss: 0.0483 - val_accuracy: 0.9947\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0732 - accuracy: 0.9733 - val_loss: 0.0505 - val_accuracy: 0.9787\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0685 - accuracy: 0.9745 - val_loss: 0.0430 - val_accuracy: 0.9894\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0842 - accuracy: 0.9655 - val_loss: 0.1157 - val_accuracy: 0.9468\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0868 - accuracy: 0.9697 - val_loss: 0.1323 - val_accuracy: 0.9521\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0882 - accuracy: 0.9655 - val_loss: 0.0854 - val_accuracy: 0.9681\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0771 - accuracy: 0.9703 - val_loss: 0.0580 - val_accuracy: 0.9787\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0710 - accuracy: 0.9733 - val_loss: 0.0420 - val_accuracy: 0.9947\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0736 - accuracy: 0.9733 - val_loss: 0.0351 - val_accuracy: 0.9894\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0671 - accuracy: 0.9780 - val_loss: 0.0463 - val_accuracy: 0.9840\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0632 - accuracy: 0.9810 - val_loss: 0.0351 - val_accuracy: 0.9947\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0575 - accuracy: 0.9798 - val_loss: 0.0313 - val_accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 70ms/step - loss: 0.0617 - accuracy: 0.9786 - val_loss: 0.0467 - val_accuracy: 0.9840\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0546 - accuracy: 0.9822 - val_loss: 0.0382 - val_accuracy: 0.9787\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0596 - accuracy: 0.9792 - val_loss: 0.0436 - val_accuracy: 0.9787\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0607 - accuracy: 0.9792 - val_loss: 0.0336 - val_accuracy: 0.9894\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0644 - accuracy: 0.9792 - val_loss: 0.0333 - val_accuracy: 0.9947\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0569 - accuracy: 0.9822 - val_loss: 0.0350 - val_accuracy: 0.9894\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0577 - accuracy: 0.9810 - val_loss: 0.0421 - val_accuracy: 0.9894\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0645 - accuracy: 0.9780 - val_loss: 0.0409 - val_accuracy: 0.9840\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0487 - accuracy: 0.9822 - val_loss: 0.0585 - val_accuracy: 0.9734\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 70ms/step - loss: 0.0805 - accuracy: 0.9703 - val_loss: 0.0572 - val_accuracy: 0.9787\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0646 - accuracy: 0.9762 - val_loss: 0.0583 - val_accuracy: 0.9681\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0697 - accuracy: 0.9768 - val_loss: 0.0373 - val_accuracy: 0.9840\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0669 - accuracy: 0.9756 - val_loss: 0.0413 - val_accuracy: 0.9894\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0666 - accuracy: 0.9739 - val_loss: 0.0312 - val_accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0677 - accuracy: 0.9727 - val_loss: 0.0386 - val_accuracy: 0.9840\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0629 - accuracy: 0.9792 - val_loss: 0.0435 - val_accuracy: 0.9840\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0638 - accuracy: 0.9750 - val_loss: 0.0380 - val_accuracy: 0.9894\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0539 - accuracy: 0.9822 - val_loss: 0.0383 - val_accuracy: 0.9787\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0626 - accuracy: 0.9780 - val_loss: 0.0513 - val_accuracy: 0.9787\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0589 - accuracy: 0.9798 - val_loss: 0.0512 - val_accuracy: 0.9734\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0613 - accuracy: 0.9762 - val_loss: 0.0341 - val_accuracy: 0.9894\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0555 - accuracy: 0.9828 - val_loss: 0.0372 - val_accuracy: 0.9894\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0515 - accuracy: 0.9810 - val_loss: 0.0383 - val_accuracy: 0.9894\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0551 - accuracy: 0.9822 - val_loss: 0.0436 - val_accuracy: 0.9787\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0575 - accuracy: 0.9792 - val_loss: 0.0407 - val_accuracy: 0.9840\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0489 - accuracy: 0.9846 - val_loss: 0.0345 - val_accuracy: 0.9734\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0473 - accuracy: 0.9857 - val_loss: 0.0385 - val_accuracy: 0.9734\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0502 - accuracy: 0.9810 - val_loss: 0.0312 - val_accuracy: 0.9894\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0460 - accuracy: 0.9822 - val_loss: 0.0369 - val_accuracy: 0.9840\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0578 - accuracy: 0.9762 - val_loss: 0.0276 - val_accuracy: 0.9894\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0509 - accuracy: 0.9792 - val_loss: 0.0424 - val_accuracy: 0.9734\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0524 - accuracy: 0.9834 - val_loss: 0.0347 - val_accuracy: 0.9840\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0488 - accuracy: 0.9851 - val_loss: 0.0335 - val_accuracy: 0.9894\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0444 - accuracy: 0.9851 - val_loss: 0.0301 - val_accuracy: 0.9947\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0469 - accuracy: 0.9834 - val_loss: 0.0280 - val_accuracy: 0.9894\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 69ms/step - loss: 0.0535 - accuracy: 0.9816 - val_loss: 0.0396 - val_accuracy: 0.9787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff4fc21f910>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mask_2 = np.ones((len(X_train_2),7))\n",
    "model_2.fit([X_train_2, X_mask_2], y_train_2_onehot, batch_size=512, epochs=100, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(vocab_size=15, hidden_size=16, num_hidden_layer=8, num_attention_heads=4,\n",
    "                    intermediate_size=64, max_position_embeddings=14)\n",
    "bert = TFBertModel(config)\n",
    "\n",
    "input_seq = tf.keras.layers.Input(shape=(14,), name='input_seq', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(14,), name='attention_mask', dtype='int32')\n",
    "\n",
    "encoder = bert(input_seq, attention_mask=mask)[0][:,0] \n",
    "\n",
    "x = tf.keras.layers.Dense(32, activation='relu')(encoder)\n",
    "y = tf.keras.layers.Dense(4, activation='softmax', name='outputs')(x)\n",
    "\n",
    "model_3 = tf.keras.Model(inputs=[input_seq, mask], outputs=y)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, decay=1e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model_3.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "5/5 [==============================] - 12s 588ms/step - loss: 1.1736 - accuracy: 0.5010 - val_loss: 1.0069 - val_accuracy: 0.6157\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 1.0308 - accuracy: 0.5981 - val_loss: 0.9731 - val_accuracy: 0.6493\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.9965 - accuracy: 0.6242 - val_loss: 0.9441 - val_accuracy: 0.6828\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.9714 - accuracy: 0.6454 - val_loss: 0.9299 - val_accuracy: 0.6903\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.9553 - accuracy: 0.6487 - val_loss: 0.9068 - val_accuracy: 0.6903\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.9411 - accuracy: 0.6483 - val_loss: 0.8888 - val_accuracy: 0.6903\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.9285 - accuracy: 0.6487 - val_loss: 0.8650 - val_accuracy: 0.6903\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.9005 - accuracy: 0.6487 - val_loss: 0.8296 - val_accuracy: 0.6903\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.8641 - accuracy: 0.6520 - val_loss: 0.7712 - val_accuracy: 0.6940\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.7913 - accuracy: 0.6682 - val_loss: 0.6730 - val_accuracy: 0.7090\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.6822 - accuracy: 0.7271 - val_loss: 0.5340 - val_accuracy: 0.8246\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.5616 - accuracy: 0.8063 - val_loss: 0.4727 - val_accuracy: 0.8507\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.4893 - accuracy: 0.8324 - val_loss: 0.4052 - val_accuracy: 0.8582\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.4193 - accuracy: 0.8561 - val_loss: 0.3343 - val_accuracy: 0.8806\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.3636 - accuracy: 0.8739 - val_loss: 0.3111 - val_accuracy: 0.8918\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.3323 - accuracy: 0.8764 - val_loss: 0.2699 - val_accuracy: 0.8955\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.3116 - accuracy: 0.8864 - val_loss: 0.2613 - val_accuracy: 0.8918\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 140ms/step - loss: 0.3002 - accuracy: 0.8913 - val_loss: 0.2376 - val_accuracy: 0.9179\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2881 - accuracy: 0.8955 - val_loss: 0.2339 - val_accuracy: 0.9179\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2784 - accuracy: 0.8963 - val_loss: 0.2234 - val_accuracy: 0.9254\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2717 - accuracy: 0.9021 - val_loss: 0.2156 - val_accuracy: 0.9179\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2709 - accuracy: 0.8938 - val_loss: 0.2121 - val_accuracy: 0.9216\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2683 - accuracy: 0.8942 - val_loss: 0.2125 - val_accuracy: 0.9291\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.2688 - accuracy: 0.8922 - val_loss: 0.2129 - val_accuracy: 0.8881\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2566 - accuracy: 0.8897 - val_loss: 0.2046 - val_accuracy: 0.9216\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.2528 - accuracy: 0.8909 - val_loss: 0.2150 - val_accuracy: 0.8918\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2526 - accuracy: 0.8926 - val_loss: 0.2008 - val_accuracy: 0.9030\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.2532 - accuracy: 0.8967 - val_loss: 0.1988 - val_accuracy: 0.9254\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.2484 - accuracy: 0.8951 - val_loss: 0.1927 - val_accuracy: 0.9254\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2437 - accuracy: 0.8959 - val_loss: 0.1935 - val_accuracy: 0.9328\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2401 - accuracy: 0.9017 - val_loss: 0.1942 - val_accuracy: 0.9328\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2430 - accuracy: 0.9050 - val_loss: 0.1938 - val_accuracy: 0.9254\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.2391 - accuracy: 0.9034 - val_loss: 0.1979 - val_accuracy: 0.9291\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.2356 - accuracy: 0.9083 - val_loss: 0.1872 - val_accuracy: 0.9328\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.2292 - accuracy: 0.9108 - val_loss: 0.1853 - val_accuracy: 0.9366\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2285 - accuracy: 0.9112 - val_loss: 0.1834 - val_accuracy: 0.9403\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.2307 - accuracy: 0.9108 - val_loss: 0.1820 - val_accuracy: 0.9328\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.2217 - accuracy: 0.9158 - val_loss: 0.1796 - val_accuracy: 0.9403\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2268 - accuracy: 0.9208 - val_loss: 0.1805 - val_accuracy: 0.9328\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2224 - accuracy: 0.9141 - val_loss: 0.1779 - val_accuracy: 0.9403\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2217 - accuracy: 0.9291 - val_loss: 0.1722 - val_accuracy: 0.9366\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2132 - accuracy: 0.9220 - val_loss: 0.1687 - val_accuracy: 0.9515\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2073 - accuracy: 0.9365 - val_loss: 0.1711 - val_accuracy: 0.9403\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.2069 - accuracy: 0.9341 - val_loss: 0.1611 - val_accuracy: 0.9590\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.2088 - accuracy: 0.9361 - val_loss: 0.1491 - val_accuracy: 0.9627\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.1994 - accuracy: 0.9386 - val_loss: 0.1542 - val_accuracy: 0.9440\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.2066 - accuracy: 0.9394 - val_loss: 0.1728 - val_accuracy: 0.9440\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.2208 - accuracy: 0.9208 - val_loss: 0.1385 - val_accuracy: 0.9664\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.1929 - accuracy: 0.9473 - val_loss: 0.1367 - val_accuracy: 0.9701\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.1890 - accuracy: 0.9407 - val_loss: 0.1390 - val_accuracy: 0.9627\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.1797 - accuracy: 0.9515 - val_loss: 0.1297 - val_accuracy: 0.9627\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.1742 - accuracy: 0.9502 - val_loss: 0.1256 - val_accuracy: 0.9627\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.1654 - accuracy: 0.9548 - val_loss: 0.1203 - val_accuracy: 0.9627\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.1647 - accuracy: 0.9511 - val_loss: 0.1170 - val_accuracy: 0.9627\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1573 - accuracy: 0.9548 - val_loss: 0.1197 - val_accuracy: 0.9627\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1532 - accuracy: 0.9535 - val_loss: 0.1169 - val_accuracy: 0.9627\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1556 - accuracy: 0.9544 - val_loss: 0.1183 - val_accuracy: 0.9627\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.1498 - accuracy: 0.9560 - val_loss: 0.1146 - val_accuracy: 0.9664\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1536 - accuracy: 0.9535 - val_loss: 0.1177 - val_accuracy: 0.9590\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.1457 - accuracy: 0.9594 - val_loss: 0.1158 - val_accuracy: 0.9627\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1417 - accuracy: 0.9598 - val_loss: 0.1228 - val_accuracy: 0.9590\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1393 - accuracy: 0.9602 - val_loss: 0.1198 - val_accuracy: 0.9627\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.1408 - accuracy: 0.9573 - val_loss: 0.1080 - val_accuracy: 0.9701\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.1369 - accuracy: 0.9585 - val_loss: 0.1089 - val_accuracy: 0.9701\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.1323 - accuracy: 0.9643 - val_loss: 0.1061 - val_accuracy: 0.9739\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1307 - accuracy: 0.9610 - val_loss: 0.1051 - val_accuracy: 0.9627\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1334 - accuracy: 0.9606 - val_loss: 0.1138 - val_accuracy: 0.9627\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.1353 - accuracy: 0.9635 - val_loss: 0.1179 - val_accuracy: 0.9627\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1372 - accuracy: 0.9606 - val_loss: 0.1053 - val_accuracy: 0.9627\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1285 - accuracy: 0.9606 - val_loss: 0.1080 - val_accuracy: 0.9664\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1269 - accuracy: 0.9643 - val_loss: 0.1109 - val_accuracy: 0.9701\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 135ms/step - loss: 0.1302 - accuracy: 0.9631 - val_loss: 0.0985 - val_accuracy: 0.9739\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1248 - accuracy: 0.9631 - val_loss: 0.0989 - val_accuracy: 0.9664\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1272 - accuracy: 0.9623 - val_loss: 0.1014 - val_accuracy: 0.9701\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.1260 - accuracy: 0.9639 - val_loss: 0.1121 - val_accuracy: 0.9627\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.1271 - accuracy: 0.9602 - val_loss: 0.0991 - val_accuracy: 0.9701\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1177 - accuracy: 0.9681 - val_loss: 0.0966 - val_accuracy: 0.9627\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1189 - accuracy: 0.9693 - val_loss: 0.1084 - val_accuracy: 0.9664\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1183 - accuracy: 0.9672 - val_loss: 0.0952 - val_accuracy: 0.9701\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1142 - accuracy: 0.9672 - val_loss: 0.1074 - val_accuracy: 0.9590\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 1s 139ms/step - loss: 0.1157 - accuracy: 0.9685 - val_loss: 0.1080 - val_accuracy: 0.9627\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1237 - accuracy: 0.9647 - val_loss: 0.1014 - val_accuracy: 0.9664\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1234 - accuracy: 0.9610 - val_loss: 0.1117 - val_accuracy: 0.9627\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1173 - accuracy: 0.9672 - val_loss: 0.0947 - val_accuracy: 0.9664\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.1103 - accuracy: 0.9714 - val_loss: 0.1080 - val_accuracy: 0.9701\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.1080 - accuracy: 0.9697 - val_loss: 0.0924 - val_accuracy: 0.9701\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1085 - accuracy: 0.9693 - val_loss: 0.0906 - val_accuracy: 0.9701\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.1090 - accuracy: 0.9701 - val_loss: 0.0884 - val_accuracy: 0.9664\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1091 - accuracy: 0.9697 - val_loss: 0.0901 - val_accuracy: 0.9739\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.1113 - accuracy: 0.9693 - val_loss: 0.0874 - val_accuracy: 0.9739\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1073 - accuracy: 0.9681 - val_loss: 0.0850 - val_accuracy: 0.9701\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1026 - accuracy: 0.9722 - val_loss: 0.0928 - val_accuracy: 0.9739\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.1054 - accuracy: 0.9710 - val_loss: 0.0937 - val_accuracy: 0.9739\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1020 - accuracy: 0.9706 - val_loss: 0.0979 - val_accuracy: 0.9701\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1081 - accuracy: 0.9701 - val_loss: 0.0876 - val_accuracy: 0.9664\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.1003 - accuracy: 0.9706 - val_loss: 0.1003 - val_accuracy: 0.9701\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 1s 137ms/step - loss: 0.1083 - accuracy: 0.9685 - val_loss: 0.0842 - val_accuracy: 0.9701\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 1s 146ms/step - loss: 0.1041 - accuracy: 0.9693 - val_loss: 0.1065 - val_accuracy: 0.9664\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1043 - accuracy: 0.9697 - val_loss: 0.0907 - val_accuracy: 0.9664\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.1020 - accuracy: 0.9710 - val_loss: 0.1079 - val_accuracy: 0.9701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff4ef608580>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_mask_3 = np.ones((len(X_train_3),14))\n",
    "model_3.fit([X_train_3, X_mask_3], y_train_3_onehot, batch_size=512, epochs=100, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (train with  whole training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_4/bert/pooler/dense/kernel:0', 'tf_bert_model_4/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "4/4 [==============================] - 10s 68ms/step - loss: 1.2807 - accuracy: 0.4046\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 1.0863 - accuracy: 0.5238\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 86ms/step - loss: 1.0214 - accuracy: 0.5382\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.9868 - accuracy: 0.5361\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 77ms/step - loss: 0.9529 - accuracy: 0.5409\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.8922 - accuracy: 0.5879\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.7911 - accuracy: 0.7119\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.6468 - accuracy: 0.7819\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.5363 - accuracy: 0.7889\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.4802 - accuracy: 0.8172\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.4421 - accuracy: 0.8236\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.4094 - accuracy: 0.8252\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.3736 - accuracy: 0.8514\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.3356 - accuracy: 0.8653\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.3030 - accuracy: 0.8760\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.2636 - accuracy: 0.9070\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.2177 - accuracy: 0.9284\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.2025 - accuracy: 0.9391\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.1830 - accuracy: 0.9337\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.1664 - accuracy: 0.9503\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.1555 - accuracy: 0.9466\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.1466 - accuracy: 0.9449\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.1427 - accuracy: 0.9508\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.1256 - accuracy: 0.9642\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.1176 - accuracy: 0.9669\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.1160 - accuracy: 0.9637\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.1180 - accuracy: 0.9562\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.1138 - accuracy: 0.9637\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.1103 - accuracy: 0.9653\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0965 - accuracy: 0.9685\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.1017 - accuracy: 0.9658\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0916 - accuracy: 0.9711\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0933 - accuracy: 0.9722\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0959 - accuracy: 0.9711\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0838 - accuracy: 0.9749\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0866 - accuracy: 0.9695\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0913 - accuracy: 0.9727\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0804 - accuracy: 0.9743\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0810 - accuracy: 0.9722\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0794 - accuracy: 0.9754\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0775 - accuracy: 0.9749\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0795 - accuracy: 0.9770\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0805 - accuracy: 0.9711\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0732 - accuracy: 0.9770\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.0784 - accuracy: 0.9781\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0843 - accuracy: 0.9727\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.1030 - accuracy: 0.9610\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0979 - accuracy: 0.9626\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0921 - accuracy: 0.9695\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0781 - accuracy: 0.9743\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.0785 - accuracy: 0.9738\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0643 - accuracy: 0.9776\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0737 - accuracy: 0.9743\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0709 - accuracy: 0.9786\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0619 - accuracy: 0.9776\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0625 - accuracy: 0.9824\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0604 - accuracy: 0.9786\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0717 - accuracy: 0.9749\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0629 - accuracy: 0.9776\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0791 - accuracy: 0.9695\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.0589 - accuracy: 0.9808\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0691 - accuracy: 0.9738\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0771 - accuracy: 0.9743\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0745 - accuracy: 0.9738\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0662 - accuracy: 0.9759\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0746 - accuracy: 0.9749\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0647 - accuracy: 0.9759\n",
      "Epoch 68/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0620 - accuracy: 0.9792\n",
      "Epoch 69/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0613 - accuracy: 0.9770\n",
      "Epoch 70/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0688 - accuracy: 0.9727\n",
      "Epoch 71/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0738 - accuracy: 0.9706\n",
      "Epoch 72/100\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.0644 - accuracy: 0.9808\n",
      "Epoch 73/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0633 - accuracy: 0.9776\n",
      "Epoch 74/100\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.0602 - accuracy: 0.9818\n",
      "Epoch 75/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0605 - accuracy: 0.9765\n",
      "Epoch 76/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0617 - accuracy: 0.9776\n",
      "Epoch 77/100\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.0514 - accuracy: 0.9802\n",
      "Epoch 78/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0458 - accuracy: 0.9850\n",
      "Epoch 79/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0656 - accuracy: 0.9759\n",
      "Epoch 80/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0540 - accuracy: 0.9824\n",
      "Epoch 81/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0485 - accuracy: 0.9872\n",
      "Epoch 82/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0443 - accuracy: 0.9861\n",
      "Epoch 83/100\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.0482 - accuracy: 0.9856\n",
      "Epoch 84/100\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0548 - accuracy: 0.9829\n",
      "Epoch 85/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0542 - accuracy: 0.9834\n",
      "Epoch 86/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0495 - accuracy: 0.9829\n",
      "Epoch 87/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0615 - accuracy: 0.9786\n",
      "Epoch 88/100\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0636 - accuracy: 0.9743\n",
      "Epoch 89/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0717 - accuracy: 0.9743\n",
      "Epoch 90/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0603 - accuracy: 0.9786\n",
      "Epoch 91/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0609 - accuracy: 0.9743\n",
      "Epoch 92/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0594 - accuracy: 0.9781\n",
      "Epoch 93/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0511 - accuracy: 0.9808\n",
      "Epoch 94/100\n",
      "4/4 [==============================] - 0s 67ms/step - loss: 0.0516 - accuracy: 0.9845\n",
      "Epoch 95/100\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.0582 - accuracy: 0.9797\n",
      "Epoch 96/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0543 - accuracy: 0.9802\n",
      "Epoch 97/100\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.0561 - accuracy: 0.9797\n",
      "Epoch 98/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0597 - accuracy: 0.9770\n",
      "Epoch 99/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0502 - accuracy: 0.9813\n",
      "Epoch 100/100\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0524 - accuracy: 0.9792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff4ffa20490>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BertConfig(vocab_size=15, hidden_size=16, num_hidden_layer=8, num_attention_heads=4,\n",
    "                    intermediate_size=64, max_position_embeddings=14)\n",
    "bert = TFBertModel(config)\n",
    "\n",
    "input_seq = tf.keras.layers.Input(shape=(7,), name='input_seq', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(7,), name='attention_mask', dtype='int32')\n",
    "\n",
    "encoder = bert(input_seq, attention_mask=mask)[0][:,0] \n",
    "\n",
    "x = tf.keras.layers.Dense(32, activation='relu')(encoder)\n",
    "y = tf.keras.layers.Dense(4, activation='softmax', name='outputs')(x)\n",
    "\n",
    "model_2 = tf.keras.Model(inputs=[input_seq, mask], outputs=y)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, decay=1e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model_2.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
    "\n",
    "X_mask_2 = np.ones((len(X_train_2),7))\n",
    "model_2.fit([X_train_2, X_mask_2], y_train_2_onehot, batch_size=512, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_5/bert/pooler/dense/kernel:0', 'tf_bert_model_5/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "6/6 [==============================] - 11s 119ms/step - loss: 1.5101 - accuracy: 0.2307\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 1.1558 - accuracy: 0.4602\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 1.0372 - accuracy: 0.5916\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 1.0098 - accuracy: 0.5913\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 1.0010 - accuracy: 0.5920\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.9932 - accuracy: 0.6028\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.9782 - accuracy: 0.6170\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.9661 - accuracy: 0.6390\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.9389 - accuracy: 0.6510\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.8891 - accuracy: 0.6607\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.8135 - accuracy: 0.6943\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.7253 - accuracy: 0.7436\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.6072 - accuracy: 0.8078\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.4845 - accuracy: 0.8485\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.4309 - accuracy: 0.8578\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.4050 - accuracy: 0.8634\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.3709 - accuracy: 0.8847\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.3596 - accuracy: 0.8906\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.3368 - accuracy: 0.8973\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.3183 - accuracy: 0.8962\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.3105 - accuracy: 0.8985\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.3018 - accuracy: 0.9003\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.2950 - accuracy: 0.9029\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.2859 - accuracy: 0.9074\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.2839 - accuracy: 0.9018\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.2743 - accuracy: 0.9052\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.2630 - accuracy: 0.9044\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.2501 - accuracy: 0.9089\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.2445 - accuracy: 0.9138\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.2538 - accuracy: 0.9186\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.2413 - accuracy: 0.9153\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.2221 - accuracy: 0.9209\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.2225 - accuracy: 0.9227\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.2051 - accuracy: 0.9235\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.2050 - accuracy: 0.9268\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1947 - accuracy: 0.9388\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1849 - accuracy: 0.9440\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1844 - accuracy: 0.9418\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1742 - accuracy: 0.9377\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1878 - accuracy: 0.9421\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1728 - accuracy: 0.9406\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.1764 - accuracy: 0.9462\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1601 - accuracy: 0.9504\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.1541 - accuracy: 0.9507\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.1439 - accuracy: 0.9578\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.1361 - accuracy: 0.9582\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 1s 116ms/step - loss: 0.1482 - accuracy: 0.9522\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1385 - accuracy: 0.9567\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1351 - accuracy: 0.9604\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1405 - accuracy: 0.9545\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1353 - accuracy: 0.9612\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1299 - accuracy: 0.9612\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1236 - accuracy: 0.9623\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.1236 - accuracy: 0.9612\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.1202 - accuracy: 0.9638\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1174 - accuracy: 0.9630\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1166 - accuracy: 0.9653\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.1141 - accuracy: 0.9668\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1130 - accuracy: 0.9657\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1196 - accuracy: 0.9619\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.1118 - accuracy: 0.9653\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1155 - accuracy: 0.9645\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.1108 - accuracy: 0.9657\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1028 - accuracy: 0.9690\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.1115 - accuracy: 0.9668\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.1045 - accuracy: 0.9686\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.1064 - accuracy: 0.9653\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.1072 - accuracy: 0.9694\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.0996 - accuracy: 0.9690\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.0981 - accuracy: 0.9716\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1006 - accuracy: 0.9694\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.0999 - accuracy: 0.9683\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.0988 - accuracy: 0.9716\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1020 - accuracy: 0.9679\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1041 - accuracy: 0.9675\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.1025 - accuracy: 0.9705\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1068 - accuracy: 0.9660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.0941 - accuracy: 0.9750\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1089 - accuracy: 0.9672\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.0976 - accuracy: 0.9713\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1047 - accuracy: 0.9672\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.1082 - accuracy: 0.9664\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.0965 - accuracy: 0.9690\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.0991 - accuracy: 0.9698\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.0944 - accuracy: 0.9720\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.0931 - accuracy: 0.9705\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.1038 - accuracy: 0.9675\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.0985 - accuracy: 0.9675\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.0924 - accuracy: 0.9709\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 1s 119ms/step - loss: 0.0910 - accuracy: 0.9735\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.0972 - accuracy: 0.9705\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.0968 - accuracy: 0.9709\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.0976 - accuracy: 0.9694\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.0999 - accuracy: 0.9698\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.0989 - accuracy: 0.9683\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 1s 118ms/step - loss: 0.1109 - accuracy: 0.9645\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.0972 - accuracy: 0.9709\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 1s 121ms/step - loss: 0.0931 - accuracy: 0.9739\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 1s 117ms/step - loss: 0.0953 - accuracy: 0.9690\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 1s 131ms/step - loss: 0.0926 - accuracy: 0.9701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff4e5fe4d90>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BertConfig(vocab_size=15, hidden_size=16, num_hidden_layer=8, num_attention_heads=4,\n",
    "                    intermediate_size=64, max_position_embeddings=14)\n",
    "bert = TFBertModel(config)\n",
    "\n",
    "input_seq = tf.keras.layers.Input(shape=(14,), name='input_seq', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(14,), name='attention_mask', dtype='int32')\n",
    "\n",
    "encoder = bert(input_seq, attention_mask=mask)[0][:,0] \n",
    "\n",
    "x = tf.keras.layers.Dense(32, activation='relu')(encoder)\n",
    "y = tf.keras.layers.Dense(4, activation='softmax', name='outputs')(x)\n",
    "\n",
    "model_3 = tf.keras.Model(inputs=[input_seq, mask], outputs=y)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, decay=1e-5)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model_3.compile(optimizer=optimizer, loss=loss, metrics=[acc])\n",
    "\n",
    "X_mask_3 = np.ones((len(X_train_3),14))\n",
    "model_3.fit([X_train_3, X_mask_3], y_train_3_onehot, batch_size=512, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "group_id_dict = {m: [] for m in id_list_test}\n",
    "\n",
    "for data in list(test_data.values):\n",
    "    loc = data[1]\n",
    "    group_id_dict[data[0]].append(loc)\n",
    "\n",
    "i = 0\n",
    "y_prob_group = np.zeros((3429,5))\n",
    "for k, v in group_id_dict.items():\n",
    "    if(i % 1000 == 0): print(i)\n",
    "    if 1 <= len(v) <= 2:\n",
    "        X = [0] * (2 - len(v)) + v\n",
    "        y_prob_group[i] = [1,0,0,0,0]\n",
    "    elif 3 <= len(v) <= 7:\n",
    "        # padding\n",
    "        X = [0] * (7 - len(v)) + v\n",
    "        X_test_mask = np.ones(7)\n",
    "        y_prob_group[i][0:4] = model_2.predict([np.array([X,]), np.array([X_test_mask,])], verbose=0)\n",
    "    elif 8 <= len(v) <= 14:\n",
    "        # padding\n",
    "        X = [0] * (14 - len(v)) + v\n",
    "        X_test_mask = np.ones(14)\n",
    "        y_prob_group[i][1:] = model_3.predict([np.array([X,]), np.array([X_test_mask,])], verbose=0)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mac_hash</th>\n",
       "      <th>C0</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b882f1d44602a25349a08f4a0af32977</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>0.998355</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86e644b498922f3a2fd0e6e1a1525de3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.001234</td>\n",
       "      <td>0.997326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>691662b04ee08015062d901a4c5628b1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52b5c510a28774237f4f118764c2ed6f</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.997899</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9f3c995e53d109f532056b6eae29a0b5</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.049677</td>\n",
       "      <td>0.948676</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3424</th>\n",
       "      <td>5001c0dfe522886ed884c0dc4e5848df</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3425</th>\n",
       "      <td>f1c8781e701e89068fa0b8a62ecbc564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.010149</td>\n",
       "      <td>0.989648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3426</th>\n",
       "      <td>646136b402e136422466a2acd8636630</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3427</th>\n",
       "      <td>c7bea6491029f86ab3d1f0f9b599ca2c</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.999350</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3428</th>\n",
       "      <td>822fd3104f38cc6374f2996d3d6920a2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3429 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              mac_hash        C0        C1        C2  \\\n",
       "0     b882f1d44602a25349a08f4a0af32977  0.001084  0.000386  0.998355   \n",
       "1     86e644b498922f3a2fd0e6e1a1525de3  0.000000  0.001393  0.000048   \n",
       "2     691662b04ee08015062d901a4c5628b1  1.000000  0.000000  0.000000   \n",
       "3     52b5c510a28774237f4f118764c2ed6f  0.001322  0.997899  0.000107   \n",
       "4     9f3c995e53d109f532056b6eae29a0b5  0.000921  0.049677  0.948676   \n",
       "...                                ...       ...       ...       ...   \n",
       "3424  5001c0dfe522886ed884c0dc4e5848df  1.000000  0.000000  0.000000   \n",
       "3425  f1c8781e701e89068fa0b8a62ecbc564  0.000000  0.000105  0.000098   \n",
       "3426  646136b402e136422466a2acd8636630  1.000000  0.000000  0.000000   \n",
       "3427  c7bea6491029f86ab3d1f0f9b599ca2c  0.000358  0.000137  0.999350   \n",
       "3428  822fd3104f38cc6374f2996d3d6920a2  1.000000  0.000000  0.000000   \n",
       "\n",
       "            C3        C4  \n",
       "0     0.000175  0.000000  \n",
       "1     0.001234  0.997326  \n",
       "2     0.000000  0.000000  \n",
       "3     0.000672  0.000000  \n",
       "4     0.000726  0.000000  \n",
       "...        ...       ...  \n",
       "3424  0.000000  0.000000  \n",
       "3425  0.010149  0.989648  \n",
       "3426  0.000000  0.000000  \n",
       "3427  0.000155  0.000000  \n",
       "3428  0.000000  0.000000  \n",
       "\n",
       "[3429 rows x 6 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = pd.DataFrame({'mac_hash':sample['mac_hash'], 'C0':y_prob_group[:,0], 'C1':y_prob_group[:,1], 'C2':y_prob_group[:,2], 'C3':y_prob_group[:,3], 'C4':y_prob_group[:,4]})\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(\"output_transformer_bert_group.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fceb98203492c09859d0b01378e726523ba812995c12a72a832827900f4e46fe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
